{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfYigiq0uiWw"
   },
   "source": [
    "# Practice VI\n",
    "***\n",
    "* Gonzalez Chacon Monica\n",
    "* Lopez Salazar Esmeralda Leticia\n",
    "* Rodriguez Nuñez Diego Eduardo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AAo8QRDfurn-",
    "outputId": "d32b0795-edc4-497b-ecc7-127ffec94db2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-sm==3.7.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.13.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.7.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PMF5Zbk1uiWx",
    "outputId": "f15333ab-0ce2-4084-e46f-89c72dbbb9b1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score , ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4nlgqeGDuiWy"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "SuzZksR2uiWy",
    "outputId": "fd152bfe-b040-4804-db96-02622f5e381d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Opinion</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Attraction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pésimo lugar</td>\n",
       "      <td>Piensen dos veces antes de ir a este hotel, te...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No vayas a lugar de Eddie</td>\n",
       "      <td>Cuatro de nosotros fuimos recientemente a Eddi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Restaurant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mala relación calidad-precio</td>\n",
       "      <td>seguiré corta y simple: limpieza\\n- bad. Tengo...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Minusválido? ¡No te alojes aquí!</td>\n",
       "      <td>Al reservar un hotel con multipropiedad Mayan ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Es una porqueria no pierdan su tiempo</td>\n",
       "      <td>No pierdan su tiempo ni dinero, venimos porque...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30207</th>\n",
       "      <td>Verdadera joya arquitectónica</td>\n",
       "      <td>Es una construcción majestuosa, creo que de la...</td>\n",
       "      <td>5</td>\n",
       "      <td>Attractive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30208</th>\n",
       "      <td>Romántico</td>\n",
       "      <td>Muy al estilo de Romeo y Julieta es este sitio...</td>\n",
       "      <td>5</td>\n",
       "      <td>Attractive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30209</th>\n",
       "      <td>Parece un castillo</td>\n",
       "      <td>Ideal para subir las escalinatas y divisar su ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Attractive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30210</th>\n",
       "      <td>Imperdible</td>\n",
       "      <td>Es imperdible, de ahí puedes ver muy bien la c...</td>\n",
       "      <td>5</td>\n",
       "      <td>Attractive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30211</th>\n",
       "      <td>Muy bonita vista</td>\n",
       "      <td>No te puedes ir de Guanajuato sin visitarlo......</td>\n",
       "      <td>5</td>\n",
       "      <td>Attractive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30212 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Title  \\\n",
       "0                               Pésimo lugar   \n",
       "1                  No vayas a lugar de Eddie   \n",
       "2               Mala relación calidad-precio   \n",
       "3           Minusválido? ¡No te alojes aquí!   \n",
       "4      Es una porqueria no pierdan su tiempo   \n",
       "...                                      ...   \n",
       "30207          Verdadera joya arquitectónica   \n",
       "30208                              Romántico   \n",
       "30209                     Parece un castillo   \n",
       "30210                             Imperdible   \n",
       "30211                       Muy bonita vista   \n",
       "\n",
       "                                                 Opinion  Polarity  Attraction  \n",
       "0      Piensen dos veces antes de ir a este hotel, te...         1       Hotel  \n",
       "1      Cuatro de nosotros fuimos recientemente a Eddi...         1  Restaurant  \n",
       "2      seguiré corta y simple: limpieza\\n- bad. Tengo...         1       Hotel  \n",
       "3      Al reservar un hotel con multipropiedad Mayan ...         1       Hotel  \n",
       "4      No pierdan su tiempo ni dinero, venimos porque...         1       Hotel  \n",
       "...                                                  ...       ...         ...  \n",
       "30207  Es una construcción majestuosa, creo que de la...         5  Attractive  \n",
       "30208  Muy al estilo de Romeo y Julieta es este sitio...         5  Attractive  \n",
       "30209  Ideal para subir las escalinatas y divisar su ...         5  Attractive  \n",
       "30210  Es imperdible, de ahí puedes ver muy bien la c...         5  Attractive  \n",
       "30211  No te puedes ir de Guanajuato sin visitarlo......         5  Attractive  \n",
       "\n",
       "[30212 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('Rest_Mex_2022.xlsx')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "aE2eTi7IuiWz",
    "outputId": "4d564da2-916c-42f7-a001-14142c51a595"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Opinion</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Attraction</th>\n",
       "      <th>Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pésimo lugar</td>\n",
       "      <td>Piensen dos veces antes de ir a este hotel, te...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>Pésimo lugar Piensen dos veces antes de ir a e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No vayas a lugar de Eddie</td>\n",
       "      <td>Cuatro de nosotros fuimos recientemente a Eddi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>No vayas a lugar de Eddie Cuatro de nosotros f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mala relación calidad-precio</td>\n",
       "      <td>seguiré corta y simple: limpieza\\n- bad. Tengo...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>Mala relación calidad-precio seguiré corta y s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Minusválido? ¡No te alojes aquí!</td>\n",
       "      <td>Al reservar un hotel con multipropiedad Mayan ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>Minusválido? ¡No te alojes aquí! Al reservar u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Es una porqueria no pierdan su tiempo</td>\n",
       "      <td>No pierdan su tiempo ni dinero, venimos porque...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hotel</td>\n",
       "      <td>Es una porqueria no pierdan su tiempo No pierd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30207</th>\n",
       "      <td>Verdadera joya arquitectónica</td>\n",
       "      <td>Es una construcción majestuosa, creo que de la...</td>\n",
       "      <td>5</td>\n",
       "      <td>Attractive</td>\n",
       "      <td>Verdadera joya arquitectónica Es una construcc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30208</th>\n",
       "      <td>Romántico</td>\n",
       "      <td>Muy al estilo de Romeo y Julieta es este sitio...</td>\n",
       "      <td>5</td>\n",
       "      <td>Attractive</td>\n",
       "      <td>Romántico Muy al estilo de Romeo y Julieta es ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30209</th>\n",
       "      <td>Parece un castillo</td>\n",
       "      <td>Ideal para subir las escalinatas y divisar su ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Attractive</td>\n",
       "      <td>Parece un castillo Ideal para subir las escali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30210</th>\n",
       "      <td>Imperdible</td>\n",
       "      <td>Es imperdible, de ahí puedes ver muy bien la c...</td>\n",
       "      <td>5</td>\n",
       "      <td>Attractive</td>\n",
       "      <td>Imperdible Es imperdible, de ahí puedes ver mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30211</th>\n",
       "      <td>Muy bonita vista</td>\n",
       "      <td>No te puedes ir de Guanajuato sin visitarlo......</td>\n",
       "      <td>5</td>\n",
       "      <td>Attractive</td>\n",
       "      <td>Muy bonita vista No te puedes ir de Guanajuato...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30212 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Title  \\\n",
       "0                               Pésimo lugar   \n",
       "1                  No vayas a lugar de Eddie   \n",
       "2               Mala relación calidad-precio   \n",
       "3           Minusválido? ¡No te alojes aquí!   \n",
       "4      Es una porqueria no pierdan su tiempo   \n",
       "...                                      ...   \n",
       "30207          Verdadera joya arquitectónica   \n",
       "30208                              Romántico   \n",
       "30209                     Parece un castillo   \n",
       "30210                             Imperdible   \n",
       "30211                       Muy bonita vista   \n",
       "\n",
       "                                                 Opinion  Polarity  \\\n",
       "0      Piensen dos veces antes de ir a este hotel, te...         1   \n",
       "1      Cuatro de nosotros fuimos recientemente a Eddi...         1   \n",
       "2      seguiré corta y simple: limpieza\\n- bad. Tengo...         1   \n",
       "3      Al reservar un hotel con multipropiedad Mayan ...         1   \n",
       "4      No pierdan su tiempo ni dinero, venimos porque...         1   \n",
       "...                                                  ...       ...   \n",
       "30207  Es una construcción majestuosa, creo que de la...         5   \n",
       "30208  Muy al estilo de Romeo y Julieta es este sitio...         5   \n",
       "30209  Ideal para subir las escalinatas y divisar su ...         5   \n",
       "30210  Es imperdible, de ahí puedes ver muy bien la c...         5   \n",
       "30211  No te puedes ir de Guanajuato sin visitarlo......         5   \n",
       "\n",
       "       Attraction                                           Features  \n",
       "0           Hotel  Pésimo lugar Piensen dos veces antes de ir a e...  \n",
       "1      Restaurant  No vayas a lugar de Eddie Cuatro de nosotros f...  \n",
       "2           Hotel  Mala relación calidad-precio seguiré corta y s...  \n",
       "3           Hotel  Minusválido? ¡No te alojes aquí! Al reservar u...  \n",
       "4           Hotel  Es una porqueria no pierdan su tiempo No pierd...  \n",
       "...           ...                                                ...  \n",
       "30207  Attractive  Verdadera joya arquitectónica Es una construcc...  \n",
       "30208  Attractive  Romántico Muy al estilo de Romeo y Julieta es ...  \n",
       "30209  Attractive  Parece un castillo Ideal para subir las escali...  \n",
       "30210  Attractive  Imperdible Es imperdible, de ahí puedes ver mu...  \n",
       "30211  Attractive  Muy bonita vista No te puedes ir de Guanajuato...  \n",
       "\n",
       "[30212 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Features'] = data['Title'].astype(str) + ' ' + data['Opinion'].astype(str)\n",
    "X = data['Features'].fillna('')\n",
    "y = data['Polarity']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0ginPzb3uiWz"
   },
   "outputs": [],
   "source": [
    "#esto ignoralo xd\n",
    "#y = y.str.replace(r'\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfGzniqfuiWz"
   },
   "source": [
    "Division del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AjNGkmRWuiWz"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jKIsRsqEuiWz"
   },
   "outputs": [],
   "source": [
    "def NormalizarTexto(texto, aplicar):\n",
    "    texto = texto.lower() if 'text_cleaning' in aplicar else texto\n",
    "\n",
    "    doc = nlp(texto)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if not token.is_punct and not token.is_space:\n",
    "            if 'stop_words' in aplicar and token.is_stop:\n",
    "                continue\n",
    "            tokens.append(token.lemma_ if 'lemmatization' in aplicar else token.text)\n",
    "    return ' '.join(tokens) if 'tokenization' in aplicar else ' '.join([texto])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mIPDyaPUuiWz",
    "outputId": "f8c0014d-537d-4909-f3f5-57c7f4355186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando combinacion: ['tokenization']\n",
      "Procesando combinacion: ['stop_words']\n",
      "Procesando combinacion: ['lemmatization']\n",
      "Procesando combinacion: ['text_cleaning']\n",
      "Procesando combinacion: ['tokenization', 'stop_words']\n",
      "Procesando combinacion: ['tokenization', 'lemmatization']\n",
      "Procesando combinacion: ['tokenization', 'text_cleaning']\n",
      "Procesando combinacion: ['stop_words', 'lemmatization']\n",
      "Procesando combinacion: ['stop_words', 'text_cleaning']\n",
      "Procesando combinacion: ['lemmatization', 'text_cleaning']\n",
      "Procesando combinacion: ['tokenization', 'stop_words', 'lemmatization']\n",
      "Procesando combinacion: ['tokenization', 'stop_words', 'text_cleaning']\n",
      "Procesando combinacion: ['tokenization', 'lemmatization', 'text_cleaning']\n",
      "Procesando combinacion: ['stop_words', 'lemmatization', 'text_cleaning']\n",
      "Procesando combinacion: ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning']\n",
      "Combinación: ['tokenization']\n",
      "Train: 18594    El mejor también en el lockdown También ahora ...\n",
      "19690    Vacaciones con mi esposo Desde que llegamos al...\n",
      "26183    Super Bowl Excelente servicio y ponen el súper...\n",
      "26029    Rico y abundante Excelente los desayunos muy a...\n",
      "16566    Tratamiento de spa Tuvimos un masaje de 80 min...\n",
      "Name: Features, dtype: object\n",
      "Test: 9730     Gran cena con fuegos artificiales después La c...\n",
      "7442     Vacaciones en familia en general estupendo Nos...\n",
      "25844    Vacaciones Gran lugar para comer Camarón azul ...\n",
      "9565     lugar romantico para llevar a tu pareja buena ...\n",
      "718      Ejecución inconsistente cocina Cenamos dos vec...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['stop_words']\n",
      "Train: 18594    El mejor también en el lockdown También ahora,...\n",
      "19690    Vacaciones con mi esposo Desde que llegamos al...\n",
      "26183    Super Bowl Excelente servicio y ponen el súper...\n",
      "26029    Rico y abundante Excelente los desayunos muy a...\n",
      "16566    Tratamiento de spa Tuvimos un masaje de 80 min...\n",
      "Name: Features, dtype: object\n",
      "Test: 9730     Gran cena con fuegos artificiales después! La ...\n",
      "7442     Vacaciones en familia en general estupendo Nos...\n",
      "25844    Vacaciones Gran lugar para comer! Camarón azul...\n",
      "9565     lugar romantico para llevar a tu pareja buena ...\n",
      "718      Ejecución inconsistente cocina Cenamos dos vec...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['lemmatization']\n",
      "Train: 18594    El mejor también en el lockdown También ahora,...\n",
      "19690    Vacaciones con mi esposo Desde que llegamos al...\n",
      "26183    Super Bowl Excelente servicio y ponen el súper...\n",
      "26029    Rico y abundante Excelente los desayunos muy a...\n",
      "16566    Tratamiento de spa Tuvimos un masaje de 80 min...\n",
      "Name: Features, dtype: object\n",
      "Test: 9730     Gran cena con fuegos artificiales después! La ...\n",
      "7442     Vacaciones en familia en general estupendo Nos...\n",
      "25844    Vacaciones Gran lugar para comer! Camarón azul...\n",
      "9565     lugar romantico para llevar a tu pareja buena ...\n",
      "718      Ejecución inconsistente cocina Cenamos dos vec...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['text_cleaning']\n",
      "Train: 18594    el mejor también en el lockdown también ahora,...\n",
      "19690    vacaciones con mi esposo desde que llegamos al...\n",
      "26183    super bowl excelente servicio y ponen el súper...\n",
      "26029    rico y abundante excelente los desayunos muy a...\n",
      "16566    tratamiento de spa tuvimos un masaje de 80 min...\n",
      "Name: Features, dtype: object\n",
      "Test: 9730     gran cena con fuegos artificiales después! la ...\n",
      "7442     vacaciones en familia en general estupendo nos...\n",
      "25844    vacaciones gran lugar para comer! camarón azul...\n",
      "9565     lugar romantico para llevar a tu pareja buena ...\n",
      "718      ejecución inconsistente cocina cenamos dos vec...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['tokenization', 'stop_words']\n",
      "Train: 18594    lockdown abiertos comida domicilio mejór mejór...\n",
      "19690    Vacaciones esposo llegamos hotel personal aten...\n",
      "26183    Super Bowl Excelente servicio ponen súper tazó...\n",
      "26029    Rico abundante Excelente desayunos abundantes ...\n",
      "16566    Tratamiento spa Tuvimos masaje 80 minutos pare...\n",
      "Name: Features, dtype: object\n",
      "Test: 9730     cena fuegos artificiales cena fenomenal servic...\n",
      "7442     Vacaciones familia general estupendo encaminam...\n",
      "25844    Vacaciones lugar comer Camarón azul pargo Cama...\n",
      "9565     lugar romantico pareja comida abundante precio...\n",
      "718      Ejecución inconsistente cocina Cenamos período...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['tokenization', 'lemmatization']\n",
      "Train: 18594    el mejor también en el lockdown también ahora ...\n",
      "19690    Vacaciones con mi esposo Desde que llegar al h...\n",
      "26183    Super Bowl Excelente servicio y poner el súper...\n",
      "26029    Rico y abundante Excelente el desayuno mucho a...\n",
      "16566    Tratamiento de spa Tuvimos uno masaje de 80 mi...\n",
      "Name: Features, dtype: object\n",
      "Test: 9730     gran cenir con fuego artificial después el cen...\n",
      "7442     Vacaciones en familia en general estupendo yo ...\n",
      "25844    Vacaciones gran lugar para comer Camarón azul ...\n",
      "9565     lugar romantico para llevar a tu pareja buen c...\n",
      "718      ejecución inconsistente cocina cenar dos vez e...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['tokenization', 'text_cleaning']\n",
      "Train: 18594    el mejor también en el lockdown también ahora ...\n",
      "19690    vacaciones con mi esposo desde que llegamos al...\n",
      "26183    super bowl excelente servicio y ponen el súper...\n",
      "26029    rico y abundante excelente los desayunos muy a...\n",
      "16566    tratamiento de spa tuvimos un masaje de 80 min...\n",
      "Name: Features, dtype: object\n",
      "Test: 9730     gran cena con fuegos artificiales después la c...\n",
      "7442     vacaciones en familia en general estupendo nos...\n",
      "25844    vacaciones gran lugar para comer camarón azul ...\n",
      "9565     lugar romantico para llevar a tu pareja buena ...\n",
      "718      ejecución inconsistente cocina cenamos dos vec...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['stop_words', 'lemmatization']\n",
      "Train: 18594    El mejor también en el lockdown También ahora,...\n",
      "19690    Vacaciones con mi esposo Desde que llegamos al...\n",
      "26183    Super Bowl Excelente servicio y ponen el súper...\n",
      "26029    Rico y abundante Excelente los desayunos muy a...\n",
      "16566    Tratamiento de spa Tuvimos un masaje de 80 min...\n",
      "Name: Features, dtype: object\n",
      "Test: 9730     Gran cena con fuegos artificiales después! La ...\n",
      "7442     Vacaciones en familia en general estupendo Nos...\n",
      "25844    Vacaciones Gran lugar para comer! Camarón azul...\n",
      "9565     lugar romantico para llevar a tu pareja buena ...\n",
      "718      Ejecución inconsistente cocina Cenamos dos vec...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['stop_words', 'text_cleaning']\n",
      "Train: 18594    el mejor también en el lockdown también ahora,...\n",
      "19690    vacaciones con mi esposo desde que llegamos al...\n",
      "26183    super bowl excelente servicio y ponen el súper...\n",
      "26029    rico y abundante excelente los desayunos muy a...\n",
      "16566    tratamiento de spa tuvimos un masaje de 80 min...\n",
      "Name: Features, dtype: object\n",
      "Test: 9730     gran cena con fuegos artificiales después! la ...\n",
      "7442     vacaciones en familia en general estupendo nos...\n",
      "25844    vacaciones gran lugar para comer! camarón azul...\n",
      "9565     lugar romantico para llevar a tu pareja buena ...\n",
      "718      ejecución inconsistente cocina cenamos dos vec...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['lemmatization', 'text_cleaning']\n",
      "Train: 18594    el mejor también en el lockdown también ahora,...\n",
      "19690    vacaciones con mi esposo desde que llegamos al...\n",
      "26183    super bowl excelente servicio y ponen el súper...\n",
      "26029    rico y abundante excelente los desayunos muy a...\n",
      "16566    tratamiento de spa tuvimos un masaje de 80 min...\n",
      "Name: Features, dtype: object\n",
      "Test: 9730     gran cena con fuegos artificiales después! la ...\n",
      "7442     vacaciones en familia en general estupendo nos...\n",
      "25844    vacaciones gran lugar para comer! camarón azul...\n",
      "9565     lugar romantico para llevar a tu pareja buena ...\n",
      "718      ejecución inconsistente cocina cenamos dos vec...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['tokenization', 'stop_words', 'lemmatization']\n",
      "Train: 18594    lockdown abierto comida domicilio mejór mejór ...\n",
      "19690    Vacaciones esposo llegar hotel personal atento...\n",
      "26183    Super Bowl Excelente servicio poner súper tazó...\n",
      "26029    Rico abundante Excelente desayuno abundante va...\n",
      "16566    Tratamiento spa Tuvimos masaje 80 minuto parej...\n",
      "Name: Features, dtype: object\n",
      "Test: 9730     cenir fuego artificial cena fenomenal servicio...\n",
      "7442     Vacaciones familia general estupendo encaminar...\n",
      "25844    Vacaciones lugar comer Camarón azul pargo Cama...\n",
      "9565     lugar romantico pareja comida abundante precio...\n",
      "718      ejecución inconsistente cocina cenar período s...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['tokenization', 'stop_words', 'text_cleaning']\n",
      "Train: 18594    lockdown abiertos comida domicilio mejór mejór...\n",
      "19690    vacaciones esposo llegamos hotel personal aten...\n",
      "26183    super bowl excelente servicio ponen súper tazó...\n",
      "26029    rico abundante excelente desayunos abundantes ...\n",
      "16566    tratamiento spa tuvimos masaje 80 minutos pare...\n",
      "Name: Features, dtype: object\n",
      "Test: 9730     cena fuegos artificiales cena fenomenal servic...\n",
      "7442     vacaciones familia general estupendo encaminam...\n",
      "25844    vacaciones lugar comer camarón azul pargo cama...\n",
      "9565     lugar romantico pareja comida abundante precio...\n",
      "718      ejecución inconsistente cocina cenamos período...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['tokenization', 'lemmatization', 'text_cleaning']\n",
      "Train: 18594    el mejor también en el lockdown también ahora ...\n",
      "19690    vacación con mi esposo desde que llegar al hot...\n",
      "26183    super bowl excelente servicio y poner el súper...\n",
      "26029    rico y abundante excelente el desayuno mucho a...\n",
      "16566    tratamiento de spa tener uno masaje de 80 minu...\n",
      "Name: Features, dtype: object\n",
      "Test: 9730     gran cena con fuego artificial después el cena...\n",
      "7442     vacación en familia en general estupendo yo en...\n",
      "25844    vacación gran lugar para comer camarón azul pa...\n",
      "9565     lugar romantico para llevar a tu pareja buen c...\n",
      "718      ejecución inconsistente cocina cenar dos vez e...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['stop_words', 'lemmatization', 'text_cleaning']\n",
      "Train: 18594    el mejor también en el lockdown también ahora,...\n",
      "19690    vacaciones con mi esposo desde que llegamos al...\n",
      "26183    super bowl excelente servicio y ponen el súper...\n",
      "26029    rico y abundante excelente los desayunos muy a...\n",
      "16566    tratamiento de spa tuvimos un masaje de 80 min...\n",
      "Name: Features, dtype: object\n",
      "Test: 9730     gran cena con fuegos artificiales después! la ...\n",
      "7442     vacaciones en familia en general estupendo nos...\n",
      "25844    vacaciones gran lugar para comer! camarón azul...\n",
      "9565     lugar romantico para llevar a tu pareja buena ...\n",
      "718      ejecución inconsistente cocina cenamos dos vec...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning']\n",
      "Train: 18594    lockdown abierto comida domicilio mejór mejór ...\n",
      "19690    vacación esposo llegar hotel personal atento c...\n",
      "26183    super bowl excelente servicio poner súper tazó...\n",
      "26029    rico abundante excelente desayuno abundante va...\n",
      "16566    tratamiento spa tener masaje 80 minuto pareja ...\n",
      "Name: Features, dtype: object\n",
      "Test: 9730     cena fuego artificial cena fenomenal servicio ...\n",
      "7442     vacación familia general estupendo encaminar p...\n",
      "25844    vacación lugar comer camarón azul pargo camaró...\n",
      "9565     lugar romantico pareja comida abundante precio...\n",
      "718      ejecución inconsistente cocina cenar período s...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "procesos = ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning']\n",
    "combos = [list(c) for i in range(1,len(procesos)+1) for c in combinations(procesos, i)]\n",
    "\n",
    "resultados = {}\n",
    "\n",
    "for combo in combos:\n",
    "    print(f'Procesando combinacion: {combo}')\n",
    "    X_train_norm = X_train.apply(lambda x: NormalizarTexto(x, aplicar=combo))\n",
    "    X_test_norm = X_test.apply(lambda x: NormalizarTexto(x, aplicar=combo))\n",
    "    resultados[str(combo)] = (X_train_norm[:5], X_test_norm[:5])\n",
    "\n",
    "for combo, (tran_sample, test_sample) in resultados.items():\n",
    "    print(f'Combinación: {combo}')\n",
    "    print(f'Train: {tran_sample}')\n",
    "    print(f'Test: {test_sample}')\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-dMXaRIFuiWz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generando representación para combinación: ['tokenization']\n",
      "Combinacion: ['tokenization'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['tokenization'] + TF-IDF + SVD\n",
      "Preview: [[ 2.41621234e-01  4.79882854e-02  1.03664829e-02  4.66655714e-02\n",
      "   3.14382104e-02  6.57808836e-02 -4.58912796e-03  5.73569045e-02\n",
      "  -5.23138241e-02 -1.19605446e-02  2.64313301e-02 -2.19615443e-02\n",
      "  -4.27485833e-02  1.96757517e-02 -1.96125377e-02 -5.24901289e-02\n",
      "  -3.53814128e-02  3.95225358e-03  6.06474060e-02  1.47009924e-02\n",
      "   3.14056836e-03  3.15836327e-03 -1.35173881e-02 -4.18870576e-02\n",
      "  -8.00366930e-03 -1.79849260e-02 -1.44672043e-03  3.11428086e-02\n",
      "   5.92961558e-02 -4.15560592e-02  3.24284961e-02 -8.49741945e-03\n",
      "  -2.58111183e-02 -2.78987076e-03  9.72460418e-03 -3.95396227e-02\n",
      "  -3.34027652e-02  1.28572741e-02 -1.15395563e-02  9.83393461e-03\n",
      "  -2.45386310e-02  2.70439979e-02 -4.87379010e-02  1.06360208e-02\n",
      "   1.49697842e-03 -2.79574240e-02 -2.70037546e-02 -2.59019407e-02\n",
      "  -7.41181493e-03 -9.08617905e-03 -4.86434484e-02 -2.82861268e-02\n",
      "   1.27117258e-03 -2.21777420e-02 -1.09209742e-02 -4.14884841e-02\n",
      "   4.00049698e-02  2.12477673e-02 -3.68250158e-02  3.00783905e-02\n",
      "   1.47578912e-02  2.19556408e-02  3.47108352e-03 -1.47997732e-02\n",
      "   6.69168924e-03 -3.20458213e-02 -3.99112233e-03  2.68553692e-02\n",
      "  -2.38501056e-02  3.94852959e-02 -3.19965099e-02  1.44475393e-02\n",
      "  -2.53386004e-02  5.47389931e-02 -2.67019147e-02  1.27956976e-02\n",
      "  -2.78358320e-02  1.98024102e-02  1.12124170e-02 -6.77652991e-04\n",
      "   1.60275409e-02 -1.56393863e-02  1.30103630e-02  9.46872788e-03\n",
      "   2.90326039e-02 -3.10360768e-02  3.15586743e-02  4.33301240e-02\n",
      "   1.87505886e-02  7.77296017e-03 -2.83189474e-02 -6.59654497e-03\n",
      "  -2.33011163e-02  1.03917732e-02 -2.98826627e-02 -3.14293482e-03\n",
      "  -4.89317823e-02  1.19535894e-02 -4.04497027e-03 -8.93539093e-03]\n",
      " [ 2.34590850e-01  1.21139648e-01 -1.88173838e-02 -1.33454611e-01\n",
      "  -3.62098135e-02 -3.57937626e-02  5.54592973e-02 -1.32885316e-02\n",
      "  -5.94639734e-03 -6.27833044e-02 -7.04584211e-03 -8.64949254e-02\n",
      "  -2.96956344e-02 -5.48347789e-02  4.09832141e-02  5.39247764e-02\n",
      "  -8.22038829e-02  1.36114003e-02  3.51998159e-02  9.45058467e-03\n",
      "   5.64285822e-02  3.38743893e-02  7.14025671e-02  1.71024241e-02\n",
      "  -3.12788362e-02 -3.03635253e-02  4.92013965e-02 -2.42415623e-02\n",
      "  -6.18147520e-02  3.07793778e-02  3.29512027e-02  1.27988418e-03\n",
      "  -2.77480846e-02  7.21114137e-03  7.96495400e-02 -8.80215053e-02\n",
      "  -7.14329951e-02 -1.06129859e-01  6.13134595e-03  1.62774664e-02\n",
      "   3.13095236e-02  1.31972268e-02 -2.39190793e-02  5.24043540e-02\n",
      "   4.95330999e-02 -4.65479805e-03 -4.02827896e-02  3.40584938e-02\n",
      "   5.80803162e-02 -7.37002396e-02  1.53471920e-02  7.01539597e-02\n",
      "  -1.23865759e-02  5.10308921e-02  8.03420034e-03  4.36808347e-02\n",
      "   1.20260960e-02  3.01207855e-02 -4.60640486e-03  8.32903708e-02\n",
      "   1.11901526e-02 -3.55855467e-02  8.29171180e-02 -2.74540288e-02\n",
      "   2.11827887e-02  3.27216084e-02  1.23465503e-01 -1.25110890e-02\n",
      "   3.45226774e-03  2.38873716e-03  1.28999120e-02  4.41597275e-03\n",
      "   2.61288985e-02 -4.52318745e-03  4.07204620e-03  1.84595925e-02\n",
      "  -4.78415861e-03  4.12942190e-02  2.74877128e-02  1.65824619e-02\n",
      "   1.89644459e-02  3.71275733e-03  9.80780048e-03  5.79044686e-02\n",
      "  -4.11071045e-02  5.10516040e-02 -3.07621197e-03 -3.81248165e-03\n",
      "   2.09690460e-02 -7.06096387e-03  4.05571665e-02 -2.04408946e-02\n",
      "   4.39482955e-02  1.87807803e-02  1.09902843e-02 -2.37799756e-02\n",
      "  -3.75051753e-02  2.65266908e-02  3.08544244e-02  2.06643054e-02]\n",
      " [ 2.36225054e-01  2.60507455e-01 -2.90489921e-02  4.56626415e-02\n",
      "  -2.06013301e-02  4.67857153e-02 -8.13241985e-02  8.05080377e-02\n",
      "   4.05486741e-02  1.94122610e-03 -1.01573701e-01 -6.91276892e-02\n",
      "   3.22687353e-02  4.18380552e-02  6.70764663e-02  1.60511473e-02\n",
      "  -1.75239978e-02 -1.05458292e-01  2.03971341e-03 -4.14855053e-02\n",
      "   1.32918138e-02 -7.01713967e-03  7.48647270e-02 -6.04499445e-02\n",
      "  -1.98998608e-02 -5.06245744e-02  7.45605958e-03 -4.53414283e-02\n",
      "   6.02872652e-02 -1.25106855e-01 -5.18521834e-02 -4.56658870e-02\n",
      "   1.82161915e-02 -3.00322902e-02  2.86486519e-02 -1.15558762e-02\n",
      "   4.55265140e-02  9.98789888e-03  1.46862442e-02  1.30298730e-02\n",
      "   1.56988650e-03  4.96942557e-02 -8.30976437e-02  4.22834485e-03\n",
      "  -7.81476021e-02  2.96357472e-02  1.16324117e-03  1.31129142e-02\n",
      "   2.00071273e-02 -3.43870557e-02  4.99170777e-03  1.05918260e-02\n",
      "   4.09849041e-02 -5.53326939e-02  7.40029054e-02 -6.09869513e-02\n",
      "   5.25131894e-03  4.64568882e-02 -5.92258227e-03  3.87833297e-02\n",
      "  -1.37758577e-02 -9.52739862e-02  3.88278086e-02  6.24292727e-02\n",
      "   7.80474803e-02 -2.07216381e-02  6.51535502e-02 -5.16742553e-03\n",
      "  -4.71849633e-02  1.20127615e-02  3.09153058e-02  1.23841228e-03\n",
      "  -3.81848873e-02  2.71654893e-02 -1.04524420e-04  1.05145574e-03\n",
      "   3.43023082e-02  6.23996021e-03  3.53942560e-02 -6.70873176e-03\n",
      "   3.33726905e-02 -3.55097634e-02  5.17386107e-02  1.92596082e-02\n",
      "   2.46012955e-02  1.70013743e-02 -3.66299830e-02  1.10985606e-02\n",
      "  -8.72551629e-03 -6.22702024e-03 -3.46924039e-02  5.72960820e-02\n",
      "  -4.79615539e-02 -3.26323610e-02  2.93833096e-02 -1.16661728e-02\n",
      "  -5.22960829e-02 -7.18606952e-03  4.96710757e-02  1.07886082e-02]]\n",
      "\n",
      "Generando representación para combinación: ['stop_words']\n",
      "Combinacion: ['stop_words'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['stop_words'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['stop_words'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['stop_words'] + TF-IDF + SVD\n",
      "Preview: [[ 2.41619242e-01  4.79950654e-02  1.03681315e-02  4.66603145e-02\n",
      "   3.14542964e-02  6.57825337e-02 -4.62627862e-03  5.73435396e-02\n",
      "  -5.23228024e-02 -1.19331136e-02  2.64505766e-02 -2.20216646e-02\n",
      "  -4.27365231e-02  1.96771349e-02 -1.96127015e-02 -5.24894145e-02\n",
      "  -3.53642953e-02  3.93121722e-03  6.06509100e-02  1.47045796e-02\n",
      "   3.13350590e-03  3.16400814e-03 -1.35251049e-02 -4.19206973e-02\n",
      "  -7.75348065e-03 -1.80402983e-02 -1.49902394e-03  3.12366428e-02\n",
      "   5.92275814e-02 -4.15646255e-02  3.24289063e-02 -8.46860456e-03\n",
      "  -2.58658213e-02 -2.82328189e-03  9.87832681e-03 -3.94539645e-02\n",
      "  -3.33829944e-02  1.30111151e-02 -1.16054918e-02  9.82627756e-03\n",
      "  -2.46787912e-02  2.69791284e-02 -4.86552943e-02  1.05317239e-02\n",
      "   1.42748976e-03 -2.79302716e-02 -2.70151014e-02 -2.59716676e-02\n",
      "  -7.38715294e-03 -9.18832499e-03 -4.86948082e-02 -2.81214328e-02\n",
      "   1.37958575e-03 -2.21523740e-02 -1.07918440e-02 -4.13757843e-02\n",
      "   4.01539439e-02  2.11904361e-02 -3.70487529e-02  2.99653529e-02\n",
      "   1.44793316e-02  2.19179092e-02  3.31951496e-03 -1.48834365e-02\n",
      "   6.75171341e-03 -3.21621309e-02 -3.82727046e-03  2.67131018e-02\n",
      "  -2.38364606e-02  3.87747594e-02 -3.26960577e-02  1.45293275e-02\n",
      "  -2.56883249e-02  5.46497053e-02 -2.67362977e-02  1.31560779e-02\n",
      "  -2.77127045e-02  1.97043077e-02  1.11372663e-02 -7.25840933e-04\n",
      "   1.58364514e-02 -1.57494232e-02  1.30601179e-02  9.48717514e-03\n",
      "   2.83733550e-02 -3.16578799e-02  3.12112066e-02  4.39959019e-02\n",
      "   1.84012484e-02  7.76038781e-03 -2.81829628e-02 -6.23230757e-03\n",
      "  -2.29678373e-02  1.06987265e-02 -2.99054929e-02 -3.32962899e-03\n",
      "  -4.88869875e-02  1.21019359e-02 -4.18733904e-03 -9.15127473e-03]\n",
      " [ 2.34590359e-01  1.21140262e-01 -1.88253806e-02 -1.33450101e-01\n",
      "  -3.62426574e-02 -3.57662870e-02  5.54809093e-02 -1.33105856e-02\n",
      "  -5.98184632e-03 -6.27510281e-02 -7.08016169e-03 -8.64938130e-02\n",
      "  -2.96827936e-02 -5.48248705e-02  4.09965636e-02  5.39039224e-02\n",
      "  -8.21905993e-02  1.35836493e-02  3.52242915e-02  9.53871278e-03\n",
      "   5.64292243e-02  3.37886059e-02  7.14027307e-02  1.71161991e-02\n",
      "  -3.09543135e-02 -3.08727961e-02  4.91816500e-02 -2.43359839e-02\n",
      "  -6.17885662e-02  3.07497075e-02  3.29702325e-02  1.25991751e-03\n",
      "  -2.77705414e-02  7.08079102e-03  7.99492602e-02 -8.77578174e-02\n",
      "  -7.20358388e-02 -1.05810915e-01  6.05757450e-03  1.62614491e-02\n",
      "   3.13160763e-02  1.31562529e-02 -2.40418051e-02  5.21238502e-02\n",
      "   4.95074155e-02 -4.61370376e-03 -4.02697634e-02  3.40766424e-02\n",
      "   5.79087562e-02 -7.39690381e-02  1.57794327e-02  6.99447459e-02\n",
      "  -1.26492292e-02  5.09124581e-02  8.05284486e-03  4.37304382e-02\n",
      "   1.19717461e-02  3.01255828e-02 -4.91516186e-03  8.33489145e-02\n",
      "   1.08440246e-02 -3.54567272e-02  8.29203646e-02 -2.76297314e-02\n",
      "   2.12751016e-02  3.30424013e-02  1.23289107e-01 -1.27680549e-02\n",
      "   3.28136168e-03  2.66018580e-03  1.28134957e-02  4.47692339e-03\n",
      "   2.61166157e-02 -4.44436177e-03  4.23040808e-03  1.85048600e-02\n",
      "  -4.79030889e-03  4.12163911e-02  2.73453261e-02  1.67591060e-02\n",
      "   1.88330032e-02  3.42877718e-03  9.74433758e-03  5.78872770e-02\n",
      "  -4.02705218e-02  5.18015952e-02 -2.67487694e-03 -3.73048625e-03\n",
      "   2.08645293e-02 -7.69103909e-03  4.04323845e-02 -2.10889450e-02\n",
      "   4.39827033e-02  1.84297702e-02  1.12365266e-02 -2.41242219e-02\n",
      "  -3.75916381e-02  2.61879166e-02  3.04925505e-02  2.22269185e-02]\n",
      " [ 2.36226471e-01  2.60501622e-01 -2.90380471e-02  4.56639564e-02\n",
      "  -2.05916006e-02  4.67281859e-02 -8.13472459e-02  8.05139081e-02\n",
      "   4.05049230e-02  1.92559374e-03 -1.01602197e-01 -6.90932693e-02\n",
      "   3.22888745e-02  4.18511269e-02  6.70853256e-02  1.60375381e-02\n",
      "  -1.74741290e-02 -1.05454525e-01  2.02326451e-03 -4.15073045e-02\n",
      "   1.33419420e-02 -6.97690015e-03  7.48776884e-02 -6.05343623e-02\n",
      "  -1.92212532e-02 -5.07461716e-02  7.53135235e-03 -4.52012922e-02\n",
      "   6.04020388e-02 -1.25148952e-01 -5.17979148e-02 -4.56171459e-02\n",
      "   1.82940263e-02 -3.00659351e-02  2.86333983e-02 -1.14801086e-02\n",
      "   4.55714281e-02  9.71013388e-03  1.46230482e-02  1.30197422e-02\n",
      "   1.47688259e-03  4.96232602e-02 -8.31428573e-02  4.33145198e-03\n",
      "  -7.81290407e-02  2.97916385e-02  1.26270385e-03  1.31382373e-02\n",
      "   1.99054866e-02 -3.43667075e-02  5.11591563e-03  1.04330342e-02\n",
      "   4.14984459e-02 -5.50687133e-02  7.37039524e-02 -6.10167122e-02\n",
      "   5.16258056e-03  4.65849718e-02 -5.96666035e-03  3.87027303e-02\n",
      "  -1.37417038e-02 -9.53480562e-02  3.92178565e-02  6.23880945e-02\n",
      "   7.80757795e-02 -2.05969480e-02  6.50056203e-02 -5.37772333e-03\n",
      "  -4.72279655e-02  1.25842125e-02  3.07319574e-02  1.10925201e-03\n",
      "  -3.81723807e-02  2.71085825e-02 -3.62594353e-05  1.29739983e-03\n",
      "   3.42257752e-02  6.13858322e-03  3.52943857e-02 -6.71733518e-03\n",
      "   3.29437811e-02 -3.57672986e-02  5.20825500e-02  1.95974466e-02\n",
      "   2.45862186e-02  1.66557811e-02 -3.64347070e-02  1.11029140e-02\n",
      "  -9.03618005e-03 -5.74207934e-03 -3.38632404e-02  5.78169830e-02\n",
      "  -4.82410522e-02 -3.23206685e-02  2.92564100e-02 -1.20486059e-02\n",
      "  -5.24956481e-02 -7.32670871e-03  4.93235043e-02  1.09798708e-02]]\n",
      "\n",
      "Generando representación para combinación: ['lemmatization']\n",
      "Combinacion: ['lemmatization'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['lemmatization'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['lemmatization'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['lemmatization'] + TF-IDF + SVD\n",
      "Preview: [[ 2.41619242e-01  4.79950654e-02  1.03681315e-02  4.66603145e-02\n",
      "   3.14542964e-02  6.57825337e-02 -4.62627862e-03  5.73435396e-02\n",
      "  -5.23228024e-02 -1.19331136e-02  2.64505766e-02 -2.20216646e-02\n",
      "  -4.27365231e-02  1.96771349e-02 -1.96127015e-02 -5.24894145e-02\n",
      "  -3.53642953e-02  3.93121722e-03  6.06509100e-02  1.47045796e-02\n",
      "   3.13350590e-03  3.16400814e-03 -1.35251049e-02 -4.19206973e-02\n",
      "  -7.75348065e-03 -1.80402983e-02 -1.49902394e-03  3.12366428e-02\n",
      "   5.92275814e-02 -4.15646255e-02  3.24289063e-02 -8.46860456e-03\n",
      "  -2.58658213e-02 -2.82328189e-03  9.87832681e-03 -3.94539645e-02\n",
      "  -3.33829944e-02  1.30111151e-02 -1.16054918e-02  9.82627756e-03\n",
      "  -2.46787912e-02  2.69791284e-02 -4.86552943e-02  1.05317239e-02\n",
      "   1.42748976e-03 -2.79302716e-02 -2.70151014e-02 -2.59716676e-02\n",
      "  -7.38715294e-03 -9.18832499e-03 -4.86948082e-02 -2.81214328e-02\n",
      "   1.37958575e-03 -2.21523740e-02 -1.07918440e-02 -4.13757843e-02\n",
      "   4.01539439e-02  2.11904361e-02 -3.70487529e-02  2.99653529e-02\n",
      "   1.44793316e-02  2.19179092e-02  3.31951496e-03 -1.48834365e-02\n",
      "   6.75171341e-03 -3.21621309e-02 -3.82727046e-03  2.67131018e-02\n",
      "  -2.38364606e-02  3.87747594e-02 -3.26960577e-02  1.45293275e-02\n",
      "  -2.56883249e-02  5.46497053e-02 -2.67362977e-02  1.31560779e-02\n",
      "  -2.77127045e-02  1.97043077e-02  1.11372663e-02 -7.25840933e-04\n",
      "   1.58364514e-02 -1.57494232e-02  1.30601179e-02  9.48717514e-03\n",
      "   2.83733550e-02 -3.16578799e-02  3.12112066e-02  4.39959019e-02\n",
      "   1.84012484e-02  7.76038781e-03 -2.81829628e-02 -6.23230757e-03\n",
      "  -2.29678373e-02  1.06987265e-02 -2.99054929e-02 -3.32962899e-03\n",
      "  -4.88869875e-02  1.21019359e-02 -4.18733904e-03 -9.15127473e-03]\n",
      " [ 2.34590359e-01  1.21140262e-01 -1.88253806e-02 -1.33450101e-01\n",
      "  -3.62426574e-02 -3.57662870e-02  5.54809093e-02 -1.33105856e-02\n",
      "  -5.98184632e-03 -6.27510281e-02 -7.08016169e-03 -8.64938130e-02\n",
      "  -2.96827936e-02 -5.48248705e-02  4.09965636e-02  5.39039224e-02\n",
      "  -8.21905993e-02  1.35836493e-02  3.52242915e-02  9.53871278e-03\n",
      "   5.64292243e-02  3.37886059e-02  7.14027307e-02  1.71161991e-02\n",
      "  -3.09543135e-02 -3.08727961e-02  4.91816500e-02 -2.43359839e-02\n",
      "  -6.17885662e-02  3.07497075e-02  3.29702325e-02  1.25991751e-03\n",
      "  -2.77705414e-02  7.08079102e-03  7.99492602e-02 -8.77578174e-02\n",
      "  -7.20358388e-02 -1.05810915e-01  6.05757450e-03  1.62614491e-02\n",
      "   3.13160763e-02  1.31562529e-02 -2.40418051e-02  5.21238502e-02\n",
      "   4.95074155e-02 -4.61370376e-03 -4.02697634e-02  3.40766424e-02\n",
      "   5.79087562e-02 -7.39690381e-02  1.57794327e-02  6.99447459e-02\n",
      "  -1.26492292e-02  5.09124581e-02  8.05284486e-03  4.37304382e-02\n",
      "   1.19717461e-02  3.01255828e-02 -4.91516186e-03  8.33489145e-02\n",
      "   1.08440246e-02 -3.54567272e-02  8.29203646e-02 -2.76297314e-02\n",
      "   2.12751016e-02  3.30424013e-02  1.23289107e-01 -1.27680549e-02\n",
      "   3.28136168e-03  2.66018580e-03  1.28134957e-02  4.47692339e-03\n",
      "   2.61166157e-02 -4.44436177e-03  4.23040808e-03  1.85048600e-02\n",
      "  -4.79030889e-03  4.12163911e-02  2.73453261e-02  1.67591060e-02\n",
      "   1.88330032e-02  3.42877718e-03  9.74433758e-03  5.78872770e-02\n",
      "  -4.02705218e-02  5.18015952e-02 -2.67487694e-03 -3.73048625e-03\n",
      "   2.08645293e-02 -7.69103909e-03  4.04323845e-02 -2.10889450e-02\n",
      "   4.39827033e-02  1.84297702e-02  1.12365266e-02 -2.41242219e-02\n",
      "  -3.75916381e-02  2.61879166e-02  3.04925505e-02  2.22269185e-02]\n",
      " [ 2.36226471e-01  2.60501622e-01 -2.90380471e-02  4.56639564e-02\n",
      "  -2.05916006e-02  4.67281859e-02 -8.13472459e-02  8.05139081e-02\n",
      "   4.05049230e-02  1.92559374e-03 -1.01602197e-01 -6.90932693e-02\n",
      "   3.22888745e-02  4.18511269e-02  6.70853256e-02  1.60375381e-02\n",
      "  -1.74741290e-02 -1.05454525e-01  2.02326451e-03 -4.15073045e-02\n",
      "   1.33419420e-02 -6.97690015e-03  7.48776884e-02 -6.05343623e-02\n",
      "  -1.92212532e-02 -5.07461716e-02  7.53135235e-03 -4.52012922e-02\n",
      "   6.04020388e-02 -1.25148952e-01 -5.17979148e-02 -4.56171459e-02\n",
      "   1.82940263e-02 -3.00659351e-02  2.86333983e-02 -1.14801086e-02\n",
      "   4.55714281e-02  9.71013388e-03  1.46230482e-02  1.30197422e-02\n",
      "   1.47688259e-03  4.96232602e-02 -8.31428573e-02  4.33145198e-03\n",
      "  -7.81290407e-02  2.97916385e-02  1.26270385e-03  1.31382373e-02\n",
      "   1.99054866e-02 -3.43667075e-02  5.11591563e-03  1.04330342e-02\n",
      "   4.14984459e-02 -5.50687133e-02  7.37039524e-02 -6.10167122e-02\n",
      "   5.16258056e-03  4.65849718e-02 -5.96666035e-03  3.87027303e-02\n",
      "  -1.37417038e-02 -9.53480562e-02  3.92178565e-02  6.23880945e-02\n",
      "   7.80757795e-02 -2.05969480e-02  6.50056203e-02 -5.37772333e-03\n",
      "  -4.72279655e-02  1.25842125e-02  3.07319574e-02  1.10925201e-03\n",
      "  -3.81723807e-02  2.71085825e-02 -3.62594353e-05  1.29739983e-03\n",
      "   3.42257752e-02  6.13858322e-03  3.52943857e-02 -6.71733518e-03\n",
      "   3.29437811e-02 -3.57672986e-02  5.20825500e-02  1.95974466e-02\n",
      "   2.45862186e-02  1.66557811e-02 -3.64347070e-02  1.11029140e-02\n",
      "  -9.03618005e-03 -5.74207934e-03 -3.38632404e-02  5.78169830e-02\n",
      "  -4.82410522e-02 -3.23206685e-02  2.92564100e-02 -1.20486059e-02\n",
      "  -5.24956481e-02 -7.32670871e-03  4.93235043e-02  1.09798708e-02]]\n",
      "\n",
      "Generando representación para combinación: ['text_cleaning']\n",
      "Combinacion: ['text_cleaning'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['text_cleaning'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['text_cleaning'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['text_cleaning'] + TF-IDF + SVD\n",
      "Preview: [[ 2.41619242e-01  4.79950654e-02  1.03681315e-02  4.66603145e-02\n",
      "   3.14542964e-02  6.57825337e-02 -4.62627862e-03  5.73435396e-02\n",
      "  -5.23228024e-02 -1.19331136e-02  2.64505766e-02 -2.20216646e-02\n",
      "  -4.27365231e-02  1.96771349e-02 -1.96127015e-02 -5.24894145e-02\n",
      "  -3.53642953e-02  3.93121722e-03  6.06509100e-02  1.47045796e-02\n",
      "   3.13350590e-03  3.16400814e-03 -1.35251049e-02 -4.19206973e-02\n",
      "  -7.75348065e-03 -1.80402983e-02 -1.49902394e-03  3.12366428e-02\n",
      "   5.92275814e-02 -4.15646255e-02  3.24289063e-02 -8.46860456e-03\n",
      "  -2.58658213e-02 -2.82328189e-03  9.87832681e-03 -3.94539645e-02\n",
      "  -3.33829944e-02  1.30111151e-02 -1.16054918e-02  9.82627756e-03\n",
      "  -2.46787912e-02  2.69791284e-02 -4.86552943e-02  1.05317239e-02\n",
      "   1.42748976e-03 -2.79302716e-02 -2.70151014e-02 -2.59716676e-02\n",
      "  -7.38715294e-03 -9.18832499e-03 -4.86948082e-02 -2.81214328e-02\n",
      "   1.37958575e-03 -2.21523740e-02 -1.07918440e-02 -4.13757843e-02\n",
      "   4.01539439e-02  2.11904361e-02 -3.70487529e-02  2.99653529e-02\n",
      "   1.44793316e-02  2.19179092e-02  3.31951496e-03 -1.48834365e-02\n",
      "   6.75171341e-03 -3.21621309e-02 -3.82727046e-03  2.67131018e-02\n",
      "  -2.38364606e-02  3.87747594e-02 -3.26960577e-02  1.45293275e-02\n",
      "  -2.56883249e-02  5.46497053e-02 -2.67362977e-02  1.31560779e-02\n",
      "  -2.77127045e-02  1.97043077e-02  1.11372663e-02 -7.25840933e-04\n",
      "   1.58364514e-02 -1.57494232e-02  1.30601179e-02  9.48717514e-03\n",
      "   2.83733550e-02 -3.16578799e-02  3.12112066e-02  4.39959019e-02\n",
      "   1.84012484e-02  7.76038781e-03 -2.81829628e-02 -6.23230757e-03\n",
      "  -2.29678373e-02  1.06987265e-02 -2.99054929e-02 -3.32962899e-03\n",
      "  -4.88869875e-02  1.21019359e-02 -4.18733904e-03 -9.15127473e-03]\n",
      " [ 2.34590359e-01  1.21140262e-01 -1.88253806e-02 -1.33450101e-01\n",
      "  -3.62426574e-02 -3.57662870e-02  5.54809093e-02 -1.33105856e-02\n",
      "  -5.98184632e-03 -6.27510281e-02 -7.08016169e-03 -8.64938130e-02\n",
      "  -2.96827936e-02 -5.48248705e-02  4.09965636e-02  5.39039224e-02\n",
      "  -8.21905993e-02  1.35836493e-02  3.52242915e-02  9.53871278e-03\n",
      "   5.64292243e-02  3.37886059e-02  7.14027307e-02  1.71161991e-02\n",
      "  -3.09543135e-02 -3.08727961e-02  4.91816500e-02 -2.43359839e-02\n",
      "  -6.17885662e-02  3.07497075e-02  3.29702325e-02  1.25991751e-03\n",
      "  -2.77705414e-02  7.08079102e-03  7.99492602e-02 -8.77578174e-02\n",
      "  -7.20358388e-02 -1.05810915e-01  6.05757450e-03  1.62614491e-02\n",
      "   3.13160763e-02  1.31562529e-02 -2.40418051e-02  5.21238502e-02\n",
      "   4.95074155e-02 -4.61370376e-03 -4.02697634e-02  3.40766424e-02\n",
      "   5.79087562e-02 -7.39690381e-02  1.57794327e-02  6.99447459e-02\n",
      "  -1.26492292e-02  5.09124581e-02  8.05284486e-03  4.37304382e-02\n",
      "   1.19717461e-02  3.01255828e-02 -4.91516186e-03  8.33489145e-02\n",
      "   1.08440246e-02 -3.54567272e-02  8.29203646e-02 -2.76297314e-02\n",
      "   2.12751016e-02  3.30424013e-02  1.23289107e-01 -1.27680549e-02\n",
      "   3.28136168e-03  2.66018580e-03  1.28134957e-02  4.47692339e-03\n",
      "   2.61166157e-02 -4.44436177e-03  4.23040808e-03  1.85048600e-02\n",
      "  -4.79030889e-03  4.12163911e-02  2.73453261e-02  1.67591060e-02\n",
      "   1.88330032e-02  3.42877718e-03  9.74433758e-03  5.78872770e-02\n",
      "  -4.02705218e-02  5.18015952e-02 -2.67487694e-03 -3.73048625e-03\n",
      "   2.08645293e-02 -7.69103909e-03  4.04323845e-02 -2.10889450e-02\n",
      "   4.39827033e-02  1.84297702e-02  1.12365266e-02 -2.41242219e-02\n",
      "  -3.75916381e-02  2.61879166e-02  3.04925505e-02  2.22269185e-02]\n",
      " [ 2.36226471e-01  2.60501622e-01 -2.90380471e-02  4.56639564e-02\n",
      "  -2.05916006e-02  4.67281859e-02 -8.13472459e-02  8.05139081e-02\n",
      "   4.05049230e-02  1.92559374e-03 -1.01602197e-01 -6.90932693e-02\n",
      "   3.22888745e-02  4.18511269e-02  6.70853256e-02  1.60375381e-02\n",
      "  -1.74741290e-02 -1.05454525e-01  2.02326451e-03 -4.15073045e-02\n",
      "   1.33419420e-02 -6.97690015e-03  7.48776884e-02 -6.05343623e-02\n",
      "  -1.92212532e-02 -5.07461716e-02  7.53135235e-03 -4.52012922e-02\n",
      "   6.04020388e-02 -1.25148952e-01 -5.17979148e-02 -4.56171459e-02\n",
      "   1.82940263e-02 -3.00659351e-02  2.86333983e-02 -1.14801086e-02\n",
      "   4.55714281e-02  9.71013388e-03  1.46230482e-02  1.30197422e-02\n",
      "   1.47688259e-03  4.96232602e-02 -8.31428573e-02  4.33145198e-03\n",
      "  -7.81290407e-02  2.97916385e-02  1.26270385e-03  1.31382373e-02\n",
      "   1.99054866e-02 -3.43667075e-02  5.11591563e-03  1.04330342e-02\n",
      "   4.14984459e-02 -5.50687133e-02  7.37039524e-02 -6.10167122e-02\n",
      "   5.16258056e-03  4.65849718e-02 -5.96666035e-03  3.87027303e-02\n",
      "  -1.37417038e-02 -9.53480562e-02  3.92178565e-02  6.23880945e-02\n",
      "   7.80757795e-02 -2.05969480e-02  6.50056203e-02 -5.37772333e-03\n",
      "  -4.72279655e-02  1.25842125e-02  3.07319574e-02  1.10925201e-03\n",
      "  -3.81723807e-02  2.71085825e-02 -3.62594353e-05  1.29739983e-03\n",
      "   3.42257752e-02  6.13858322e-03  3.52943857e-02 -6.71733518e-03\n",
      "   3.29437811e-02 -3.57672986e-02  5.20825500e-02  1.95974466e-02\n",
      "   2.45862186e-02  1.66557811e-02 -3.64347070e-02  1.11029140e-02\n",
      "  -9.03618005e-03 -5.74207934e-03 -3.38632404e-02  5.78169830e-02\n",
      "  -4.82410522e-02 -3.23206685e-02  2.92564100e-02 -1.20486059e-02\n",
      "  -5.24956481e-02 -7.32670871e-03  4.93235043e-02  1.09798708e-02]]\n",
      "\n",
      "Generando representación para combinación: ['tokenization', 'stop_words']\n",
      "Combinacion: ['tokenization', 'stop_words'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'stop_words'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'stop_words'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['tokenization', 'stop_words'] + TF-IDF + SVD\n",
      "Preview: [[ 1.19097448e-01  3.67229368e-02  9.67755576e-02  2.18563027e-02\n",
      "   6.50681074e-02 -5.29518126e-02 -1.52124542e-02 -2.45868988e-02\n",
      "  -2.25107015e-02  1.28439469e-02 -1.25698393e-02  5.13737696e-02\n",
      "  -1.11586954e-02 -1.13931152e-02 -6.98776966e-03  3.84060009e-03\n",
      "   6.87766918e-03 -9.69474136e-03  4.58738162e-03  1.29007233e-02\n",
      "  -6.26309163e-04 -1.75743119e-02  4.75369499e-02  2.91206644e-02\n",
      "   2.80915154e-02  9.07895158e-03 -2.21530628e-02 -1.94888560e-03\n",
      "  -3.78296843e-02  4.85725068e-02  1.40669066e-02 -4.88947881e-02\n",
      "   1.51012393e-02  9.16765138e-03  4.40113834e-03 -5.25550472e-02\n",
      "   2.39559979e-02 -1.65337677e-02  4.12762005e-02  1.22759760e-02\n",
      "   8.29970062e-03 -2.01802357e-02 -2.35914490e-03 -1.86962612e-02\n",
      "   3.78174280e-03  4.18131113e-03  2.50682838e-03  1.96353662e-02\n",
      "  -2.86975543e-02  1.44376401e-02 -1.33731968e-02  7.31115283e-03\n",
      "   1.80283267e-03  6.13081963e-03 -6.35187145e-02 -2.12470356e-03\n",
      "  -6.67468544e-03 -7.64290443e-03  1.54338779e-03 -3.91276536e-02\n",
      "   5.00832351e-02  3.48053715e-03 -2.72900155e-02 -6.88942901e-03\n",
      "   4.67068143e-04 -2.31838036e-02  5.58112945e-03  2.47333669e-02\n",
      "  -2.91617516e-02 -7.22504663e-03  3.70147086e-02 -1.52795914e-02\n",
      "   1.55792028e-02 -3.93402965e-02  1.44290093e-02 -8.42222499e-03\n",
      "  -2.80573504e-02 -1.58554712e-02 -2.46859556e-02  2.92430808e-02\n",
      "   2.65578658e-02 -1.85334243e-02  9.55146935e-03  8.67144605e-03\n",
      "  -4.07381650e-03 -1.03645972e-02  2.71385313e-02  6.94824122e-02\n",
      "   2.80104017e-03 -8.39798981e-03 -2.13085830e-02  2.22180984e-02\n",
      "   2.49615707e-02  8.46762303e-04 -2.26740966e-02  3.82666585e-02\n",
      "   5.35272786e-05 -1.38560565e-03 -3.12327235e-03 -3.65339594e-02]\n",
      " [ 1.47990817e-01  7.37032352e-02 -1.15698344e-01 -8.47721227e-02\n",
      "  -4.04998900e-02 -1.03750149e-01 -3.02397448e-02  4.97741464e-02\n",
      "  -1.12783247e-01 -5.28036605e-03  4.20150790e-02 -1.74485727e-02\n",
      "   1.91667789e-02 -1.13633882e-02 -2.21054448e-02  2.93685681e-02\n",
      "   2.64551964e-02  1.65222751e-02  7.00320733e-03 -2.77523018e-02\n",
      "   4.89413298e-02 -7.95812147e-02  4.12673764e-02  3.28929781e-02\n",
      "  -6.17959205e-04  5.53357722e-02 -3.91905927e-02 -1.55201928e-02\n",
      "  -2.94179776e-02  2.46878184e-02  6.30908324e-03  4.56694420e-02\n",
      "   6.18928119e-03  3.04343117e-02 -4.65887893e-03  2.32140810e-02\n",
      "   4.98042547e-02 -7.22015083e-03  1.30649130e-01  1.06933244e-03\n",
      "  -3.42006179e-02  5.10392853e-02 -1.10841754e-03  5.24734986e-02\n",
      "  -7.80257505e-02 -2.94189061e-02 -1.77499361e-03 -1.31559942e-02\n",
      "   1.48381224e-02 -4.65613915e-02 -2.75163397e-02 -2.12903584e-02\n",
      "   4.85341190e-03 -4.99873795e-04  3.88691653e-02 -2.37769024e-02\n",
      "  -3.28306125e-02  1.24571031e-02 -4.48296148e-02  1.10238799e-02\n",
      "   2.78106596e-02 -4.59032577e-02  4.30451519e-02 -1.74228337e-02\n",
      "   5.76201962e-03 -3.42500246e-02 -2.35048351e-02 -2.48308225e-02\n",
      "   6.55926791e-03  1.78163184e-02  3.04492414e-02 -2.67151512e-02\n",
      "   1.94873928e-03 -5.57211359e-02 -3.14783571e-02  1.81840277e-02\n",
      "   2.53521500e-02 -2.50989324e-02 -2.38818013e-02  7.07289470e-02\n",
      "  -3.92543663e-02 -3.07420270e-02  5.40677554e-02 -3.56956197e-02\n",
      "   8.23815359e-03  4.50239850e-02  3.15835982e-02 -2.74417956e-02\n",
      "   1.14678639e-02 -1.23295094e-02  9.14430101e-03  6.78300630e-03\n",
      "  -4.21778489e-02 -3.39545664e-03 -1.95720595e-02 -3.85365303e-02\n",
      "  -2.79665606e-02 -1.84258900e-02  2.14420735e-02  2.17523646e-02]\n",
      " [ 2.14115240e-01  2.23300048e-01 -1.68747880e-02  7.78677095e-02\n",
      "  -1.69868503e-02  8.21518463e-02 -7.58462162e-02  5.41651868e-02\n",
      "   2.25650904e-02  1.97443416e-02  5.08355100e-02  7.27061564e-02\n",
      "  -3.01177606e-02 -2.22942732e-02 -4.83153962e-02 -8.72049127e-03\n",
      "   6.44490017e-03  1.49287540e-02 -3.26506758e-02 -4.66579824e-02\n",
      "  -3.31386382e-02 -1.56852970e-02  1.21403627e-01  3.15521705e-02\n",
      "   7.52418616e-02 -9.36332447e-03 -7.17940027e-03 -5.87198263e-02\n",
      "   7.89793539e-02  6.05058312e-02  6.20306822e-02 -4.48506765e-03\n",
      "   1.88582725e-02  9.67822030e-02 -9.44264270e-02  8.93828374e-03\n",
      "   4.97723807e-02 -4.30676921e-02  2.63925520e-02  3.95193322e-02\n",
      "  -1.45065801e-02  6.54939688e-02 -4.88633941e-03  7.50874257e-04\n",
      "  -6.37120584e-02 -5.19625835e-02 -8.33138317e-02  7.70051109e-02\n",
      "  -5.61760136e-02 -2.25516630e-02 -1.54571614e-02  1.00158939e-02\n",
      "   8.39395698e-02 -8.11670232e-03 -2.96897557e-02 -2.46333327e-02\n",
      "  -3.72046269e-02  1.22664925e-02  7.65783261e-02 -2.30717596e-02\n",
      "   3.73509826e-02  1.93883341e-02  3.61555775e-02 -2.59051080e-02\n",
      "   6.74321880e-02  3.97706682e-02 -1.03119584e-02 -3.63118320e-02\n",
      "   1.46657705e-02  1.32668518e-02 -5.85897828e-02 -5.37416774e-03\n",
      "  -1.64232715e-02  2.31787726e-02 -5.17491689e-02  1.75296486e-03\n",
      "  -1.42053569e-02  3.18527065e-02  1.42032984e-02 -1.70890859e-02\n",
      "   2.00473083e-02 -5.78291927e-02 -3.09134651e-03 -7.93240876e-02\n",
      "  -2.37998904e-02 -6.76896080e-02 -1.53684692e-02 -1.96933882e-02\n",
      "  -8.88906077e-03  1.23086638e-02 -2.23940805e-02 -1.30584326e-02\n",
      "   7.66565406e-04  2.09734599e-02 -1.52154132e-03  1.80560040e-03\n",
      "  -6.24405223e-04  4.12470430e-03 -3.96658992e-02  2.17845326e-02]]\n",
      "\n",
      "Generando representación para combinación: ['tokenization', 'lemmatization']\n",
      "Combinacion: ['tokenization', 'lemmatization'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'lemmatization'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'lemmatization'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['tokenization', 'lemmatization'] + TF-IDF + SVD\n",
      "Preview: [[ 2.75200997e-01  3.84991567e-02  3.59691265e-02 -7.05004901e-03\n",
      "  -2.59766986e-02 -5.04596649e-02  9.90203514e-02 -1.81587612e-02\n",
      "   5.27963638e-02 -4.75100317e-03 -1.36244793e-02  8.08561144e-02\n",
      "   2.97909956e-02  7.16845424e-03  2.71044547e-02 -2.65904436e-02\n",
      "   1.19049885e-02 -1.99166952e-02 -3.27717023e-02  1.23476801e-02\n",
      "   2.94627143e-02  2.24280015e-02  2.20447763e-02  3.55591729e-03\n",
      "   3.73728271e-02  1.24939395e-02 -4.19378937e-02  4.40272402e-02\n",
      "  -3.37584744e-03 -3.77290442e-03 -1.66846311e-02 -6.39150290e-02\n",
      "   2.00328268e-02  3.53397252e-02 -2.07201916e-02  2.91905813e-02\n",
      "  -9.50872059e-03 -3.01962710e-03  6.36121221e-03  3.04418433e-02\n",
      "  -2.36464150e-02  1.21009403e-02  3.01966376e-02 -1.29426070e-02\n",
      "  -3.37065706e-02 -9.84819771e-03 -5.91522644e-02 -5.48828028e-02\n",
      "  -2.77584163e-02 -4.75833429e-02  2.54762070e-03  1.87289579e-02\n",
      "   8.03979775e-03 -2.29454666e-02 -2.61660724e-02 -3.83465490e-02\n",
      "   1.36134414e-02 -1.04954896e-02  1.58943292e-02 -4.47423548e-02\n",
      "   1.46541069e-03 -2.25590428e-02  1.45617977e-02 -3.29987695e-02\n",
      "  -3.13816253e-02 -3.11645886e-02  3.57688008e-02 -4.57982379e-02\n",
      "  -2.84739298e-02  1.81996443e-02  9.04062486e-03 -3.28072297e-02\n",
      "  -3.07773276e-02 -2.11425393e-02  1.71179122e-02 -7.66994085e-05\n",
      "  -5.18544806e-04  1.30969738e-02 -1.49272995e-02 -9.84242305e-04\n",
      "  -2.73929008e-02 -6.52853753e-03 -3.83967907e-02 -3.19175586e-02\n",
      "   2.05656578e-03  1.02755926e-02 -2.31253063e-02  1.44470115e-02\n",
      "  -2.40374778e-02 -1.15927861e-02  1.29547642e-02 -2.74012865e-03\n",
      "   1.43093423e-02  1.48078348e-03  6.82666705e-02  1.59266156e-02\n",
      "   2.41053059e-02  1.88919890e-02  1.71123311e-02 -8.18453269e-03]\n",
      " [ 2.60138014e-01  1.11660034e-01  1.09267100e-02 -1.67910316e-01\n",
      "   6.20532630e-02  4.73661058e-04 -5.07484839e-02  4.93978550e-02\n",
      "   2.18005755e-02 -9.11554437e-02  3.01230118e-04  2.38499512e-02\n",
      "  -4.54515085e-02 -9.34261353e-02  1.81781231e-02 -4.71907187e-02\n",
      "   4.77871090e-02 -1.05893615e-02 -1.10481954e-02  4.49544877e-02\n",
      "  -2.26608281e-02  2.61646530e-02  1.59180137e-02 -5.58661163e-02\n",
      "  -1.83033931e-02 -4.31051436e-02  2.16935422e-02  2.81580019e-02\n",
      "   1.57839658e-03  8.88029161e-02  4.70061232e-02  2.66121335e-02\n",
      "  -7.78152873e-02  1.52864326e-02 -6.55812513e-02 -1.87326192e-02\n",
      "  -2.09841306e-03  7.78360175e-02 -1.35467346e-02  1.26930751e-01\n",
      "   1.63518772e-02  1.79508993e-02  4.16386465e-02 -2.15326119e-02\n",
      "  -6.51575985e-02 -7.68396371e-02  7.32841783e-02 -6.27667018e-02\n",
      "   1.13106645e-02 -6.78283706e-03  1.99363633e-02 -4.20978776e-02\n",
      "  -2.01806589e-02  5.06363022e-02 -2.61698763e-02  4.87908550e-03\n",
      "   1.27934819e-02  4.73932741e-02  2.29881162e-02  2.22048567e-02\n",
      "   8.44289743e-02  1.96103713e-02 -3.11499885e-02  3.03154644e-02\n",
      "  -5.81164042e-02 -5.33427717e-02  5.83837442e-02  1.75353150e-02\n",
      "  -2.65252136e-02 -4.03145696e-03 -4.65268238e-02 -2.02556492e-02\n",
      "   2.56933882e-03  1.02549574e-03  6.84124385e-03  3.51639666e-02\n",
      "   6.24564720e-02 -7.37677184e-02 -7.69937404e-03 -1.97624627e-02\n",
      "  -8.84026315e-03  2.96149817e-02 -1.02402569e-03 -2.60086764e-02\n",
      "  -5.77772619e-02  5.20251285e-02  2.95095073e-02  1.47008662e-02\n",
      "  -1.52152898e-02  1.18801526e-02  1.91374550e-02  1.10369216e-02\n",
      "   1.04800814e-02  9.19297222e-03  4.31868952e-02 -4.46711815e-02\n",
      "   1.92230915e-02 -1.46593040e-03 -1.45178495e-02 -2.02886646e-03]\n",
      " [ 3.06031329e-01  2.90870590e-01 -7.11904115e-03 -2.67217625e-03\n",
      "   1.56756631e-02 -1.81224223e-02  7.97558765e-02 -8.93042823e-02\n",
      "  -5.30441980e-02  1.61413712e-02  7.75910614e-02 -1.17809222e-01\n",
      "  -3.73706182e-02 -4.55071171e-02 -2.94009692e-02  7.32219424e-02\n",
      "   9.49797529e-02  3.62935383e-03 -4.20047407e-02  8.19543065e-02\n",
      "   1.11902572e-02 -7.07108906e-03  1.04456496e-01  8.42840215e-03\n",
      "  -1.83772262e-02  6.43666180e-04 -1.34223979e-02 -1.07625924e-01\n",
      "   1.75835586e-02 -1.42294191e-02  4.89400518e-02  1.19061396e-02\n",
      "   3.93195947e-02 -5.83244429e-02 -2.38209784e-02  6.26311625e-02\n",
      "  -7.54369549e-02 -5.03590640e-02  1.85468200e-02 -1.87982959e-03\n",
      "   1.92633200e-03  1.39639063e-02  7.02814251e-02  3.03649023e-02\n",
      "  -2.49947777e-02 -8.81793164e-02 -2.31906519e-02 -4.63359242e-02\n",
      "  -1.47851547e-02 -8.43227879e-02  5.37224677e-02 -4.53350325e-02\n",
      "  -9.76223853e-02 -3.53696124e-02 -8.66067163e-03 -1.57937515e-02\n",
      "   9.84718760e-02 -6.85905884e-02 -2.19285140e-02  1.68832137e-02\n",
      "   1.05006597e-02  2.82287085e-02  7.53559833e-03  1.72026265e-02\n",
      "  -4.44649838e-02 -3.05092066e-02  5.99329420e-02 -4.15823757e-02\n",
      "   3.23760883e-02 -3.75824159e-02  4.96844885e-03  1.06017284e-02\n",
      "  -2.45771138e-02 -3.84751314e-03  1.05615713e-01  8.30065990e-03\n",
      "  -2.20266183e-02  2.14496213e-02 -1.82268594e-02  3.40359646e-02\n",
      "   7.77980145e-02 -2.66224728e-02  7.26868527e-02 -5.65936020e-02\n",
      "   1.85968033e-02 -1.39800798e-02  7.42747325e-03 -1.51184165e-02\n",
      "  -1.66991310e-03 -5.04621242e-02  1.74929246e-02 -5.92990081e-03\n",
      "   4.72567806e-02  5.80386718e-02  3.16894284e-02  1.94245043e-02\n",
      "  -1.78627028e-02 -7.42976034e-02  4.11567982e-02 -6.57907916e-02]]\n",
      "\n",
      "Generando representación para combinación: ['tokenization', 'text_cleaning']\n",
      "Combinacion: ['tokenization', 'text_cleaning'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'text_cleaning'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'text_cleaning'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['tokenization', 'text_cleaning'] + TF-IDF + SVD\n",
      "Preview: [[ 2.41621128e-01  4.79883765e-02  1.03658395e-02  4.66660834e-02\n",
      "   3.14375607e-02  6.57812806e-02 -4.58907753e-03  5.73571292e-02\n",
      "  -5.23139870e-02 -1.19610777e-02  2.64330892e-02 -2.19537894e-02\n",
      "  -4.27507076e-02  1.96757068e-02 -1.96126663e-02 -5.24894247e-02\n",
      "  -3.53837782e-02  3.95172699e-03  6.06471940e-02  1.46999936e-02\n",
      "   3.14098495e-03  3.15918900e-03 -1.35179085e-02 -4.18822190e-02\n",
      "  -8.02240662e-03 -1.79831542e-02 -1.44407941e-03  3.11470809e-02\n",
      "   5.92932606e-02 -4.15554957e-02  3.24289592e-02 -8.50032348e-03\n",
      "  -2.58098955e-02 -2.78518627e-03  9.71749433e-03 -3.95437219e-02\n",
      "  -3.34018074e-02  1.28563318e-02 -1.15387027e-02  9.83141741e-03\n",
      "  -2.45413226e-02  2.70386531e-02 -4.87383137e-02  1.06392846e-02\n",
      "   1.49632836e-03 -2.79678526e-02 -2.70007298e-02 -2.58939275e-02\n",
      "  -7.41421343e-03 -9.09106552e-03 -4.86431766e-02  2.82859692e-02\n",
      "   1.27281210e-03 -2.21726691e-02 -1.09333892e-02 -4.15009088e-02\n",
      "   3.99927072e-02  2.12486133e-02 -3.68177683e-02  3.00869784e-02\n",
      "   1.47514277e-02  2.19586337e-02  3.47151386e-03 -1.48034501e-02\n",
      "   6.69195952e-03 -3.20457086e-02 -3.99921544e-03  2.68657852e-02\n",
      "  -2.38627594e-02  3.94665680e-02 -3.20066514e-02  1.44540843e-02\n",
      "  -2.53392470e-02  5.47384728e-02 -2.66964706e-02  1.28036714e-02\n",
      "  -2.78407975e-02  1.97941160e-02  1.12043344e-02 -6.74763297e-04\n",
      "   1.60321483e-02 -1.56367308e-02  1.30208219e-02  9.47114230e-03\n",
      "   2.90159898e-02 -3.10577718e-02  3.15711326e-02  4.33064470e-02\n",
      "   1.87919296e-02  7.76971777e-03 -2.83209417e-02 -6.62291956e-03\n",
      "  -2.32956999e-02  1.03645035e-02 -2.98773345e-02 -3.14127222e-03\n",
      "  -4.89333030e-02  1.19284379e-02 -4.06329427e-03 -8.93662612e-03]\n",
      " [ 2.34590789e-01  1.21139677e-01 -1.88167394e-02 -1.33454209e-01\n",
      "  -3.62076018e-02 -3.57953513e-02  5.54592173e-02 -1.32882984e-02\n",
      "  -5.94828585e-03 -6.27841859e-02 -7.03451132e-03 -8.64934732e-02\n",
      "  -2.97006827e-02 -5.48346011e-02  4.09834984e-02  5.39256331e-02\n",
      "  -8.22031551e-02  1.36155959e-02  3.51961250e-02  9.45066511e-03\n",
      "   5.64294764e-02  3.38728767e-02  7.14025924e-02  1.71072750e-02\n",
      "  -3.13035229e-02 -3.03374138e-02  4.92006899e-02 -2.42486595e-02\n",
      "  -6.18131405e-02  3.07790741e-02  3.29497714e-02  1.27857382e-03\n",
      "  -2.77484035e-02  7.21599305e-03  7.96354358e-02 -8.80355057e-02\n",
      "  -7.14316792e-02 -1.06130677e-01  6.13717975e-03  1.62920352e-02\n",
      "   3.13093313e-02  1.31911505e-02 -2.39194205e-02  5.24063157e-02\n",
      "   4.95258689e-02 -4.66124327e-03 -4.02786287e-02  3.40659835e-02\n",
      "   5.80920864e-02 -7.36878027e-02  1.53550658e-02 -7.01456134e-02\n",
      "  -1.23683395e-02  5.10435069e-02  8.02811333e-03  4.36755684e-02\n",
      "   1.20510050e-02  3.01209473e-02 -4.58170351e-03  8.32880975e-02\n",
      "   1.11873774e-02 -3.55917528e-02  8.29195325e-02 -2.74505411e-02\n",
      "   2.11842503e-02  3.27178043e-02  1.23470329e-01 -1.24903073e-02\n",
      "   3.45146755e-03  2.40370834e-03  1.28946612e-02  4.40920663e-03\n",
      "   2.61288450e-02 -4.52201244e-03  4.06815189e-03  1.84607508e-02\n",
      "  -4.78556601e-03  4.12997498e-02  2.74748535e-02  1.65894378e-02\n",
      "   1.89569898e-02  3.71035962e-03  9.80546319e-03  5.78999616e-02\n",
      "  -4.10724647e-02  5.10858674e-02 -3.08801843e-03 -3.81259214e-03\n",
      "   2.09551004e-02 -7.07707455e-03  4.05796247e-02 -2.04095803e-02\n",
      "   4.39357109e-02  1.88259029e-02  1.09891092e-02 -2.37770502e-02\n",
      "  -3.75241368e-02  2.65186787e-02  3.08229965e-02  2.06385837e-02]\n",
      " [ 2.36224975e-01  2.60507513e-01 -2.90490011e-02  4.56623216e-02\n",
      "  -2.06029139e-02  4.67870838e-02 -8.13234657e-02  8.05072549e-02\n",
      "   4.05497399e-02  1.94043856e-03 -1.01563821e-01 -6.91419245e-02\n",
      "   3.22654371e-02  4.18386270e-02  6.70771690e-02  1.60520184e-02\n",
      "  -1.75313501e-02 -1.05456925e-01  2.03611298e-03 -4.14863691e-02\n",
      "   1.32932559e-02 -7.01701791e-03  7.48646346e-02 -6.04458535e-02\n",
      "  -1.99463270e-02 -5.06126892e-02  7.45380707e-03 -4.53368466e-02\n",
      "   6.02907053e-02 -1.25104590e-01 -5.18512168e-02 -4.56685400e-02\n",
      "   1.82172797e-02 -3.00362252e-02  2.86485279e-02 -1.15598067e-02\n",
      "   4.55250490e-02  9.99154768e-03  1.46852973e-02  1.30329045e-02\n",
      "   1.56721512e-03  4.96951722e-02 -8.30971395e-02  4.22410515e-03\n",
      "  -7.81484361e-02  2.96347579e-02  1.15533100e-03  1.31085848e-02\n",
      "   2.00163227e-02 -3.43852046e-02  4.99632073e-03 -1.05938037e-02\n",
      "   4.09736567e-02 -5.53490573e-02  7.39884006e-02 -6.09905418e-02\n",
      "   5.25255311e-03  4.64630466e-02 -5.90893046e-03  3.87803410e-02\n",
      "  -1.37685773e-02 -9.52714775e-02  3.88234417e-02  6.24371972e-02\n",
      "   7.80494004e-02 -2.07199558e-02  6.51537893e-02 -5.15077367e-03\n",
      "  -4.71961928e-02  1.20063743e-02  3.09207833e-02  1.24623446e-03\n",
      "  -3.81821121e-02  2.71694976e-02 -1.04809141e-04  1.06297715e-03\n",
      "   3.43012178e-02  6.24277152e-03  3.53948189e-02 -6.69929038e-03\n",
      "   3.33657667e-02 -3.55065303e-02  5.17375675e-02  1.92638134e-02\n",
      "   2.46116494e-02  1.69775100e-02 -3.66266340e-02  1.11152747e-02\n",
      "  -8.71733714e-03 -6.22347924e-03 -3.47246543e-02  5.72530733e-02\n",
      "  -4.79405183e-02 -3.26868946e-02  2.93887532e-02 -1.16525926e-02\n",
      "  -5.23048380e-02 -7.17751311e-03  4.96641029e-02  1.08170967e-02]]\n",
      "\n",
      "Generando representación para combinación: ['stop_words', 'lemmatization']\n",
      "Combinacion: ['stop_words', 'lemmatization'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['stop_words', 'lemmatization'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['stop_words', 'lemmatization'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['stop_words', 'lemmatization'] + TF-IDF + SVD\n",
      "Preview: [[ 2.41619242e-01  4.79950654e-02  1.03681315e-02  4.66603145e-02\n",
      "   3.14542964e-02  6.57825337e-02 -4.62627862e-03  5.73435396e-02\n",
      "  -5.23228024e-02 -1.19331136e-02  2.64505766e-02 -2.20216646e-02\n",
      "  -4.27365231e-02  1.96771349e-02 -1.96127015e-02 -5.24894145e-02\n",
      "  -3.53642953e-02  3.93121722e-03  6.06509100e-02  1.47045796e-02\n",
      "   3.13350590e-03  3.16400814e-03 -1.35251049e-02 -4.19206973e-02\n",
      "  -7.75348065e-03 -1.80402983e-02 -1.49902394e-03  3.12366428e-02\n",
      "   5.92275814e-02 -4.15646255e-02  3.24289063e-02 -8.46860456e-03\n",
      "  -2.58658213e-02 -2.82328189e-03  9.87832681e-03 -3.94539645e-02\n",
      "  -3.33829944e-02  1.30111151e-02 -1.16054918e-02  9.82627756e-03\n",
      "  -2.46787912e-02  2.69791284e-02 -4.86552943e-02  1.05317239e-02\n",
      "   1.42748976e-03 -2.79302716e-02 -2.70151014e-02 -2.59716676e-02\n",
      "  -7.38715294e-03 -9.18832499e-03 -4.86948082e-02 -2.81214328e-02\n",
      "   1.37958575e-03 -2.21523740e-02 -1.07918440e-02 -4.13757843e-02\n",
      "   4.01539439e-02  2.11904361e-02 -3.70487529e-02  2.99653529e-02\n",
      "   1.44793316e-02  2.19179092e-02  3.31951496e-03 -1.48834365e-02\n",
      "   6.75171341e-03 -3.21621309e-02 -3.82727046e-03  2.67131018e-02\n",
      "  -2.38364606e-02  3.87747594e-02 -3.26960577e-02  1.45293275e-02\n",
      "  -2.56883249e-02  5.46497053e-02 -2.67362977e-02  1.31560779e-02\n",
      "  -2.77127045e-02  1.97043077e-02  1.11372663e-02 -7.25840933e-04\n",
      "   1.58364514e-02 -1.57494232e-02  1.30601179e-02  9.48717514e-03\n",
      "   2.83733550e-02 -3.16578799e-02  3.12112066e-02  4.39959019e-02\n",
      "   1.84012484e-02  7.76038781e-03 -2.81829628e-02 -6.23230757e-03\n",
      "  -2.29678373e-02  1.06987265e-02 -2.99054929e-02 -3.32962899e-03\n",
      "  -4.88869875e-02  1.21019359e-02 -4.18733904e-03 -9.15127473e-03]\n",
      " [ 2.34590359e-01  1.21140262e-01 -1.88253806e-02 -1.33450101e-01\n",
      "  -3.62426574e-02 -3.57662870e-02  5.54809093e-02 -1.33105856e-02\n",
      "  -5.98184632e-03 -6.27510281e-02 -7.08016169e-03 -8.64938130e-02\n",
      "  -2.96827936e-02 -5.48248705e-02  4.09965636e-02  5.39039224e-02\n",
      "  -8.21905993e-02  1.35836493e-02  3.52242915e-02  9.53871278e-03\n",
      "   5.64292243e-02  3.37886059e-02  7.14027307e-02  1.71161991e-02\n",
      "  -3.09543135e-02 -3.08727961e-02  4.91816500e-02 -2.43359839e-02\n",
      "  -6.17885662e-02  3.07497075e-02  3.29702325e-02  1.25991751e-03\n",
      "  -2.77705414e-02  7.08079102e-03  7.99492602e-02 -8.77578174e-02\n",
      "  -7.20358388e-02 -1.05810915e-01  6.05757450e-03  1.62614491e-02\n",
      "   3.13160763e-02  1.31562529e-02 -2.40418051e-02  5.21238502e-02\n",
      "   4.95074155e-02 -4.61370376e-03 -4.02697634e-02  3.40766424e-02\n",
      "   5.79087562e-02 -7.39690381e-02  1.57794327e-02  6.99447459e-02\n",
      "  -1.26492292e-02  5.09124581e-02  8.05284486e-03  4.37304382e-02\n",
      "   1.19717461e-02  3.01255828e-02 -4.91516186e-03  8.33489145e-02\n",
      "   1.08440246e-02 -3.54567272e-02  8.29203646e-02 -2.76297314e-02\n",
      "   2.12751016e-02  3.30424013e-02  1.23289107e-01 -1.27680549e-02\n",
      "   3.28136168e-03  2.66018580e-03  1.28134957e-02  4.47692339e-03\n",
      "   2.61166157e-02 -4.44436177e-03  4.23040808e-03  1.85048600e-02\n",
      "  -4.79030889e-03  4.12163911e-02  2.73453261e-02  1.67591060e-02\n",
      "   1.88330032e-02  3.42877718e-03  9.74433758e-03  5.78872770e-02\n",
      "  -4.02705218e-02  5.18015952e-02 -2.67487694e-03 -3.73048625e-03\n",
      "   2.08645293e-02 -7.69103909e-03  4.04323845e-02 -2.10889450e-02\n",
      "   4.39827033e-02  1.84297702e-02  1.12365266e-02 -2.41242219e-02\n",
      "  -3.75916381e-02  2.61879166e-02  3.04925505e-02  2.22269185e-02]\n",
      " [ 2.36226471e-01  2.60501622e-01 -2.90380471e-02  4.56639564e-02\n",
      "  -2.05916006e-02  4.67281859e-02 -8.13472459e-02  8.05139081e-02\n",
      "   4.05049230e-02  1.92559374e-03 -1.01602197e-01 -6.90932693e-02\n",
      "   3.22888745e-02  4.18511269e-02  6.70853256e-02  1.60375381e-02\n",
      "  -1.74741290e-02 -1.05454525e-01  2.02326451e-03 -4.15073045e-02\n",
      "   1.33419420e-02 -6.97690015e-03  7.48776884e-02 -6.05343623e-02\n",
      "  -1.92212532e-02 -5.07461716e-02  7.53135235e-03 -4.52012922e-02\n",
      "   6.04020388e-02 -1.25148952e-01 -5.17979148e-02 -4.56171459e-02\n",
      "   1.82940263e-02 -3.00659351e-02  2.86333983e-02 -1.14801086e-02\n",
      "   4.55714281e-02  9.71013388e-03  1.46230482e-02  1.30197422e-02\n",
      "   1.47688259e-03  4.96232602e-02 -8.31428573e-02  4.33145198e-03\n",
      "  -7.81290407e-02  2.97916385e-02  1.26270385e-03  1.31382373e-02\n",
      "   1.99054866e-02 -3.43667075e-02  5.11591563e-03  1.04330342e-02\n",
      "   4.14984459e-02 -5.50687133e-02  7.37039524e-02 -6.10167122e-02\n",
      "   5.16258056e-03  4.65849718e-02 -5.96666035e-03  3.87027303e-02\n",
      "  -1.37417038e-02 -9.53480562e-02  3.92178565e-02  6.23880945e-02\n",
      "   7.80757795e-02 -2.05969480e-02  6.50056203e-02 -5.37772333e-03\n",
      "  -4.72279655e-02  1.25842125e-02  3.07319574e-02  1.10925201e-03\n",
      "  -3.81723807e-02  2.71085825e-02 -3.62594353e-05  1.29739983e-03\n",
      "   3.42257752e-02  6.13858322e-03  3.52943857e-02 -6.71733518e-03\n",
      "   3.29437811e-02 -3.57672986e-02  5.20825500e-02  1.95974466e-02\n",
      "   2.45862186e-02  1.66557811e-02 -3.64347070e-02  1.11029140e-02\n",
      "  -9.03618005e-03 -5.74207934e-03 -3.38632404e-02  5.78169830e-02\n",
      "  -4.82410522e-02 -3.23206685e-02  2.92564100e-02 -1.20486059e-02\n",
      "  -5.24956481e-02 -7.32670871e-03  4.93235043e-02  1.09798708e-02]]\n",
      "\n",
      "Generando representación para combinación: ['stop_words', 'text_cleaning']\n",
      "Combinacion: ['stop_words', 'text_cleaning'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['stop_words', 'text_cleaning'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['stop_words', 'text_cleaning'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['stop_words', 'text_cleaning'] + TF-IDF + SVD\n",
      "Preview: [[ 2.41619242e-01  4.79950654e-02  1.03681315e-02  4.66603145e-02\n",
      "   3.14542964e-02  6.57825337e-02 -4.62627862e-03  5.73435396e-02\n",
      "  -5.23228024e-02 -1.19331136e-02  2.64505766e-02 -2.20216646e-02\n",
      "  -4.27365231e-02  1.96771349e-02 -1.96127015e-02 -5.24894145e-02\n",
      "  -3.53642953e-02  3.93121722e-03  6.06509100e-02  1.47045796e-02\n",
      "   3.13350590e-03  3.16400814e-03 -1.35251049e-02 -4.19206973e-02\n",
      "  -7.75348065e-03 -1.80402983e-02 -1.49902394e-03  3.12366428e-02\n",
      "   5.92275814e-02 -4.15646255e-02  3.24289063e-02 -8.46860456e-03\n",
      "  -2.58658213e-02 -2.82328189e-03  9.87832681e-03 -3.94539645e-02\n",
      "  -3.33829944e-02  1.30111151e-02 -1.16054918e-02  9.82627756e-03\n",
      "  -2.46787912e-02  2.69791284e-02 -4.86552943e-02  1.05317239e-02\n",
      "   1.42748976e-03 -2.79302716e-02 -2.70151014e-02 -2.59716676e-02\n",
      "  -7.38715294e-03 -9.18832499e-03 -4.86948082e-02 -2.81214328e-02\n",
      "   1.37958575e-03 -2.21523740e-02 -1.07918440e-02 -4.13757843e-02\n",
      "   4.01539439e-02  2.11904361e-02 -3.70487529e-02  2.99653529e-02\n",
      "   1.44793316e-02  2.19179092e-02  3.31951496e-03 -1.48834365e-02\n",
      "   6.75171341e-03 -3.21621309e-02 -3.82727046e-03  2.67131018e-02\n",
      "  -2.38364606e-02  3.87747594e-02 -3.26960577e-02  1.45293275e-02\n",
      "  -2.56883249e-02  5.46497053e-02 -2.67362977e-02  1.31560779e-02\n",
      "  -2.77127045e-02  1.97043077e-02  1.11372663e-02 -7.25840933e-04\n",
      "   1.58364514e-02 -1.57494232e-02  1.30601179e-02  9.48717514e-03\n",
      "   2.83733550e-02 -3.16578799e-02  3.12112066e-02  4.39959019e-02\n",
      "   1.84012484e-02  7.76038781e-03 -2.81829628e-02 -6.23230757e-03\n",
      "  -2.29678373e-02  1.06987265e-02 -2.99054929e-02 -3.32962899e-03\n",
      "  -4.88869875e-02  1.21019359e-02 -4.18733904e-03 -9.15127473e-03]\n",
      " [ 2.34590359e-01  1.21140262e-01 -1.88253806e-02 -1.33450101e-01\n",
      "  -3.62426574e-02 -3.57662870e-02  5.54809093e-02 -1.33105856e-02\n",
      "  -5.98184632e-03 -6.27510281e-02 -7.08016169e-03 -8.64938130e-02\n",
      "  -2.96827936e-02 -5.48248705e-02  4.09965636e-02  5.39039224e-02\n",
      "  -8.21905993e-02  1.35836493e-02  3.52242915e-02  9.53871278e-03\n",
      "   5.64292243e-02  3.37886059e-02  7.14027307e-02  1.71161991e-02\n",
      "  -3.09543135e-02 -3.08727961e-02  4.91816500e-02 -2.43359839e-02\n",
      "  -6.17885662e-02  3.07497075e-02  3.29702325e-02  1.25991751e-03\n",
      "  -2.77705414e-02  7.08079102e-03  7.99492602e-02 -8.77578174e-02\n",
      "  -7.20358388e-02 -1.05810915e-01  6.05757450e-03  1.62614491e-02\n",
      "   3.13160763e-02  1.31562529e-02 -2.40418051e-02  5.21238502e-02\n",
      "   4.95074155e-02 -4.61370376e-03 -4.02697634e-02  3.40766424e-02\n",
      "   5.79087562e-02 -7.39690381e-02  1.57794327e-02  6.99447459e-02\n",
      "  -1.26492292e-02  5.09124581e-02  8.05284486e-03  4.37304382e-02\n",
      "   1.19717461e-02  3.01255828e-02 -4.91516186e-03  8.33489145e-02\n",
      "   1.08440246e-02 -3.54567272e-02  8.29203646e-02 -2.76297314e-02\n",
      "   2.12751016e-02  3.30424013e-02  1.23289107e-01 -1.27680549e-02\n",
      "   3.28136168e-03  2.66018580e-03  1.28134957e-02  4.47692339e-03\n",
      "   2.61166157e-02 -4.44436177e-03  4.23040808e-03  1.85048600e-02\n",
      "  -4.79030889e-03  4.12163911e-02  2.73453261e-02  1.67591060e-02\n",
      "   1.88330032e-02  3.42877718e-03  9.74433758e-03  5.78872770e-02\n",
      "  -4.02705218e-02  5.18015952e-02 -2.67487694e-03 -3.73048625e-03\n",
      "   2.08645293e-02 -7.69103909e-03  4.04323845e-02 -2.10889450e-02\n",
      "   4.39827033e-02  1.84297702e-02  1.12365266e-02 -2.41242219e-02\n",
      "  -3.75916381e-02  2.61879166e-02  3.04925505e-02  2.22269185e-02]\n",
      " [ 2.36226471e-01  2.60501622e-01 -2.90380471e-02  4.56639564e-02\n",
      "  -2.05916006e-02  4.67281859e-02 -8.13472459e-02  8.05139081e-02\n",
      "   4.05049230e-02  1.92559374e-03 -1.01602197e-01 -6.90932693e-02\n",
      "   3.22888745e-02  4.18511269e-02  6.70853256e-02  1.60375381e-02\n",
      "  -1.74741290e-02 -1.05454525e-01  2.02326451e-03 -4.15073045e-02\n",
      "   1.33419420e-02 -6.97690015e-03  7.48776884e-02 -6.05343623e-02\n",
      "  -1.92212532e-02 -5.07461716e-02  7.53135235e-03 -4.52012922e-02\n",
      "   6.04020388e-02 -1.25148952e-01 -5.17979148e-02 -4.56171459e-02\n",
      "   1.82940263e-02 -3.00659351e-02  2.86333983e-02 -1.14801086e-02\n",
      "   4.55714281e-02  9.71013388e-03  1.46230482e-02  1.30197422e-02\n",
      "   1.47688259e-03  4.96232602e-02 -8.31428573e-02  4.33145198e-03\n",
      "  -7.81290407e-02  2.97916385e-02  1.26270385e-03  1.31382373e-02\n",
      "   1.99054866e-02 -3.43667075e-02  5.11591563e-03  1.04330342e-02\n",
      "   4.14984459e-02 -5.50687133e-02  7.37039524e-02 -6.10167122e-02\n",
      "   5.16258056e-03  4.65849718e-02 -5.96666035e-03  3.87027303e-02\n",
      "  -1.37417038e-02 -9.53480562e-02  3.92178565e-02  6.23880945e-02\n",
      "   7.80757795e-02 -2.05969480e-02  6.50056203e-02 -5.37772333e-03\n",
      "  -4.72279655e-02  1.25842125e-02  3.07319574e-02  1.10925201e-03\n",
      "  -3.81723807e-02  2.71085825e-02 -3.62594353e-05  1.29739983e-03\n",
      "   3.42257752e-02  6.13858322e-03  3.52943857e-02 -6.71733518e-03\n",
      "   3.29437811e-02 -3.57672986e-02  5.20825500e-02  1.95974466e-02\n",
      "   2.45862186e-02  1.66557811e-02 -3.64347070e-02  1.11029140e-02\n",
      "  -9.03618005e-03 -5.74207934e-03 -3.38632404e-02  5.78169830e-02\n",
      "  -4.82410522e-02 -3.23206685e-02  2.92564100e-02 -1.20486059e-02\n",
      "  -5.24956481e-02 -7.32670871e-03  4.93235043e-02  1.09798708e-02]]\n",
      "\n",
      "Generando representación para combinación: ['lemmatization', 'text_cleaning']\n",
      "Combinacion: ['lemmatization', 'text_cleaning'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['lemmatization', 'text_cleaning'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['lemmatization', 'text_cleaning'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "Preview: [[ 2.41619242e-01  4.79950654e-02  1.03681315e-02  4.66603145e-02\n",
      "   3.14542964e-02  6.57825337e-02 -4.62627862e-03  5.73435396e-02\n",
      "  -5.23228024e-02 -1.19331136e-02  2.64505766e-02 -2.20216646e-02\n",
      "  -4.27365231e-02  1.96771349e-02 -1.96127015e-02 -5.24894145e-02\n",
      "  -3.53642953e-02  3.93121722e-03  6.06509100e-02  1.47045796e-02\n",
      "   3.13350590e-03  3.16400814e-03 -1.35251049e-02 -4.19206973e-02\n",
      "  -7.75348065e-03 -1.80402983e-02 -1.49902394e-03  3.12366428e-02\n",
      "   5.92275814e-02 -4.15646255e-02  3.24289063e-02 -8.46860456e-03\n",
      "  -2.58658213e-02 -2.82328189e-03  9.87832681e-03 -3.94539645e-02\n",
      "  -3.33829944e-02  1.30111151e-02 -1.16054918e-02  9.82627756e-03\n",
      "  -2.46787912e-02  2.69791284e-02 -4.86552943e-02  1.05317239e-02\n",
      "   1.42748976e-03 -2.79302716e-02 -2.70151014e-02 -2.59716676e-02\n",
      "  -7.38715294e-03 -9.18832499e-03 -4.86948082e-02 -2.81214328e-02\n",
      "   1.37958575e-03 -2.21523740e-02 -1.07918440e-02 -4.13757843e-02\n",
      "   4.01539439e-02  2.11904361e-02 -3.70487529e-02  2.99653529e-02\n",
      "   1.44793316e-02  2.19179092e-02  3.31951496e-03 -1.48834365e-02\n",
      "   6.75171341e-03 -3.21621309e-02 -3.82727046e-03  2.67131018e-02\n",
      "  -2.38364606e-02  3.87747594e-02 -3.26960577e-02  1.45293275e-02\n",
      "  -2.56883249e-02  5.46497053e-02 -2.67362977e-02  1.31560779e-02\n",
      "  -2.77127045e-02  1.97043077e-02  1.11372663e-02 -7.25840933e-04\n",
      "   1.58364514e-02 -1.57494232e-02  1.30601179e-02  9.48717514e-03\n",
      "   2.83733550e-02 -3.16578799e-02  3.12112066e-02  4.39959019e-02\n",
      "   1.84012484e-02  7.76038781e-03 -2.81829628e-02 -6.23230757e-03\n",
      "  -2.29678373e-02  1.06987265e-02 -2.99054929e-02 -3.32962899e-03\n",
      "  -4.88869875e-02  1.21019359e-02 -4.18733904e-03 -9.15127473e-03]\n",
      " [ 2.34590359e-01  1.21140262e-01 -1.88253806e-02 -1.33450101e-01\n",
      "  -3.62426574e-02 -3.57662870e-02  5.54809093e-02 -1.33105856e-02\n",
      "  -5.98184632e-03 -6.27510281e-02 -7.08016169e-03 -8.64938130e-02\n",
      "  -2.96827936e-02 -5.48248705e-02  4.09965636e-02  5.39039224e-02\n",
      "  -8.21905993e-02  1.35836493e-02  3.52242915e-02  9.53871278e-03\n",
      "   5.64292243e-02  3.37886059e-02  7.14027307e-02  1.71161991e-02\n",
      "  -3.09543135e-02 -3.08727961e-02  4.91816500e-02 -2.43359839e-02\n",
      "  -6.17885662e-02  3.07497075e-02  3.29702325e-02  1.25991751e-03\n",
      "  -2.77705414e-02  7.08079102e-03  7.99492602e-02 -8.77578174e-02\n",
      "  -7.20358388e-02 -1.05810915e-01  6.05757450e-03  1.62614491e-02\n",
      "   3.13160763e-02  1.31562529e-02 -2.40418051e-02  5.21238502e-02\n",
      "   4.95074155e-02 -4.61370376e-03 -4.02697634e-02  3.40766424e-02\n",
      "   5.79087562e-02 -7.39690381e-02  1.57794327e-02  6.99447459e-02\n",
      "  -1.26492292e-02  5.09124581e-02  8.05284486e-03  4.37304382e-02\n",
      "   1.19717461e-02  3.01255828e-02 -4.91516186e-03  8.33489145e-02\n",
      "   1.08440246e-02 -3.54567272e-02  8.29203646e-02 -2.76297314e-02\n",
      "   2.12751016e-02  3.30424013e-02  1.23289107e-01 -1.27680549e-02\n",
      "   3.28136168e-03  2.66018580e-03  1.28134957e-02  4.47692339e-03\n",
      "   2.61166157e-02 -4.44436177e-03  4.23040808e-03  1.85048600e-02\n",
      "  -4.79030889e-03  4.12163911e-02  2.73453261e-02  1.67591060e-02\n",
      "   1.88330032e-02  3.42877718e-03  9.74433758e-03  5.78872770e-02\n",
      "  -4.02705218e-02  5.18015952e-02 -2.67487694e-03 -3.73048625e-03\n",
      "   2.08645293e-02 -7.69103909e-03  4.04323845e-02 -2.10889450e-02\n",
      "   4.39827033e-02  1.84297702e-02  1.12365266e-02 -2.41242219e-02\n",
      "  -3.75916381e-02  2.61879166e-02  3.04925505e-02  2.22269185e-02]\n",
      " [ 2.36226471e-01  2.60501622e-01 -2.90380471e-02  4.56639564e-02\n",
      "  -2.05916006e-02  4.67281859e-02 -8.13472459e-02  8.05139081e-02\n",
      "   4.05049230e-02  1.92559374e-03 -1.01602197e-01 -6.90932693e-02\n",
      "   3.22888745e-02  4.18511269e-02  6.70853256e-02  1.60375381e-02\n",
      "  -1.74741290e-02 -1.05454525e-01  2.02326451e-03 -4.15073045e-02\n",
      "   1.33419420e-02 -6.97690015e-03  7.48776884e-02 -6.05343623e-02\n",
      "  -1.92212532e-02 -5.07461716e-02  7.53135235e-03 -4.52012922e-02\n",
      "   6.04020388e-02 -1.25148952e-01 -5.17979148e-02 -4.56171459e-02\n",
      "   1.82940263e-02 -3.00659351e-02  2.86333983e-02 -1.14801086e-02\n",
      "   4.55714281e-02  9.71013388e-03  1.46230482e-02  1.30197422e-02\n",
      "   1.47688259e-03  4.96232602e-02 -8.31428573e-02  4.33145198e-03\n",
      "  -7.81290407e-02  2.97916385e-02  1.26270385e-03  1.31382373e-02\n",
      "   1.99054866e-02 -3.43667075e-02  5.11591563e-03  1.04330342e-02\n",
      "   4.14984459e-02 -5.50687133e-02  7.37039524e-02 -6.10167122e-02\n",
      "   5.16258056e-03  4.65849718e-02 -5.96666035e-03  3.87027303e-02\n",
      "  -1.37417038e-02 -9.53480562e-02  3.92178565e-02  6.23880945e-02\n",
      "   7.80757795e-02 -2.05969480e-02  6.50056203e-02 -5.37772333e-03\n",
      "  -4.72279655e-02  1.25842125e-02  3.07319574e-02  1.10925201e-03\n",
      "  -3.81723807e-02  2.71085825e-02 -3.62594353e-05  1.29739983e-03\n",
      "   3.42257752e-02  6.13858322e-03  3.52943857e-02 -6.71733518e-03\n",
      "   3.29437811e-02 -3.57672986e-02  5.20825500e-02  1.95974466e-02\n",
      "   2.45862186e-02  1.66557811e-02 -3.64347070e-02  1.11029140e-02\n",
      "  -9.03618005e-03 -5.74207934e-03 -3.38632404e-02  5.78169830e-02\n",
      "  -4.82410522e-02 -3.23206685e-02  2.92564100e-02 -1.20486059e-02\n",
      "  -5.24956481e-02 -7.32670871e-03  4.93235043e-02  1.09798708e-02]]\n",
      "\n",
      "Generando representación para combinación: ['tokenization', 'stop_words', 'lemmatization']\n",
      "Combinacion: ['tokenization', 'stop_words', 'lemmatization'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'stop_words', 'lemmatization'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD\n",
      "Preview: [[ 1.35841021e-01  2.26149523e-02  1.02582051e-01  1.48950523e-02\n",
      "   8.06559933e-02  3.60105948e-02  6.50567187e-02  9.43325183e-03\n",
      "  -2.81040637e-03 -2.42980223e-02 -3.77539048e-02 -2.96213633e-02\n",
      "   2.83211502e-03 -8.20373428e-03 -9.57557736e-03  7.11913236e-03\n",
      "   1.91218811e-03  1.78063418e-02 -7.81032703e-03  2.07832155e-02\n",
      "   1.70251772e-02  2.17100966e-02  3.34035368e-02 -1.41764569e-03\n",
      "  -1.60231440e-02 -2.25349440e-02 -7.44489955e-02 -4.99100631e-02\n",
      "  -3.54722093e-02  3.83836166e-02  3.87457061e-02 -3.83928917e-02\n",
      "  -1.90172698e-03 -9.24765250e-03 -4.59359062e-02  1.71174253e-02\n",
      "  -4.24471743e-02 -2.09978650e-02 -6.20415138e-04 -1.21819255e-02\n",
      "  -2.64618628e-02 -6.97801864e-03 -2.23341933e-02 -5.65250767e-03\n",
      "   1.60736463e-02  1.69012561e-02  2.13672623e-03 -5.73515750e-03\n",
      "  -1.54291399e-02  8.74307092e-03  1.55883063e-02 -3.26716007e-02\n",
      "   7.84061897e-03 -2.26384304e-02 -1.81619231e-02 -1.80508107e-02\n",
      "  -1.58816659e-02 -1.69722142e-02  8.88058438e-03 -2.28665044e-02\n",
      "  -3.40543824e-03 -8.38646638e-03  9.22823738e-03 -1.79492659e-03\n",
      "   4.02811406e-02  1.35458488e-02  3.03213187e-02 -1.29028067e-02\n",
      "   6.79715595e-03 -4.53072479e-02  1.71188733e-02 -7.97717889e-03\n",
      "   1.11591720e-02 -6.31100476e-02 -1.65478446e-02  1.22670097e-02\n",
      "   1.42971060e-02 -2.67786050e-02 -3.19649880e-02 -3.97514349e-04\n",
      "   1.41075860e-02 -1.72473210e-02 -3.28377688e-02 -5.16949457e-02\n",
      "   2.56571737e-02  5.66348627e-04  8.22768034e-05 -2.08585878e-03\n",
      "   3.26838883e-02 -1.38579327e-02 -2.26779261e-02 -3.21191003e-03\n",
      "   2.64368047e-03  6.72506141e-02 -2.23753604e-02 -5.30649260e-03\n",
      "  -2.64115126e-02 -3.06844710e-02  2.04621259e-02  1.49253766e-02]\n",
      " [ 1.35164611e-01  9.17474070e-02 -7.53835729e-02 -7.79744535e-02\n",
      "  -6.72512935e-02  1.09565472e-01  1.89784210e-02  1.11793868e-01\n",
      "   5.78197133e-02  1.09552762e-02 -2.99323816e-02  2.86666055e-02\n",
      "   1.25592396e-02 -3.02412386e-02  1.34929869e-02  2.98101369e-02\n",
      "  -1.02991326e-02 -6.76627178e-03  1.71359694e-02 -4.34156098e-03\n",
      "   8.18948030e-02 -1.16245611e-02  4.72409686e-02 -4.22970043e-02\n",
      "  -6.43764909e-02 -2.71565120e-02  1.22880310e-02 -1.44824221e-02\n",
      "  -1.15190171e-05 -1.18902313e-02  8.75636709e-04  1.65270427e-02\n",
      "  -5.44757897e-02 -7.79612316e-03 -5.39738930e-02 -2.94480314e-02\n",
      "  -1.20144403e-02 -6.08402762e-02  1.61441811e-02 -4.00058504e-03\n",
      "   1.59096592e-03 -1.29721854e-01  8.49755647e-02  3.61841722e-03\n",
      "  -3.22136679e-02  3.25868127e-02 -8.97770523e-02  6.29623944e-02\n",
      "  -6.15729788e-02  6.86419735e-03  2.02300292e-02  3.39235453e-02\n",
      "  -9.13868085e-04 -4.31902655e-02  2.46101957e-02 -1.97846265e-03\n",
      "  -3.50406196e-02  2.34694123e-03 -4.01361479e-02  5.66863732e-02\n",
      "   1.73498960e-02 -1.04859285e-02  2.71610306e-03 -4.87160483e-02\n",
      "  -6.31991152e-02  1.33834330e-02  2.03755023e-02 -2.25793501e-02\n",
      "  -1.34728593e-02  1.00126952e-02  2.99042004e-02  1.36188873e-02\n",
      "   2.53510570e-02 -1.35270452e-02 -5.16160705e-02 -3.76386061e-02\n",
      "  -2.98651564e-02 -1.65859084e-02 -2.61083847e-03  2.05455052e-02\n",
      "  -3.30441755e-02 -1.13974907e-02 -1.90985122e-02 -3.84811996e-02\n",
      "  -6.68256846e-02  1.94246931e-02 -3.86569382e-03 -1.23485120e-02\n",
      "   3.11076150e-02  6.63962332e-03  1.45457016e-02  7.91117571e-02\n",
      "   3.46794302e-02  4.83877448e-03 -9.48559018e-03 -3.50884820e-03\n",
      "   5.47193443e-02 -5.57084375e-03  1.32315779e-02 -2.75904496e-02]\n",
      " [ 2.08774967e-01  2.49090733e-01 -3.83081490e-03  5.20590434e-02\n",
      "   1.85013856e-02 -1.99944558e-02 -9.09874692e-02 -6.04618793e-02\n",
      "   3.73281397e-02  6.91888466e-02 -7.96266358e-02 -1.84223926e-02\n",
      "   4.58585212e-03 -7.58209640e-02 -2.43237152e-02  1.98254231e-02\n",
      "   2.70359472e-03  2.33156384e-02 -1.66188263e-02  2.31555724e-04\n",
      "   1.23739515e-01  1.17942569e-02 -1.05179403e-02  8.33908140e-03\n",
      "  -5.32595597e-02 -5.40701619e-02 -3.06211230e-02  1.14764094e-02\n",
      "  -1.00439783e-02 -7.13684460e-03 -2.12134522e-02 -4.83435158e-02\n",
      "  -2.03623942e-03  4.13580448e-03 -2.88197587e-02 -3.82535477e-02\n",
      "  -7.10249032e-02  4.26041046e-02  1.31953379e-02  5.57486378e-02\n",
      "   3.36606276e-02  2.67478884e-02 -2.32502956e-03 -1.86073947e-02\n",
      "  -4.91739014e-02  1.40020273e-01 -7.53670231e-02  8.00965807e-02\n",
      "   8.15152645e-02  8.60998990e-03  7.18811587e-03 -5.81253956e-03\n",
      "   1.52038972e-02  1.26325629e-01 -3.29956303e-02  2.31364366e-04\n",
      "  -1.89690353e-02 -1.86111967e-02  4.29363138e-02 -2.20429483e-02\n",
      "   1.05119038e-02 -6.07492282e-02 -4.52165902e-02 -6.85079088e-03\n",
      "   5.16072506e-02  3.34387111e-02 -2.69974059e-02 -1.16532543e-01\n",
      "   6.43389901e-02  2.92334949e-02  3.50839853e-02  3.50272484e-02\n",
      "   2.98610736e-02 -1.23777819e-03  1.32618348e-02 -4.89653260e-02\n",
      "  -3.86383748e-02  1.92040453e-02 -2.65072225e-02  5.58682628e-03\n",
      "  -6.43068493e-02 -1.64023961e-03 -7.98844766e-02 -5.86361470e-02\n",
      "  -1.13398790e-02 -2.05208202e-02  1.93433717e-02 -4.31526756e-03\n",
      "   3.23949866e-02  2.57036190e-04 -2.82264230e-03  7.49472542e-02\n",
      "   8.14487574e-03 -6.16798500e-02 -8.53171544e-03  3.33483495e-02\n",
      "  -3.63441303e-02  4.96374627e-02 -8.31985558e-03  1.96647441e-02]]\n",
      "\n",
      "Generando representación para combinación: ['tokenization', 'stop_words', 'text_cleaning']\n",
      "Combinacion: ['tokenization', 'stop_words', 'text_cleaning'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'stop_words', 'text_cleaning'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD\n",
      "Preview: [[ 1.19106415e-01  3.67448001e-02  9.67817553e-02  2.18552262e-02\n",
      "   6.50572958e-02 -5.29630046e-02 -1.53848471e-02 -2.43887771e-02\n",
      "  -2.25170860e-02  1.28845222e-02 -1.27602513e-02  5.13100926e-02\n",
      "  -1.11945532e-02 -1.13920502e-02 -7.01660009e-03  3.82077573e-03\n",
      "   6.82304016e-03 -9.70448151e-03  4.59710908e-03  1.30133834e-02\n",
      "  -5.43433680e-04 -1.75051326e-02  4.75242358e-02  2.90109833e-02\n",
      "   2.82255661e-02  9.12957063e-03 -2.21682086e-02 -2.10848643e-03\n",
      "  -3.78706621e-02  4.86103519e-02  1.37928211e-02 -4.88510751e-02\n",
      "   1.50356185e-02 -8.94641282e-03  5.17248753e-03 -5.25546803e-02\n",
      "   2.43098095e-02 -1.65707067e-02  4.10156989e-02  1.22943893e-02\n",
      "   8.51291559e-03 -2.01567481e-02 -2.89975399e-03 -1.87103905e-02\n",
      "   3.63587510e-03  4.47024364e-03  3.22751084e-03  1.95810863e-02\n",
      "  -2.87800122e-02  1.39667415e-02 -1.34542339e-02  7.29200175e-03\n",
      "   2.01356331e-03  6.15961108e-03 -6.36527996e-02 -2.03525306e-03\n",
      "  -6.91252586e-03 -7.80155936e-03  1.38301830e-03 -3.92847781e-02\n",
      "   4.98613958e-02  4.58170614e-03 -2.73959914e-02 -6.94029931e-03\n",
      "   4.82429043e-04 -2.31167999e-02  5.16801869e-03  2.29243447e-02\n",
      "  -2.98461418e-02 -6.76363788e-03  3.71464928e-02 -1.57068831e-02\n",
      "   1.53320782e-02 -3.90610316e-02  1.46380490e-02 -1.01076776e-02\n",
      "  -2.85013995e-02 -1.57956373e-02 -2.39027057e-02  2.90373339e-02\n",
      "   2.76610573e-02  1.69520987e-02  9.81511730e-03  8.05168904e-03\n",
      "  -4.94432965e-03 -9.10700156e-03  2.76832726e-02  6.97512952e-02\n",
      "   3.28591988e-03 -8.30736072e-03 -2.01327243e-02  2.17448519e-02\n",
      "  -2.54467444e-02  5.74178271e-04 -2.34340152e-02  3.77657785e-02\n",
      "  -8.00784788e-04 -1.76966353e-03 -2.64459800e-03 -3.68788335e-02]\n",
      " [ 1.47989276e-01  7.36843405e-02 -1.15701281e-01 -8.47720775e-02\n",
      "  -4.05645772e-02 -1.03660326e-01 -3.00600253e-02  5.01236773e-02\n",
      "  -1.12761507e-01 -5.28903568e-03  4.20815317e-02 -1.72867622e-02\n",
      "   1.92854767e-02 -1.11733263e-02 -2.21384780e-02  2.92734100e-02\n",
      "   2.65609176e-02  1.64316290e-02  7.14717512e-03 -2.75132910e-02\n",
      "   4.89961733e-02 -7.96007501e-02  4.11814741e-02  3.30588633e-02\n",
      "  -3.88566263e-04  5.55262604e-02 -3.91729098e-02 -1.55537488e-02\n",
      "  -2.93906002e-02  2.47278162e-02  6.23757628e-03  4.57968401e-02\n",
      "   6.17132785e-03 -3.06697147e-02 -4.66379764e-03  2.29691876e-02\n",
      "   4.95506726e-02 -8.11060199e-03  1.30678336e-01  1.39573547e-03\n",
      "  -3.40250276e-02  5.10231116e-02 -8.39697281e-05  5.28062444e-02\n",
      "  -7.75840520e-02 -3.02899901e-02 -2.39503332e-03 -1.25998132e-02\n",
      "   1.45464712e-02 -4.65569466e-02 -2.71199777e-02 -2.13613692e-02\n",
      "   4.74321306e-03 -7.92350831e-05  3.90646245e-02 -2.39270296e-02\n",
      "  -3.24554400e-02  1.17769393e-02 -4.54686392e-02  1.05425546e-02\n",
      "   2.83081477e-02 -4.54834769e-02  4.30689528e-02 -1.69706575e-02\n",
      "   5.55925668e-03 -3.45480972e-02 -2.37936239e-02 -2.51699657e-02\n",
      "   8.06155366e-03  1.70324166e-02  2.93472161e-02 -2.68127399e-02\n",
      "   1.74796330e-03 -5.64869855e-02 -3.23258818e-02  2.01470600e-02\n",
      "   2.08860588e-02 -2.42202096e-02 -2.36584958e-02  7.05393032e-02\n",
      "  -3.76625601e-02  3.36666373e-02  5.33965085e-02 -3.51439511e-02\n",
      "   9.23145089e-03  4.69880726e-02  2.89589504e-02 -2.85891213e-02\n",
      "   1.16586454e-02 -1.20411808e-02  7.54800883e-03  7.50388171e-03\n",
      "   4.22591920e-02 -4.06238745e-03 -1.89095160e-02 -4.04545323e-02\n",
      "  -2.70143974e-02 -1.88096558e-02  2.26357591e-02  2.05307492e-02]\n",
      " [ 2.14116750e-01  2.23297713e-01 -1.69229913e-02  7.78782084e-02\n",
      "  -1.70692808e-02  8.22238268e-02 -7.54854288e-02  5.44036300e-02\n",
      "   2.26430443e-02  1.98422956e-02  5.06419865e-02  7.28921542e-02\n",
      "  -3.00234732e-02 -2.22024007e-02 -4.83109992e-02 -8.77851644e-03\n",
      "   6.67299153e-03  1.48114918e-02 -3.22887615e-02 -4.66323837e-02\n",
      "  -3.28852285e-02 -1.56753896e-02  1.21734284e-01  3.11825898e-02\n",
      "   7.53901337e-02 -9.03934499e-03 -7.18262412e-03 -5.86668469e-02\n",
      "   7.91318524e-02  6.09591257e-02  6.17133452e-02 -4.36394681e-03\n",
      "   1.85184993e-02 -9.66052967e-02 -9.40053093e-02  7.46058761e-03\n",
      "   4.98349548e-02 -4.34307958e-02  2.59910992e-02  3.92338737e-02\n",
      "  -1.38820178e-02  6.58634814e-02 -3.99672934e-03  6.06822334e-04\n",
      "  -6.28246565e-02 -5.18291187e-02 -8.08493463e-02  7.91838727e-02\n",
      "  -5.66662038e-02 -2.26145804e-02 -1.60039161e-02  8.35849845e-03\n",
      "   8.49911566e-02 -6.86650309e-03 -2.98180497e-02 -2.43219956e-02\n",
      "  -3.75492092e-02  1.27486425e-02  7.60166759e-02 -2.33488552e-02\n",
      "   3.70717738e-02  1.99070151e-02  3.68793443e-02 -2.55555342e-02\n",
      "   6.73880862e-02  3.96230121e-02 -9.91428631e-03 -3.56755271e-02\n",
      "   1.57754993e-02  1.26405574e-02 -5.93941638e-02 -4.85160220e-03\n",
      "  -1.63719905e-02  2.43573093e-02 -5.08754265e-02  1.31386384e-03\n",
      "  -1.48264698e-02  3.29483778e-02  1.36651585e-02 -1.77604381e-02\n",
      "   2.24573298e-02  5.60895498e-02 -3.13951475e-03 -7.93870503e-02\n",
      "  -2.47525723e-02 -6.91836762e-02 -1.15958309e-02 -1.97293381e-02\n",
      "  -9.53904365e-03  1.24212125e-02 -2.14389681e-02 -1.28678108e-02\n",
      "  -2.50326419e-04  2.11500669e-02 -2.54273120e-03  1.97852443e-03\n",
      "  -3.07433344e-04  4.39161901e-03 -3.95612607e-02  2.26455043e-02]]\n",
      "\n",
      "Generando representación para combinación: ['tokenization', 'lemmatization', 'text_cleaning']\n",
      "Combinacion: ['tokenization', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "Preview: [[ 2.78709525e-01  3.82300507e-02  3.70756716e-02 -3.97521838e-03\n",
      "  -2.54870158e-02 -5.00134757e-02  9.60298708e-02 -2.30025163e-02\n",
      "   5.77646934e-02  1.02547234e-02 -1.11180700e-02  8.32476139e-02\n",
      "  -3.75848119e-02  1.14941182e-02  3.00074287e-02 -1.68309501e-02\n",
      "   4.00765102e-03 -1.78488070e-02 -3.90794946e-02 -7.75063488e-03\n",
      "  -1.48056957e-02 -2.88259675e-02  2.02388037e-02  1.29601298e-02\n",
      "   3.12873334e-02 -1.38766412e-02 -2.11620440e-02  5.85519609e-02\n",
      "  -5.81197731e-03  1.22709696e-03  2.01275699e-02  5.78177650e-02\n",
      "   2.49390099e-02  3.33712837e-02 -3.25960720e-02  1.83898773e-02\n",
      "  -3.67318217e-03 -1.12541235e-02  1.02319928e-02  3.19979315e-02\n",
      "  -3.35337753e-02 -1.06079526e-02  3.86533266e-02 -1.37789687e-02\n",
      "   3.12739105e-02  1.04391165e-02 -6.33255012e-02 -5.21632746e-02\n",
      "  -3.11659384e-02 -4.58647919e-02  3.36706273e-03 -2.49021901e-03\n",
      "   1.52645962e-02 -2.59882263e-02 -1.53494078e-02 -2.01501353e-02\n",
      "   3.93212722e-02  7.98720780e-03  7.65706474e-03 -4.42616686e-02\n",
      "   1.48430965e-02 -2.24054493e-02  6.75778041e-04  2.82708554e-02\n",
      "   4.11718773e-02 -2.49484327e-02  2.04574056e-02 -5.89566046e-02\n",
      "   2.44707534e-02 -1.15838621e-02  3.12638387e-02 -3.95274199e-02\n",
      "  -6.78046904e-03 -1.94200532e-02  2.68130490e-02 -7.09170955e-03\n",
      "  -5.89419253e-03 -2.45627510e-02 -1.95574566e-03 -1.50980882e-02\n",
      "  -1.71803762e-02 -1.47213465e-02  1.77473101e-02 -4.28842204e-02\n",
      "   8.39528571e-03 -1.36172695e-02 -7.56820599e-03 -7.15081855e-04\n",
      "  -1.78335343e-02 -4.64009339e-03  1.02236812e-02  2.41344602e-02\n",
      "  -2.01128061e-02  3.30991384e-02 -3.25192939e-02  3.59105886e-02\n",
      "  -2.79811505e-02  2.29855007e-02 -2.63751988e-03 -1.22558125e-02]\n",
      " [ 2.64951499e-01  1.13125365e-01  6.79277824e-04 -1.71446814e-01\n",
      "   6.33666172e-02 -2.76516474e-03 -4.35321500e-02  5.19319737e-02\n",
      "   2.92364869e-02 -9.32019088e-02 -1.19005999e-02  2.94830134e-02\n",
      "   2.46272582e-02 -9.49470747e-02  3.00522221e-02 -4.93577560e-02\n",
      "   3.82090645e-02 -1.23557362e-02 -1.38055599e-02 -5.41728890e-02\n",
      "   1.29816676e-02 -1.90401089e-02  1.88305458e-02 -5.62227836e-02\n",
      "  -1.18225811e-02  5.66959474e-02  2.51730923e-02  1.80077947e-02\n",
      "   5.13705209e-02 -9.54015487e-02  5.53069481e-03 -2.43269001e-02\n",
      "  -6.50104252e-02  1.15611112e-02 -3.76728030e-02 -3.12589715e-02\n",
      "  -1.24899144e-02  7.61408119e-02  2.78665595e-02  1.24990949e-01\n",
      "   1.48126577e-02 -2.77177251e-02  6.10973385e-02 -2.79658498e-02\n",
      "   4.11143633e-02 -9.94829758e-02  4.72292975e-02 -5.47426613e-02\n",
      "   1.18738899e-02 -6.94140601e-03  4.08356123e-02 -2.14103810e-02\n",
      "  -4.24858419e-02  2.44289926e-02 -4.09808270e-02  5.85229460e-05\n",
      "   2.87257773e-02 -1.45104221e-02 -4.40278407e-02  5.43863807e-02\n",
      "   5.78330235e-02  1.90882247e-02 -3.93002913e-02  4.99751097e-02\n",
      "   3.27810598e-02  3.88122710e-02  7.46364357e-02 -9.70361203e-03\n",
      "   3.50757244e-02  1.46226784e-02 -2.93773849e-02 -2.32852964e-02\n",
      "  -1.52586411e-02  1.06083062e-02 -1.81488092e-02  8.64484841e-03\n",
      "  -8.15455906e-02  5.93081672e-02 -3.79718319e-02 -1.11906017e-02\n",
      "  -3.07450516e-02 -2.73123616e-02  1.83753030e-02  1.45664316e-03\n",
      "  -8.29027903e-02 -1.78835673e-03  2.58149552e-02  4.70068063e-03\n",
      "   1.70889699e-03  2.02558957e-02 -1.10895044e-02 -3.10828653e-03\n",
      "   4.76022685e-04 -2.84402337e-02 -6.75161723e-02  2.31857097e-02\n",
      "  -3.80676901e-02 -1.76273994e-02  9.70819081e-04  3.60631327e-02]\n",
      " [ 3.05728749e-01  2.87929201e-01 -6.86897841e-03 -3.35818518e-04\n",
      "   1.33791668e-02 -1.55380971e-02  8.54747986e-02 -9.03953597e-02\n",
      "  -5.39937012e-02 -3.11271639e-04  6.49172493e-02 -1.22321409e-01\n",
      "   5.29739368e-02 -5.26648085e-02 -1.90096502e-02  6.19050883e-02\n",
      "   8.46294456e-02 -6.10164863e-02 -3.37048245e-02 -6.63847031e-02\n",
      "  -2.80755180e-02  1.63286188e-02  9.55439851e-02  1.07208404e-02\n",
      "  -2.07335783e-02 -6.58893901e-02  5.25365380e-02 -7.07135111e-02\n",
      "   2.61571563e-02 -8.23232691e-03 -5.10463444e-02 -1.11990597e-02\n",
      "   3.73212681e-02 -5.88542341e-02 -4.27576598e-02  4.54339792e-02\n",
      "  -6.29858843e-02 -5.24649681e-02  2.48244186e-02 -7.31442477e-03\n",
      "  -1.23971513e-02 -1.26947733e-02  5.60797710e-02  3.70106203e-02\n",
      "  -1.14157525e-02 -7.20804283e-02 -5.55037687e-02 -4.97634531e-02\n",
      "  -2.93055257e-02 -3.95183162e-02  7.06568539e-02 -4.27695605e-02\n",
      "  -1.02927465e-01 -8.84567687e-02 -2.55947599e-02  2.81502405e-02\n",
      "   5.48291733e-02 -7.73787822e-02  7.78019909e-02 -2.30591612e-02\n",
      "  -1.37053257e-04  8.59706760e-03  1.47072893e-02  5.40939709e-02\n",
      "   2.51199176e-02  2.80193729e-02  4.96254910e-02 -2.95221851e-02\n",
      "  -5.60175770e-02 -5.85016094e-02  1.21942730e-02 -2.64726623e-02\n",
      "   2.48313334e-02  3.64446861e-02  7.08531079e-02  4.39883640e-02\n",
      "   3.23879286e-02 -9.80551912e-03 -1.81681227e-02  3.36060082e-02\n",
      "   7.40788315e-02  6.04423205e-02 -4.22435786e-02 -3.15662350e-02\n",
      "  -1.46096458e-02 -2.73252398e-02  2.19585126e-02  2.36874176e-02\n",
      "   3.06081487e-03 -3.71388402e-02 -1.58897201e-02  8.45788372e-03\n",
      "   1.33489633e-02  1.25577537e-02 -2.36278847e-03  7.50035659e-02\n",
      "  -2.55332957e-02 -6.55686320e-02  1.74722729e-02  7.96851826e-02]]\n",
      "\n",
      "Generando representación para combinación: ['stop_words', 'lemmatization', 'text_cleaning']\n",
      "Combinacion: ['stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "Preview: [[ 2.41619242e-01  4.79950654e-02  1.03681315e-02  4.66603145e-02\n",
      "   3.14542964e-02  6.57825337e-02 -4.62627862e-03  5.73435396e-02\n",
      "  -5.23228024e-02 -1.19331136e-02  2.64505766e-02 -2.20216646e-02\n",
      "  -4.27365231e-02  1.96771349e-02 -1.96127015e-02 -5.24894145e-02\n",
      "  -3.53642953e-02  3.93121722e-03  6.06509100e-02  1.47045796e-02\n",
      "   3.13350590e-03  3.16400814e-03 -1.35251049e-02 -4.19206973e-02\n",
      "  -7.75348065e-03 -1.80402983e-02 -1.49902394e-03  3.12366428e-02\n",
      "   5.92275814e-02 -4.15646255e-02  3.24289063e-02 -8.46860456e-03\n",
      "  -2.58658213e-02 -2.82328189e-03  9.87832681e-03 -3.94539645e-02\n",
      "  -3.33829944e-02  1.30111151e-02 -1.16054918e-02  9.82627756e-03\n",
      "  -2.46787912e-02  2.69791284e-02 -4.86552943e-02  1.05317239e-02\n",
      "   1.42748976e-03 -2.79302716e-02 -2.70151014e-02 -2.59716676e-02\n",
      "  -7.38715294e-03 -9.18832499e-03 -4.86948082e-02 -2.81214328e-02\n",
      "   1.37958575e-03 -2.21523740e-02 -1.07918440e-02 -4.13757843e-02\n",
      "   4.01539439e-02  2.11904361e-02 -3.70487529e-02  2.99653529e-02\n",
      "   1.44793316e-02  2.19179092e-02  3.31951496e-03 -1.48834365e-02\n",
      "   6.75171341e-03 -3.21621309e-02 -3.82727046e-03  2.67131018e-02\n",
      "  -2.38364606e-02  3.87747594e-02 -3.26960577e-02  1.45293275e-02\n",
      "  -2.56883249e-02  5.46497053e-02 -2.67362977e-02  1.31560779e-02\n",
      "  -2.77127045e-02  1.97043077e-02  1.11372663e-02 -7.25840933e-04\n",
      "   1.58364514e-02 -1.57494232e-02  1.30601179e-02  9.48717514e-03\n",
      "   2.83733550e-02 -3.16578799e-02  3.12112066e-02  4.39959019e-02\n",
      "   1.84012484e-02  7.76038781e-03 -2.81829628e-02 -6.23230757e-03\n",
      "  -2.29678373e-02  1.06987265e-02 -2.99054929e-02 -3.32962899e-03\n",
      "  -4.88869875e-02  1.21019359e-02 -4.18733904e-03 -9.15127473e-03]\n",
      " [ 2.34590359e-01  1.21140262e-01 -1.88253806e-02 -1.33450101e-01\n",
      "  -3.62426574e-02 -3.57662870e-02  5.54809093e-02 -1.33105856e-02\n",
      "  -5.98184632e-03 -6.27510281e-02 -7.08016169e-03 -8.64938130e-02\n",
      "  -2.96827936e-02 -5.48248705e-02  4.09965636e-02  5.39039224e-02\n",
      "  -8.21905993e-02  1.35836493e-02  3.52242915e-02  9.53871278e-03\n",
      "   5.64292243e-02  3.37886059e-02  7.14027307e-02  1.71161991e-02\n",
      "  -3.09543135e-02 -3.08727961e-02  4.91816500e-02 -2.43359839e-02\n",
      "  -6.17885662e-02  3.07497075e-02  3.29702325e-02  1.25991751e-03\n",
      "  -2.77705414e-02  7.08079102e-03  7.99492602e-02 -8.77578174e-02\n",
      "  -7.20358388e-02 -1.05810915e-01  6.05757450e-03  1.62614491e-02\n",
      "   3.13160763e-02  1.31562529e-02 -2.40418051e-02  5.21238502e-02\n",
      "   4.95074155e-02 -4.61370376e-03 -4.02697634e-02  3.40766424e-02\n",
      "   5.79087562e-02 -7.39690381e-02  1.57794327e-02  6.99447459e-02\n",
      "  -1.26492292e-02  5.09124581e-02  8.05284486e-03  4.37304382e-02\n",
      "   1.19717461e-02  3.01255828e-02 -4.91516186e-03  8.33489145e-02\n",
      "   1.08440246e-02 -3.54567272e-02  8.29203646e-02 -2.76297314e-02\n",
      "   2.12751016e-02  3.30424013e-02  1.23289107e-01 -1.27680549e-02\n",
      "   3.28136168e-03  2.66018580e-03  1.28134957e-02  4.47692339e-03\n",
      "   2.61166157e-02 -4.44436177e-03  4.23040808e-03  1.85048600e-02\n",
      "  -4.79030889e-03  4.12163911e-02  2.73453261e-02  1.67591060e-02\n",
      "   1.88330032e-02  3.42877718e-03  9.74433758e-03  5.78872770e-02\n",
      "  -4.02705218e-02  5.18015952e-02 -2.67487694e-03 -3.73048625e-03\n",
      "   2.08645293e-02 -7.69103909e-03  4.04323845e-02 -2.10889450e-02\n",
      "   4.39827033e-02  1.84297702e-02  1.12365266e-02 -2.41242219e-02\n",
      "  -3.75916381e-02  2.61879166e-02  3.04925505e-02  2.22269185e-02]\n",
      " [ 2.36226471e-01  2.60501622e-01 -2.90380471e-02  4.56639564e-02\n",
      "  -2.05916006e-02  4.67281859e-02 -8.13472459e-02  8.05139081e-02\n",
      "   4.05049230e-02  1.92559374e-03 -1.01602197e-01 -6.90932693e-02\n",
      "   3.22888745e-02  4.18511269e-02  6.70853256e-02  1.60375381e-02\n",
      "  -1.74741290e-02 -1.05454525e-01  2.02326451e-03 -4.15073045e-02\n",
      "   1.33419420e-02 -6.97690015e-03  7.48776884e-02 -6.05343623e-02\n",
      "  -1.92212532e-02 -5.07461716e-02  7.53135235e-03 -4.52012922e-02\n",
      "   6.04020388e-02 -1.25148952e-01 -5.17979148e-02 -4.56171459e-02\n",
      "   1.82940263e-02 -3.00659351e-02  2.86333983e-02 -1.14801086e-02\n",
      "   4.55714281e-02  9.71013388e-03  1.46230482e-02  1.30197422e-02\n",
      "   1.47688259e-03  4.96232602e-02 -8.31428573e-02  4.33145198e-03\n",
      "  -7.81290407e-02  2.97916385e-02  1.26270385e-03  1.31382373e-02\n",
      "   1.99054866e-02 -3.43667075e-02  5.11591563e-03  1.04330342e-02\n",
      "   4.14984459e-02 -5.50687133e-02  7.37039524e-02 -6.10167122e-02\n",
      "   5.16258056e-03  4.65849718e-02 -5.96666035e-03  3.87027303e-02\n",
      "  -1.37417038e-02 -9.53480562e-02  3.92178565e-02  6.23880945e-02\n",
      "   7.80757795e-02 -2.05969480e-02  6.50056203e-02 -5.37772333e-03\n",
      "  -4.72279655e-02  1.25842125e-02  3.07319574e-02  1.10925201e-03\n",
      "  -3.81723807e-02  2.71085825e-02 -3.62594353e-05  1.29739983e-03\n",
      "   3.42257752e-02  6.13858322e-03  3.52943857e-02 -6.71733518e-03\n",
      "   3.29437811e-02 -3.57672986e-02  5.20825500e-02  1.95974466e-02\n",
      "   2.45862186e-02  1.66557811e-02 -3.64347070e-02  1.11029140e-02\n",
      "  -9.03618005e-03 -5.74207934e-03 -3.38632404e-02  5.78169830e-02\n",
      "  -4.82410522e-02 -3.23206685e-02  2.92564100e-02 -1.20486059e-02\n",
      "  -5.24956481e-02 -7.32670871e-03  4.93235043e-02  1.09798708e-02]]\n",
      "\n",
      "Generando representación para combinación: ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning']\n",
      "Combinacion: ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "Preview: [[ 1.39036174e-01  2.00035200e-02  1.05936498e-01  1.74041627e-02\n",
      "   8.26639728e-02  3.38452259e-02  6.67382876e-02  1.01763775e-02\n",
      "   5.36402702e-03 -2.33214887e-02 -1.49495389e-03 -4.27471179e-02\n",
      "   2.78048529e-03  1.64913385e-03 -1.09038258e-02  1.35270085e-03\n",
      "  -2.27136862e-03 -1.80175682e-02 -2.21869232e-04  2.31725954e-02\n",
      "   2.44855659e-02  2.70033907e-03  2.16186190e-02 -2.48265876e-02\n",
      "   4.12199225e-03  2.12736499e-02  8.18686666e-02 -2.43835465e-02\n",
      "  -5.33662796e-02  5.21811294e-02 -8.43439640e-03 -3.71048036e-02\n",
      "   1.23063045e-02 -2.62985536e-02  5.12252999e-02 -4.46014331e-02\n",
      "   4.66435018e-03  1.03224094e-02 -3.68424666e-03 -1.47293690e-02\n",
      "  -2.11274888e-03  1.69581913e-02  1.67766344e-02 -7.71104252e-03\n",
      "  -1.57273397e-02  1.50950455e-02  1.51081947e-02 -1.77207302e-03\n",
      "  -1.52691942e-02 -6.85592313e-03  1.86944069e-02 -3.54962593e-02\n",
      "   4.98580619e-03 -2.61097669e-02 -9.67699348e-03  3.89729829e-04\n",
      "  -6.94641053e-03 -1.61945189e-02 -1.23875210e-03 -3.36361244e-02\n",
      "  -4.18354587e-03  8.26358560e-03  8.05623835e-03 -2.05601004e-02\n",
      "   1.48347180e-02 -1.85625927e-02  5.35706136e-03 -1.11686013e-03\n",
      "   2.12684610e-02 -6.96360942e-03  3.64994066e-02  1.65405359e-02\n",
      "  -1.51488868e-02 -7.78160878e-02 -1.13002061e-02 -1.44956849e-02\n",
      "  -7.39540592e-03 -1.95285652e-02  1.50097814e-03  6.55057519e-03\n",
      "  -1.15457681e-02  1.45325664e-03  2.29421992e-02  2.77167077e-02\n",
      "   5.91220656e-03  3.43129784e-04  1.37394348e-02  3.63763916e-03\n",
      "   2.73008754e-02  4.72359476e-02  2.45687454e-02  4.68092723e-02\n",
      "   1.02163752e-02  5.16158217e-02  1.63383351e-02  4.60526605e-02\n",
      "  -9.34342689e-03 -1.80158835e-02 -2.45492510e-02  2.29606606e-02]\n",
      " [ 1.44377964e-01  9.31399647e-02 -8.30380237e-02 -8.40076593e-02\n",
      "  -6.34241184e-02  1.16950634e-01  2.83494787e-02  1.09177979e-01\n",
      "   3.79366563e-02  2.23141366e-02  3.78568818e-02  3.01552890e-04\n",
      "   1.64306659e-02 -3.47618744e-02 -6.03247287e-03  2.20355784e-02\n",
      "  -1.35594191e-02  1.36041118e-02  1.28549192e-02  2.78712876e-03\n",
      "   6.00877449e-02 -3.00325157e-02  1.74243607e-02 -6.14214748e-02\n",
      "   3.83121341e-02  7.18060912e-02  2.71237071e-03 -2.81188674e-02\n",
      "   3.56864941e-03 -1.27025961e-02  1.07975766e-02  9.68475960e-03\n",
      "   1.92947381e-02  1.79531471e-02  5.85616342e-02 -1.28754809e-02\n",
      "   2.59141335e-02  3.92539058e-03  5.53088809e-03 -4.52201618e-02\n",
      "  -6.20595438e-02  4.10724360e-02 -1.02621565e-01  3.77021043e-02\n",
      "   6.28002118e-02  9.19038395e-02 -7.58099039e-02  4.27590826e-02\n",
      "  -5.96360811e-02  2.26366197e-03 -1.51306996e-02  6.38802513e-02\n",
      "   8.73462269e-03 -2.53437762e-02 -5.07586413e-02 -3.67119830e-02\n",
      "  -3.91411665e-02  1.94061304e-02  1.06324241e-02  6.48051428e-02\n",
      "   1.39768575e-02  3.75992413e-02 -9.90724434e-03  8.68805664e-03\n",
      "  -6.48896468e-02 -2.03657213e-02 -4.81306896e-04  7.53775562e-03\n",
      "   3.21711076e-02  1.13983979e-02 -4.36223211e-02  2.01560802e-02\n",
      "  -2.08836041e-02 -1.07863773e-02 -2.96705579e-02 -6.81647051e-02\n",
      "  -4.80625266e-02  2.48886205e-02 -4.39267752e-02  4.74152515e-02\n",
      "  -2.76910048e-02 -2.45554278e-02 -3.29082626e-02  6.42682068e-03\n",
      "  -1.36672280e-02 -2.64695901e-02  2.41361621e-02  4.24647682e-03\n",
      "  -8.50958341e-03  2.90525514e-02 -4.26402477e-02  2.47199655e-02\n",
      "   3.83400703e-02  1.75987141e-03  1.04232390e-02 -1.48766338e-02\n",
      "   3.38120588e-02 -1.75726612e-02  8.16122560e-04  1.40917652e-02]\n",
      " [ 2.07547010e-01  2.46230275e-01  3.55636097e-03  5.55186777e-02\n",
      "   2.53116878e-02 -1.68289106e-02 -8.47782406e-02 -5.64815452e-02\n",
      "   6.50143226e-02  7.95268230e-02  1.27184692e-02 -4.21268472e-02\n",
      "  -1.34574015e-02 -6.86937979e-02 -4.94127580e-02  1.11058128e-02\n",
      "   1.93397032e-03 -1.79786349e-02 -1.28811931e-03 -1.13509683e-03\n",
      "   1.06141545e-01 -5.19518517e-02 -1.29224933e-03  9.98659592e-03\n",
      "   1.72557396e-02  8.05645605e-02  3.08313553e-02  2.83346700e-02\n",
      "   4.22747379e-03 -1.72854688e-02 -1.18733029e-02 -4.27004688e-02\n",
      "  -9.04731380e-03 -1.26479700e-02  4.92707876e-02  1.31316535e-02\n",
      "   7.06286144e-02  2.31372001e-02  1.49335359e-02  8.89637119e-02\n",
      "   2.20805489e-02 -3.70842349e-02  3.35219792e-03 -3.89298957e-03\n",
      "  -5.73067098e-02  1.48865779e-01 -2.85178120e-02  6.57666586e-02\n",
      "   7.29726531e-02  5.80286935e-02  3.33359077e-02  4.63222716e-03\n",
      "   1.03198835e-02  1.23056253e-01  2.02923240e-02  1.20965804e-02\n",
      "   5.00950953e-03  1.01442089e-02  3.10803602e-02 -3.82419373e-02\n",
      "  -1.80381279e-02  2.18698300e-02 -6.25973541e-02 -3.92712595e-02\n",
      "   1.51160293e-02 -6.78215690e-03  2.00395261e-02  1.24984350e-01\n",
      "   3.75820398e-03 -3.71255911e-02 -3.20951081e-02  2.45522948e-02\n",
      "  -2.19443560e-03  4.09529980e-03  3.28648049e-02 -9.08622382e-02\n",
      "  -6.43976904e-03  4.72734100e-03 -5.60013047e-02  4.24957047e-02\n",
      "  -2.52755125e-02 -2.22273097e-02  2.17035587e-02  2.57631666e-02\n",
      "  -4.44950858e-02  3.85999916e-02 -1.90376454e-02 -1.73341158e-02\n",
      "   7.27040661e-03  4.36146978e-02 -7.43253906e-02 -2.22348277e-03\n",
      "   6.41345702e-02 -5.00145611e-03 -7.42518441e-02 -3.97653319e-02\n",
      "  -5.43543824e-02  5.59375379e-03 -1.10278341e-02  2.02478192e-02]]\n"
     ]
    }
   ],
   "source": [
    "representaciones = {}\n",
    "\n",
    "for combo in combos:\n",
    "    print(f'\\nGenerando representación para combinación: {combo}')\n",
    "\n",
    "    X_test_norm = X_test.apply(lambda x: NormalizarTexto(x, aplicar=combo))\n",
    "    X_train_norm = X_train.apply(lambda x: NormalizarTexto(x, aplicar=combo))\n",
    "\n",
    "    #Binarizada\n",
    "    vectorizer_binary = CountVectorizer(binary=True)\n",
    "    X_train_binary = vectorizer_binary.fit_transform(X_train_norm)\n",
    "    X_test_binary = vectorizer_binary.transform(X_test_norm)\n",
    "    representaciones[f'{combo} + Binarized'] = (X_train_binary, X_test_binary)\n",
    "    print(f'Combinacion: {combo} + Binarized\\nPreview:',X_train_binary[:5].toarray())\n",
    "\n",
    "    #Frecuencia\n",
    "    vectorizer_freq = CountVectorizer()\n",
    "    X_train_freq = vectorizer_freq.fit_transform(X_train_norm)\n",
    "    X_test_freq = vectorizer_freq.transform(X_test_norm)\n",
    "    representaciones[f'{combo} + Frequency'] = (X_train_freq, X_test_freq)\n",
    "    print(f'Combinacion: {combo} + Frequency\\nPreview:',X_train_freq[:5].toarray())\n",
    "\n",
    "    #TF-IDF\n",
    "    vectorizer_tfidf = TfidfVectorizer()\n",
    "    X_train_tfidf = vectorizer_tfidf.fit_transform(X_train_norm)\n",
    "    X_test_tfidf = vectorizer_tfidf.transform(X_test_norm)\n",
    "    representaciones[f'{combo} + TF-IDF'] = (X_train_tfidf, X_test_tfidf)\n",
    "    print(f'Combinacion: {combo} + TF-IDF\\nPreview:',X_train_tfidf[:5].toarray())\n",
    "\n",
    "    #TF-IDF + SVD\n",
    "    svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "    X_train_svd = svd.fit_transform(X_train_tfidf)\n",
    "    X_test_svd = svd.transform(X_test_tfidf)\n",
    "    representaciones[f'{combo} + TF-IDF + SVD'] = (X_train_svd, X_test_svd)\n",
    "    print(f'Combinacion: {combo} + TF-IDF + SVD\\nPreview:',X_train_svd[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GJrxRqI5uiW0"
   },
   "source": [
    "Entrenar y evaluar clasificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9Cnmt4XjuiW0"
   },
   "outputs": [],
   "source": [
    "def TrainandEvaluate(classifier,X_train,X_test,y_train,y_test):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    return f1, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OvQPo11EuiW0"
   },
   "outputs": [],
   "source": [
    "classifiers ={\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Naive Bayes': MultinomialNB(alpha = 0.5),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(200,100)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzT0k5gtuiW0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization'] + Binarized\n",
      "F1 Score para Logistic Regression con ['tokenization'] + Binarized: 0.452\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.32      0.38       104\n",
      "           2       0.28      0.21      0.24       145\n",
      "           3       0.41      0.35      0.38       422\n",
      "           4       0.41      0.38      0.40      1163\n",
      "           5       0.84      0.88      0.86      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.49      0.43      0.45      6043\n",
      "weighted avg       0.71      0.72      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization'] + Frequency\n",
      "F1 Score para Logistic Regression con ['tokenization'] + Frequency: 0.479\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.39      0.45       104\n",
      "           2       0.34      0.26      0.29       145\n",
      "           3       0.42      0.34      0.38       422\n",
      "           4       0.43      0.40      0.41      1163\n",
      "           5       0.84      0.89      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.51      0.45      0.48      6043\n",
      "weighted avg       0.71      0.73      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['tokenization'] + TF-IDF: 0.423\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.24      0.35       104\n",
      "           2       0.39      0.05      0.09       145\n",
      "           3       0.49      0.33      0.40       422\n",
      "           4       0.50      0.34      0.40      1163\n",
      "           5       0.81      0.94      0.87      4209\n",
      "\n",
      "    accuracy                           0.75      6043\n",
      "   macro avg       0.57      0.38      0.42      6043\n",
      "weighted avg       0.71      0.75      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['tokenization'] + TF-IDF + SVD: 0.337\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.10      0.16       104\n",
      "           2       0.10      0.01      0.01       145\n",
      "           3       0.42      0.26      0.32       422\n",
      "           4       0.46      0.26      0.33      1163\n",
      "           5       0.78      0.95      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.45      0.31      0.34      6043\n",
      "weighted avg       0.67      0.73      0.69      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words'] + Binarized\n",
      "F1 Score para Logistic Regression con ['stop_words'] + Binarized: 0.452\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.33      0.39       104\n",
      "           2       0.28      0.21      0.24       145\n",
      "           3       0.41      0.35      0.38       422\n",
      "           4       0.41      0.38      0.40      1163\n",
      "           5       0.84      0.88      0.86      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.48      0.43      0.45      6043\n",
      "weighted avg       0.70      0.72      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words'] + Frequency\n",
      "F1 Score para Logistic Regression con ['stop_words'] + Frequency: 0.477\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.38      0.44       104\n",
      "           2       0.34      0.26      0.29       145\n",
      "           3       0.43      0.34      0.38       422\n",
      "           4       0.43      0.39      0.41      1163\n",
      "           5       0.84      0.89      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.51      0.45      0.48      6043\n",
      "weighted avg       0.71      0.73      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['stop_words'] + TF-IDF: 0.428\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.25      0.37       104\n",
      "           2       0.42      0.06      0.10       145\n",
      "           3       0.50      0.33      0.40       422\n",
      "           4       0.49      0.34      0.40      1163\n",
      "           5       0.81      0.94      0.87      4209\n",
      "\n",
      "    accuracy                           0.75      6043\n",
      "   macro avg       0.58      0.38      0.43      6043\n",
      "weighted avg       0.72      0.75      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['stop_words'] + TF-IDF + SVD: 0.334\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.10      0.16       104\n",
      "           2       0.00      0.00      0.00       145\n",
      "           3       0.42      0.26      0.32       422\n",
      "           4       0.46      0.26      0.34      1163\n",
      "           5       0.78      0.95      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.43      0.31      0.33      6043\n",
      "weighted avg       0.67      0.73      0.69      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['lemmatization'] + Binarized\n",
      "F1 Score para Logistic Regression con ['lemmatization'] + Binarized: 0.452\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.33      0.39       104\n",
      "           2       0.28      0.21      0.24       145\n",
      "           3       0.41      0.35      0.38       422\n",
      "           4       0.41      0.38      0.40      1163\n",
      "           5       0.84      0.88      0.86      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.48      0.43      0.45      6043\n",
      "weighted avg       0.70      0.72      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['lemmatization'] + Frequency\n",
      "F1 Score para Logistic Regression con ['lemmatization'] + Frequency: 0.477\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.38      0.44       104\n",
      "           2       0.34      0.26      0.29       145\n",
      "           3       0.43      0.34      0.38       422\n",
      "           4       0.43      0.39      0.41      1163\n",
      "           5       0.84      0.89      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.51      0.45      0.48      6043\n",
      "weighted avg       0.71      0.73      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['lemmatization'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['lemmatization'] + TF-IDF: 0.428\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.25      0.37       104\n",
      "           2       0.42      0.06      0.10       145\n",
      "           3       0.50      0.33      0.40       422\n",
      "           4       0.49      0.34      0.40      1163\n",
      "           5       0.81      0.94      0.87      4209\n",
      "\n",
      "    accuracy                           0.75      6043\n",
      "   macro avg       0.58      0.38      0.43      6043\n",
      "weighted avg       0.72      0.75      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['lemmatization'] + TF-IDF + SVD: 0.334\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.10      0.16       104\n",
      "           2       0.00      0.00      0.00       145\n",
      "           3       0.42      0.26      0.32       422\n",
      "           4       0.46      0.26      0.34      1163\n",
      "           5       0.78      0.95      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.43      0.31      0.33      6043\n",
      "weighted avg       0.67      0.73      0.69      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['text_cleaning'] + Binarized\n",
      "F1 Score para Logistic Regression con ['text_cleaning'] + Binarized: 0.452\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.33      0.39       104\n",
      "           2       0.28      0.21      0.24       145\n",
      "           3       0.41      0.35      0.38       422\n",
      "           4       0.41      0.38      0.40      1163\n",
      "           5       0.84      0.88      0.86      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.48      0.43      0.45      6043\n",
      "weighted avg       0.70      0.72      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['text_cleaning'] + Frequency\n",
      "F1 Score para Logistic Regression con ['text_cleaning'] + Frequency: 0.477\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.38      0.44       104\n",
      "           2       0.34      0.26      0.29       145\n",
      "           3       0.43      0.34      0.38       422\n",
      "           4       0.43      0.39      0.41      1163\n",
      "           5       0.84      0.89      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.51      0.45      0.48      6043\n",
      "weighted avg       0.71      0.73      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['text_cleaning'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['text_cleaning'] + TF-IDF: 0.428\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.25      0.37       104\n",
      "           2       0.42      0.06      0.10       145\n",
      "           3       0.50      0.33      0.40       422\n",
      "           4       0.49      0.34      0.40      1163\n",
      "           5       0.81      0.94      0.87      4209\n",
      "\n",
      "    accuracy                           0.75      6043\n",
      "   macro avg       0.58      0.38      0.43      6043\n",
      "weighted avg       0.72      0.75      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['text_cleaning'] + TF-IDF + SVD: 0.334\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.10      0.16       104\n",
      "           2       0.00      0.00      0.00       145\n",
      "           3       0.42      0.26      0.32       422\n",
      "           4       0.46      0.26      0.34      1163\n",
      "           5       0.78      0.95      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.43      0.31      0.33      6043\n",
      "weighted avg       0.67      0.73      0.69      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words'] + Binarized\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words'] + Binarized: 0.452\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.37      0.42       104\n",
      "           2       0.34      0.20      0.25       145\n",
      "           3       0.42      0.31      0.36       422\n",
      "           4       0.40      0.35      0.37      1163\n",
      "           5       0.82      0.89      0.85      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.50      0.42      0.45      6043\n",
      "weighted avg       0.69      0.72      0.70      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words'] + Frequency\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words'] + Frequency: 0.471\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.52      0.38      0.44       104\n",
      "           2       0.39      0.22      0.28       145\n",
      "           3       0.45      0.35      0.39       422\n",
      "           4       0.42      0.36      0.39      1163\n",
      "           5       0.82      0.89      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.52      0.44      0.47      6043\n",
      "weighted avg       0.70      0.73      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words'] + TF-IDF: 0.347\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.10      0.16       104\n",
      "           2       0.50      0.03      0.06       145\n",
      "           3       0.46      0.24      0.31       422\n",
      "           4       0.44      0.28      0.34      1163\n",
      "           5       0.79      0.95      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.54      0.32      0.35      6043\n",
      "weighted avg       0.68      0.73      0.69      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words'] + TF-IDF + SVD: 0.254\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.01      0.02       104\n",
      "           2       0.00      0.00      0.00       145\n",
      "           3       0.33      0.10      0.16       422\n",
      "           4       0.41      0.18      0.25      1163\n",
      "           5       0.75      0.97      0.84      4209\n",
      "\n",
      "    accuracy                           0.71      6043\n",
      "   macro avg       0.50      0.25      0.25      6043\n",
      "weighted avg       0.64      0.71      0.65      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'lemmatization'] + Binarized\n",
      "F1 Score para Logistic Regression con ['tokenization', 'lemmatization'] + Binarized: 0.463\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      0.37      0.40       104\n",
      "           2       0.28      0.21      0.24       145\n",
      "           3       0.43      0.38      0.40       422\n",
      "           4       0.44      0.38      0.41      1163\n",
      "           5       0.84      0.89      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.48      0.45      0.46      6043\n",
      "weighted avg       0.71      0.73      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'lemmatization'] + Frequency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dern9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score para Logistic Regression con ['tokenization', 'lemmatization'] + Frequency: 0.492\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.52      0.47      0.49       104\n",
      "           2       0.33      0.25      0.28       145\n",
      "           3       0.45      0.40      0.42       422\n",
      "           4       0.44      0.37      0.40      1163\n",
      "           5       0.84      0.89      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.51      0.48      0.49      6043\n",
      "weighted avg       0.72      0.73      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'lemmatization'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['tokenization', 'lemmatization'] + TF-IDF: 0.446\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.30      0.40       104\n",
      "           2       0.41      0.09      0.15       145\n",
      "           3       0.51      0.33      0.40       422\n",
      "           4       0.50      0.35      0.41      1163\n",
      "           5       0.81      0.94      0.87      4209\n",
      "\n",
      "    accuracy                           0.75      6043\n",
      "   macro avg       0.56      0.40      0.45      6043\n",
      "weighted avg       0.72      0.75      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['tokenization', 'lemmatization'] + TF-IDF + SVD: 0.355\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.52      0.15      0.24       104\n",
      "           2       0.10      0.01      0.01       145\n",
      "           3       0.40      0.26      0.31       422\n",
      "           4       0.48      0.28      0.35      1163\n",
      "           5       0.79      0.95      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.46      0.33      0.36      6043\n",
      "weighted avg       0.68      0.73      0.69      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Logistic Regression con ['tokenization', 'text_cleaning'] + Binarized: 0.452\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.48      0.32      0.38       104\n",
      "           2       0.28      0.21      0.24       145\n",
      "           3       0.41      0.35      0.38       422\n",
      "           4       0.41      0.38      0.40      1163\n",
      "           5       0.84      0.88      0.86      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.48      0.43      0.45      6043\n",
      "weighted avg       0.71      0.72      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Logistic Regression con ['tokenization', 'text_cleaning'] + Frequency: 0.477\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.38      0.44       104\n",
      "           2       0.35      0.26      0.29       145\n",
      "           3       0.42      0.34      0.38       422\n",
      "           4       0.43      0.40      0.41      1163\n",
      "           5       0.84      0.88      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.51      0.45      0.48      6043\n",
      "weighted avg       0.71      0.73      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['tokenization', 'text_cleaning'] + TF-IDF: 0.423\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.24      0.35       104\n",
      "           2       0.39      0.05      0.09       145\n",
      "           3       0.50      0.33      0.40       422\n",
      "           4       0.50      0.34      0.40      1163\n",
      "           5       0.81      0.95      0.87      4209\n",
      "\n",
      "    accuracy                           0.75      6043\n",
      "   macro avg       0.57      0.38      0.42      6043\n",
      "weighted avg       0.72      0.75      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['tokenization', 'text_cleaning'] + TF-IDF + SVD: 0.334\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.48      0.10      0.16       104\n",
      "           2       0.00      0.00      0.00       145\n",
      "           3       0.42      0.26      0.32       422\n",
      "           4       0.46      0.26      0.33      1163\n",
      "           5       0.78      0.95      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.43      0.31      0.33      6043\n",
      "weighted avg       0.67      0.73      0.69      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'lemmatization'] + Binarized\n",
      "F1 Score para Logistic Regression con ['stop_words', 'lemmatization'] + Binarized: 0.452\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.33      0.39       104\n",
      "           2       0.28      0.21      0.24       145\n",
      "           3       0.41      0.35      0.38       422\n",
      "           4       0.41      0.38      0.40      1163\n",
      "           5       0.84      0.88      0.86      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.48      0.43      0.45      6043\n",
      "weighted avg       0.70      0.72      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'lemmatization'] + Frequency\n",
      "F1 Score para Logistic Regression con ['stop_words', 'lemmatization'] + Frequency: 0.477\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.38      0.44       104\n",
      "           2       0.34      0.26      0.29       145\n",
      "           3       0.43      0.34      0.38       422\n",
      "           4       0.43      0.39      0.41      1163\n",
      "           5       0.84      0.89      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.51      0.45      0.48      6043\n",
      "weighted avg       0.71      0.73      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'lemmatization'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['stop_words', 'lemmatization'] + TF-IDF: 0.428\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.25      0.37       104\n",
      "           2       0.42      0.06      0.10       145\n",
      "           3       0.50      0.33      0.40       422\n",
      "           4       0.49      0.34      0.40      1163\n",
      "           5       0.81      0.94      0.87      4209\n",
      "\n",
      "    accuracy                           0.75      6043\n",
      "   macro avg       0.58      0.38      0.43      6043\n",
      "weighted avg       0.72      0.75      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['stop_words', 'lemmatization'] + TF-IDF + SVD: 0.334\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.10      0.16       104\n",
      "           2       0.00      0.00      0.00       145\n",
      "           3       0.42      0.26      0.32       422\n",
      "           4       0.46      0.26      0.34      1163\n",
      "           5       0.78      0.95      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.43      0.31      0.33      6043\n",
      "weighted avg       0.67      0.73      0.69      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'text_cleaning'] + Binarized\n",
      "F1 Score para Logistic Regression con ['stop_words', 'text_cleaning'] + Binarized: 0.452\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.33      0.39       104\n",
      "           2       0.28      0.21      0.24       145\n",
      "           3       0.41      0.35      0.38       422\n",
      "           4       0.41      0.38      0.40      1163\n",
      "           5       0.84      0.88      0.86      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.48      0.43      0.45      6043\n",
      "weighted avg       0.70      0.72      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'text_cleaning'] + Frequency\n",
      "F1 Score para Logistic Regression con ['stop_words', 'text_cleaning'] + Frequency: 0.477\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.38      0.44       104\n",
      "           2       0.34      0.26      0.29       145\n",
      "           3       0.43      0.34      0.38       422\n",
      "           4       0.43      0.39      0.41      1163\n",
      "           5       0.84      0.89      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.51      0.45      0.48      6043\n",
      "weighted avg       0.71      0.73      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['stop_words', 'text_cleaning'] + TF-IDF: 0.428\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.25      0.37       104\n",
      "           2       0.42      0.06      0.10       145\n",
      "           3       0.50      0.33      0.40       422\n",
      "           4       0.49      0.34      0.40      1163\n",
      "           5       0.81      0.94      0.87      4209\n",
      "\n",
      "    accuracy                           0.75      6043\n",
      "   macro avg       0.58      0.38      0.43      6043\n",
      "weighted avg       0.72      0.75      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['stop_words', 'text_cleaning'] + TF-IDF + SVD: 0.334\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.10      0.16       104\n",
      "           2       0.00      0.00      0.00       145\n",
      "           3       0.42      0.26      0.32       422\n",
      "           4       0.46      0.26      0.34      1163\n",
      "           5       0.78      0.95      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.43      0.31      0.33      6043\n",
      "weighted avg       0.67      0.73      0.69      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Logistic Regression con ['lemmatization', 'text_cleaning'] + Binarized: 0.452\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.33      0.39       104\n",
      "           2       0.28      0.21      0.24       145\n",
      "           3       0.41      0.35      0.38       422\n",
      "           4       0.41      0.38      0.40      1163\n",
      "           5       0.84      0.88      0.86      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.48      0.43      0.45      6043\n",
      "weighted avg       0.70      0.72      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Logistic Regression con ['lemmatization', 'text_cleaning'] + Frequency: 0.477\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.38      0.44       104\n",
      "           2       0.34      0.26      0.29       145\n",
      "           3       0.43      0.34      0.38       422\n",
      "           4       0.43      0.39      0.41      1163\n",
      "           5       0.84      0.89      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.51      0.45      0.48      6043\n",
      "weighted avg       0.71      0.73      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['lemmatization', 'text_cleaning'] + TF-IDF: 0.428\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.25      0.37       104\n",
      "           2       0.42      0.06      0.10       145\n",
      "           3       0.50      0.33      0.40       422\n",
      "           4       0.49      0.34      0.40      1163\n",
      "           5       0.81      0.94      0.87      4209\n",
      "\n",
      "    accuracy                           0.75      6043\n",
      "   macro avg       0.58      0.38      0.43      6043\n",
      "weighted avg       0.72      0.75      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.334\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.10      0.16       104\n",
      "           2       0.00      0.00      0.00       145\n",
      "           3       0.42      0.26      0.32       422\n",
      "           4       0.46      0.26      0.34      1163\n",
      "           5       0.78      0.95      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.43      0.31      0.33      6043\n",
      "weighted avg       0.67      0.73      0.69      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'lemmatization'] + Binarized\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization'] + Binarized: 0.450\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.43      0.35      0.38       104\n",
      "           2       0.30      0.22      0.25       145\n",
      "           3       0.45      0.33      0.38       422\n",
      "           4       0.41      0.35      0.38      1163\n",
      "           5       0.82      0.89      0.86      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.48      0.43      0.45      6043\n",
      "weighted avg       0.70      0.72      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'lemmatization'] + Frequency\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization'] + Frequency: 0.464\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.45      0.36      0.40       104\n",
      "           2       0.36      0.26      0.30       145\n",
      "           3       0.44      0.34      0.38       422\n",
      "           4       0.42      0.34      0.38      1163\n",
      "           5       0.82      0.90      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.50      0.44      0.46      6043\n",
      "weighted avg       0.70      0.73      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'lemmatization'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF: 0.383\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.18      0.28       104\n",
      "           2       0.60      0.06      0.11       145\n",
      "           3       0.45      0.23      0.31       422\n",
      "           4       0.45      0.29      0.35      1163\n",
      "           5       0.79      0.95      0.86      4209\n",
      "\n",
      "    accuracy                           0.74      6043\n",
      "   macro avg       0.58      0.34      0.38      6043\n",
      "weighted avg       0.69      0.74      0.70      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD: 0.284\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.06      0.11       104\n",
      "           2       0.29      0.01      0.03       145\n",
      "           3       0.32      0.13      0.18       422\n",
      "           4       0.44      0.19      0.26      1163\n",
      "           5       0.75      0.96      0.84      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.49      0.27      0.28      6043\n",
      "weighted avg       0.65      0.72      0.65      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'text_cleaning'] + Binarized\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'text_cleaning'] + Binarized: 0.451\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.37      0.42       104\n",
      "           2       0.34      0.20      0.25       145\n",
      "           3       0.42      0.31      0.36       422\n",
      "           4       0.41      0.35      0.38      1163\n",
      "           5       0.82      0.89      0.85      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.49      0.42      0.45      6043\n",
      "weighted avg       0.70      0.72      0.70      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'text_cleaning'] + Frequency\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'text_cleaning'] + Frequency: 0.469\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.38      0.44       104\n",
      "           2       0.38      0.21      0.27       145\n",
      "           3       0.45      0.35      0.39       422\n",
      "           4       0.42      0.36      0.39      1163\n",
      "           5       0.82      0.89      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.52      0.44      0.47      6043\n",
      "weighted avg       0.70      0.73      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF: 0.344\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.10      0.16       104\n",
      "           2       0.38      0.03      0.06       145\n",
      "           3       0.45      0.23      0.30       422\n",
      "           4       0.44      0.28      0.34      1163\n",
      "           5       0.79      0.94      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.51      0.32      0.34      6043\n",
      "weighted avg       0.68      0.73      0.69      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD: 0.252\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.01      0.02       104\n",
      "           2       0.00      0.00      0.00       145\n",
      "           3       0.32      0.10      0.15       422\n",
      "           4       0.41      0.17      0.25      1163\n",
      "           5       0.75      0.96      0.84      4209\n",
      "\n",
      "    accuracy                           0.71      6043\n",
      "   macro avg       0.50      0.25      0.25      6043\n",
      "weighted avg       0.64      0.71      0.65      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Logistic Regression con ['tokenization', 'lemmatization', 'text_cleaning'] + Binarized: 0.462\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      0.36      0.39       104\n",
      "           2       0.29      0.23      0.25       145\n",
      "           3       0.42      0.39      0.41       422\n",
      "           4       0.43      0.37      0.40      1163\n",
      "           5       0.84      0.89      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.48      0.45      0.46      6043\n",
      "weighted avg       0.71      0.73      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + Frequency\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dern9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score para Logistic Regression con ['tokenization', 'lemmatization', 'text_cleaning'] + Frequency: 0.498\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.54      0.49      0.52       104\n",
      "           2       0.36      0.28      0.31       145\n",
      "           3       0.43      0.37      0.40       422\n",
      "           4       0.43      0.37      0.40      1163\n",
      "           5       0.84      0.89      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.52      0.48      0.50      6043\n",
      "weighted avg       0.71      0.73      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF: 0.444\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.30      0.40       104\n",
      "           2       0.38      0.08      0.14       145\n",
      "           3       0.51      0.33      0.40       422\n",
      "           4       0.50      0.35      0.41      1163\n",
      "           5       0.81      0.94      0.87      4209\n",
      "\n",
      "    accuracy                           0.75      6043\n",
      "   macro avg       0.56      0.40      0.44      6043\n",
      "weighted avg       0.72      0.75      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.356\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.16      0.25       104\n",
      "           2       0.10      0.01      0.01       145\n",
      "           3       0.39      0.24      0.30       422\n",
      "           4       0.47      0.28      0.35      1163\n",
      "           5       0.79      0.95      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.46      0.33      0.36      6043\n",
      "weighted avg       0.68      0.73      0.69      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Logistic Regression con ['stop_words', 'lemmatization', 'text_cleaning'] + Binarized: 0.452\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.33      0.39       104\n",
      "           2       0.28      0.21      0.24       145\n",
      "           3       0.41      0.35      0.38       422\n",
      "           4       0.41      0.38      0.40      1163\n",
      "           5       0.84      0.88      0.86      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.48      0.43      0.45      6043\n",
      "weighted avg       0.70      0.72      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Logistic Regression con ['stop_words', 'lemmatization', 'text_cleaning'] + Frequency: 0.477\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.38      0.44       104\n",
      "           2       0.34      0.26      0.29       145\n",
      "           3       0.43      0.34      0.38       422\n",
      "           4       0.43      0.39      0.41      1163\n",
      "           5       0.84      0.89      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.51      0.45      0.48      6043\n",
      "weighted avg       0.71      0.73      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF: 0.428\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.25      0.37       104\n",
      "           2       0.42      0.06      0.10       145\n",
      "           3       0.50      0.33      0.40       422\n",
      "           4       0.49      0.34      0.40      1163\n",
      "           5       0.81      0.94      0.87      4209\n",
      "\n",
      "    accuracy                           0.75      6043\n",
      "   macro avg       0.58      0.38      0.43      6043\n",
      "weighted avg       0.72      0.75      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.334\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.10      0.16       104\n",
      "           2       0.00      0.00      0.00       145\n",
      "           3       0.42      0.26      0.32       422\n",
      "           4       0.46      0.26      0.34      1163\n",
      "           5       0.78      0.95      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.43      0.31      0.33      6043\n",
      "weighted avg       0.67      0.73      0.69      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized: 0.448\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      0.35      0.39       104\n",
      "           2       0.29      0.22      0.25       145\n",
      "           3       0.43      0.32      0.37       422\n",
      "           4       0.41      0.35      0.38      1163\n",
      "           5       0.82      0.89      0.86      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.48      0.42      0.45      6043\n",
      "weighted avg       0.70      0.72      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency: 0.464\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      0.35      0.39       104\n",
      "           2       0.36      0.29      0.32       145\n",
      "           3       0.43      0.33      0.37       422\n",
      "           4       0.41      0.34      0.38      1163\n",
      "           5       0.83      0.89      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.50      0.44      0.46      6043\n",
      "weighted avg       0.70      0.73      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF: 0.386\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.20      0.30       104\n",
      "           2       0.53      0.06      0.10       145\n",
      "           3       0.45      0.24      0.31       422\n",
      "           4       0.45      0.29      0.35      1163\n",
      "           5       0.79      0.94      0.86      4209\n",
      "\n",
      "    accuracy                           0.74      6043\n",
      "   macro avg       0.57      0.35      0.39      6043\n",
      "weighted avg       0.69      0.74      0.70      6043\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.291\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.10      0.17       104\n",
      "           2       0.25      0.01      0.01       145\n",
      "           3       0.31      0.12      0.17       422\n",
      "           4       0.42      0.19      0.26      1163\n",
      "           5       0.75      0.96      0.84      4209\n",
      "\n",
      "    accuracy                           0.71      6043\n",
      "   macro avg       0.49      0.27      0.29      6043\n",
      "weighted avg       0.65      0.71      0.65      6043\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization'] + Binarized\n",
      "F1 Score para Naive Bayes con ['tokenization'] + Binarized: 0.359\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.09      0.15       104\n",
      "           2       0.42      0.03      0.06       145\n",
      "           3       0.34      0.26      0.30       422\n",
      "           4       0.38      0.52      0.44      1163\n",
      "           5       0.86      0.84      0.85      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.51      0.35      0.36      6043\n",
      "weighted avg       0.71      0.70      0.70      6043\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization'] + Frequency\n",
      "F1 Score para Naive Bayes con ['tokenization'] + Frequency: 0.402\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.16      0.26       104\n",
      "           2       0.33      0.06      0.10       145\n",
      "           3       0.35      0.36      0.35       422\n",
      "           4       0.39      0.55      0.45      1163\n",
      "           5       0.88      0.81      0.84      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.51      0.39      0.40      6043\n",
      "weighted avg       0.73      0.70      0.71      6043\n",
      "\n",
      "Skipping Naive Bayes with ['tokenization'] + TF-IDF\n",
      "Skipping Naive Bayes with ['tokenization'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['stop_words'] + Binarized\n",
      "F1 Score para Naive Bayes con ['stop_words'] + Binarized: 0.359\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['stop_words'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.09      0.15       104\n",
      "           2       0.42      0.03      0.06       145\n",
      "           3       0.35      0.26      0.30       422\n",
      "           4       0.38      0.52      0.44      1163\n",
      "           5       0.86      0.84      0.85      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.51      0.35      0.36      6043\n",
      "weighted avg       0.71      0.70      0.70      6043\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['stop_words'] + Frequency\n",
      "F1 Score para Naive Bayes con ['stop_words'] + Frequency: 0.402\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['stop_words'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.16      0.26       104\n",
      "           2       0.33      0.06      0.10       145\n",
      "           3       0.35      0.36      0.35       422\n",
      "           4       0.39      0.55      0.45      1163\n",
      "           5       0.88      0.81      0.84      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.51      0.39      0.40      6043\n",
      "weighted avg       0.73      0.70      0.71      6043\n",
      "\n",
      "Skipping Naive Bayes with ['stop_words'] + TF-IDF\n",
      "Skipping Naive Bayes with ['stop_words'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['lemmatization'] + Binarized\n",
      "F1 Score para Naive Bayes con ['lemmatization'] + Binarized: 0.359\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.09      0.15       104\n",
      "           2       0.42      0.03      0.06       145\n",
      "           3       0.35      0.26      0.30       422\n",
      "           4       0.38      0.52      0.44      1163\n",
      "           5       0.86      0.84      0.85      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.51      0.35      0.36      6043\n",
      "weighted avg       0.71      0.70      0.70      6043\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['lemmatization'] + Frequency\n",
      "F1 Score para Naive Bayes con ['lemmatization'] + Frequency: 0.402\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.16      0.26       104\n",
      "           2       0.33      0.06      0.10       145\n",
      "           3       0.35      0.36      0.35       422\n",
      "           4       0.39      0.55      0.45      1163\n",
      "           5       0.88      0.81      0.84      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.51      0.39      0.40      6043\n",
      "weighted avg       0.73      0.70      0.71      6043\n",
      "\n",
      "Skipping Naive Bayes with ['lemmatization'] + TF-IDF\n",
      "Skipping Naive Bayes with ['lemmatization'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['text_cleaning'] + Binarized\n",
      "F1 Score para Naive Bayes con ['text_cleaning'] + Binarized: 0.359\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.09      0.15       104\n",
      "           2       0.42      0.03      0.06       145\n",
      "           3       0.35      0.26      0.30       422\n",
      "           4       0.38      0.52      0.44      1163\n",
      "           5       0.86      0.84      0.85      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.51      0.35      0.36      6043\n",
      "weighted avg       0.71      0.70      0.70      6043\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['text_cleaning'] + Frequency\n",
      "F1 Score para Naive Bayes con ['text_cleaning'] + Frequency: 0.402\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.16      0.26       104\n",
      "           2       0.33      0.06      0.10       145\n",
      "           3       0.35      0.36      0.35       422\n",
      "           4       0.39      0.55      0.45      1163\n",
      "           5       0.88      0.81      0.84      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.51      0.39      0.40      6043\n",
      "weighted avg       0.73      0.70      0.71      6043\n",
      "\n",
      "Skipping Naive Bayes with ['text_cleaning'] + TF-IDF\n",
      "Skipping Naive Bayes with ['text_cleaning'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'stop_words'] + Binarized\n",
      "F1 Score para Naive Bayes con ['tokenization', 'stop_words'] + Binarized: 0.372\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'stop_words'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.13      0.21       104\n",
      "           2       0.44      0.05      0.09       145\n",
      "           3       0.34      0.25      0.29       422\n",
      "           4       0.37      0.51      0.43      1163\n",
      "           5       0.85      0.83      0.84      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.50      0.35      0.37      6043\n",
      "weighted avg       0.71      0.70      0.69      6043\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'stop_words'] + Frequency\n",
      "F1 Score para Naive Bayes con ['tokenization', 'stop_words'] + Frequency: 0.390\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'stop_words'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.17      0.26       104\n",
      "           2       0.38      0.07      0.12       145\n",
      "           3       0.34      0.28      0.31       422\n",
      "           4       0.37      0.52      0.43      1163\n",
      "           5       0.86      0.82      0.84      4209\n",
      "\n",
      "    accuracy                           0.69      6043\n",
      "   macro avg       0.49      0.37      0.39      6043\n",
      "weighted avg       0.71      0.69      0.70      6043\n",
      "\n",
      "Skipping Naive Bayes with ['tokenization', 'stop_words'] + TF-IDF\n",
      "Skipping Naive Bayes with ['tokenization', 'stop_words'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'lemmatization'] + Binarized\n",
      "F1 Score para Naive Bayes con ['tokenization', 'lemmatization'] + Binarized: 0.353\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.09      0.15       104\n",
      "           2       0.33      0.03      0.05       145\n",
      "           3       0.35      0.25      0.29       422\n",
      "           4       0.37      0.51      0.43      1163\n",
      "           5       0.85      0.83      0.84      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.48      0.34      0.35      6043\n",
      "weighted avg       0.71      0.70      0.69      6043\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'lemmatization'] + Frequency\n",
      "F1 Score para Naive Bayes con ['tokenization', 'lemmatization'] + Frequency: 0.402\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.15      0.24       104\n",
      "           2       0.39      0.06      0.11       145\n",
      "           3       0.36      0.36      0.36       422\n",
      "           4       0.39      0.56      0.46      1163\n",
      "           5       0.88      0.81      0.84      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.52      0.39      0.40      6043\n",
      "weighted avg       0.73      0.70      0.71      6043\n",
      "\n",
      "Skipping Naive Bayes with ['tokenization', 'lemmatization'] + TF-IDF\n",
      "Skipping Naive Bayes with ['tokenization', 'lemmatization'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Naive Bayes con ['tokenization', 'text_cleaning'] + Binarized: 0.359\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.09      0.15       104\n",
      "           2       0.42      0.03      0.06       145\n",
      "           3       0.34      0.26      0.30       422\n",
      "           4       0.38      0.52      0.44      1163\n",
      "           5       0.86      0.84      0.85      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.51      0.35      0.36      6043\n",
      "weighted avg       0.71      0.70      0.70      6043\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Naive Bayes con ['tokenization', 'text_cleaning'] + Frequency: 0.402\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.16      0.26       104\n",
      "           2       0.33      0.06      0.10       145\n",
      "           3       0.35      0.36      0.35       422\n",
      "           4       0.39      0.55      0.45      1163\n",
      "           5       0.88      0.81      0.84      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.51      0.39      0.40      6043\n",
      "weighted avg       0.73      0.70      0.71      6043\n",
      "\n",
      "Skipping Naive Bayes with ['tokenization', 'text_cleaning'] + TF-IDF\n",
      "Skipping Naive Bayes with ['tokenization', 'text_cleaning'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['stop_words', 'lemmatization'] + Binarized\n",
      "F1 Score para Naive Bayes con ['stop_words', 'lemmatization'] + Binarized: 0.359\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['stop_words', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.09      0.15       104\n",
      "           2       0.42      0.03      0.06       145\n",
      "           3       0.35      0.26      0.30       422\n",
      "           4       0.38      0.52      0.44      1163\n",
      "           5       0.86      0.84      0.85      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.51      0.35      0.36      6043\n",
      "weighted avg       0.71      0.70      0.70      6043\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['stop_words', 'lemmatization'] + Frequency\n",
      "F1 Score para Naive Bayes con ['stop_words', 'lemmatization'] + Frequency: 0.402\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['stop_words', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.16      0.26       104\n",
      "           2       0.33      0.06      0.10       145\n",
      "           3       0.35      0.36      0.35       422\n",
      "           4       0.39      0.55      0.45      1163\n",
      "           5       0.88      0.81      0.84      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.51      0.39      0.40      6043\n",
      "weighted avg       0.73      0.70      0.71      6043\n",
      "\n",
      "Skipping Naive Bayes with ['stop_words', 'lemmatization'] + TF-IDF\n",
      "Skipping Naive Bayes with ['stop_words', 'lemmatization'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['stop_words', 'text_cleaning'] + Binarized\n",
      "F1 Score para Naive Bayes con ['stop_words', 'text_cleaning'] + Binarized: 0.359\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['stop_words', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.09      0.15       104\n",
      "           2       0.42      0.03      0.06       145\n",
      "           3       0.35      0.26      0.30       422\n",
      "           4       0.38      0.52      0.44      1163\n",
      "           5       0.86      0.84      0.85      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.51      0.35      0.36      6043\n",
      "weighted avg       0.71      0.70      0.70      6043\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['stop_words', 'text_cleaning'] + Frequency\n",
      "F1 Score para Naive Bayes con ['stop_words', 'text_cleaning'] + Frequency: 0.402\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['stop_words', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.16      0.26       104\n",
      "           2       0.33      0.06      0.10       145\n",
      "           3       0.35      0.36      0.35       422\n",
      "           4       0.39      0.55      0.45      1163\n",
      "           5       0.88      0.81      0.84      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.51      0.39      0.40      6043\n",
      "weighted avg       0.73      0.70      0.71      6043\n",
      "\n",
      "Skipping Naive Bayes with ['stop_words', 'text_cleaning'] + TF-IDF\n",
      "Skipping Naive Bayes with ['stop_words', 'text_cleaning'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Naive Bayes con ['lemmatization', 'text_cleaning'] + Binarized: 0.359\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.09      0.15       104\n",
      "           2       0.42      0.03      0.06       145\n",
      "           3       0.35      0.26      0.30       422\n",
      "           4       0.38      0.52      0.44      1163\n",
      "           5       0.86      0.84      0.85      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.51      0.35      0.36      6043\n",
      "weighted avg       0.71      0.70      0.70      6043\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Naive Bayes con ['lemmatization', 'text_cleaning'] + Frequency: 0.402\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.16      0.26       104\n",
      "           2       0.33      0.06      0.10       145\n",
      "           3       0.35      0.36      0.35       422\n",
      "           4       0.39      0.55      0.45      1163\n",
      "           5       0.88      0.81      0.84      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.51      0.39      0.40      6043\n",
      "weighted avg       0.73      0.70      0.71      6043\n",
      "\n",
      "Skipping Naive Bayes with ['lemmatization', 'text_cleaning'] + TF-IDF\n",
      "Skipping Naive Bayes with ['lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'stop_words', 'lemmatization'] + Binarized\n",
      "F1 Score para Naive Bayes con ['tokenization', 'stop_words', 'lemmatization'] + Binarized: 0.357\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'stop_words', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.55      0.11      0.18       104\n",
      "           2       0.33      0.03      0.06       145\n",
      "           3       0.33      0.23      0.27       422\n",
      "           4       0.37      0.53      0.44      1163\n",
      "           5       0.85      0.82      0.84      4209\n",
      "\n",
      "    accuracy                           0.69      6043\n",
      "   macro avg       0.49      0.34      0.36      6043\n",
      "weighted avg       0.71      0.69      0.69      6043\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'stop_words', 'lemmatization'] + Frequency\n",
      "F1 Score para Naive Bayes con ['tokenization', 'stop_words', 'lemmatization'] + Frequency: 0.377\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'stop_words', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.47      0.14      0.22       104\n",
      "           2       0.36      0.06      0.10       145\n",
      "           3       0.33      0.28      0.30       422\n",
      "           4       0.37      0.52      0.43      1163\n",
      "           5       0.86      0.81      0.83      4209\n",
      "\n",
      "    accuracy                           0.69      6043\n",
      "   macro avg       0.48      0.36      0.38      6043\n",
      "weighted avg       0.71      0.69      0.69      6043\n",
      "\n",
      "Skipping Naive Bayes with ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF\n",
      "Skipping Naive Bayes with ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'stop_words', 'text_cleaning'] + Binarized\n",
      "F1 Score para Naive Bayes con ['tokenization', 'stop_words', 'text_cleaning'] + Binarized: 0.372\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'stop_words', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.13      0.21       104\n",
      "           2       0.44      0.05      0.09       145\n",
      "           3       0.34      0.25      0.29       422\n",
      "           4       0.37      0.51      0.43      1163\n",
      "           5       0.85      0.83      0.84      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.50      0.35      0.37      6043\n",
      "weighted avg       0.71      0.70      0.69      6043\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'stop_words', 'text_cleaning'] + Frequency\n",
      "F1 Score para Naive Bayes con ['tokenization', 'stop_words', 'text_cleaning'] + Frequency: 0.390\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'stop_words', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.17      0.26       104\n",
      "           2       0.38      0.07      0.12       145\n",
      "           3       0.34      0.28      0.31       422\n",
      "           4       0.37      0.52      0.43      1163\n",
      "           5       0.86      0.82      0.84      4209\n",
      "\n",
      "    accuracy                           0.69      6043\n",
      "   macro avg       0.49      0.37      0.39      6043\n",
      "weighted avg       0.71      0.69      0.69      6043\n",
      "\n",
      "Skipping Naive Bayes with ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF\n",
      "Skipping Naive Bayes with ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Naive Bayes con ['tokenization', 'lemmatization', 'text_cleaning'] + Binarized: 0.352\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.47      0.08      0.13       104\n",
      "           2       0.40      0.03      0.05       145\n",
      "           3       0.35      0.27      0.30       422\n",
      "           4       0.37      0.51      0.43      1163\n",
      "           5       0.85      0.83      0.84      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.49      0.34      0.35      6043\n",
      "weighted avg       0.71      0.70      0.69      6043\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Naive Bayes con ['tokenization', 'lemmatization', 'text_cleaning'] + Frequency: 0.400\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.55      0.15      0.24       104\n",
      "           2       0.42      0.06      0.10       145\n",
      "           3       0.36      0.37      0.36       422\n",
      "           4       0.39      0.56      0.46      1163\n",
      "           5       0.88      0.81      0.84      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.52      0.39      0.40      6043\n",
      "weighted avg       0.73      0.70      0.71      6043\n",
      "\n",
      "Skipping Naive Bayes with ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "Skipping Naive Bayes with ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Naive Bayes con ['stop_words', 'lemmatization', 'text_cleaning'] + Binarized: 0.359\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['stop_words', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.09      0.15       104\n",
      "           2       0.42      0.03      0.06       145\n",
      "           3       0.35      0.26      0.30       422\n",
      "           4       0.38      0.52      0.44      1163\n",
      "           5       0.86      0.84      0.85      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.51      0.35      0.36      6043\n",
      "weighted avg       0.71      0.70      0.70      6043\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Naive Bayes con ['stop_words', 'lemmatization', 'text_cleaning'] + Frequency: 0.402\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['stop_words', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.16      0.26       104\n",
      "           2       0.33      0.06      0.10       145\n",
      "           3       0.35      0.36      0.35       422\n",
      "           4       0.39      0.55      0.45      1163\n",
      "           5       0.88      0.81      0.84      4209\n",
      "\n",
      "    accuracy                           0.70      6043\n",
      "   macro avg       0.51      0.39      0.40      6043\n",
      "weighted avg       0.73      0.70      0.71      6043\n",
      "\n",
      "Skipping Naive Bayes with ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "Skipping Naive Bayes with ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Naive Bayes con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized: 0.354\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.45      0.10      0.16       104\n",
      "           2       0.29      0.03      0.06       145\n",
      "           3       0.35      0.24      0.29       422\n",
      "           4       0.36      0.52      0.43      1163\n",
      "           5       0.85      0.82      0.84      4209\n",
      "\n",
      "    accuracy                           0.69      6043\n",
      "   macro avg       0.46      0.34      0.35      6043\n",
      "weighted avg       0.70      0.69      0.69      6043\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Naive Bayes con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency: 0.380\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.47      0.14      0.22       104\n",
      "           2       0.39      0.06      0.11       145\n",
      "           3       0.33      0.28      0.30       422\n",
      "           4       0.37      0.52      0.43      1163\n",
      "           5       0.86      0.81      0.83      4209\n",
      "\n",
      "    accuracy                           0.69      6043\n",
      "   macro avg       0.48      0.36      0.38      6043\n",
      "weighted avg       0.71      0.69      0.69      6043\n",
      "\n",
      "Skipping Naive Bayes with ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "Skipping Naive Bayes with ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization'] + Binarized\n",
      "F1 Score para Neural Network con ['tokenization'] + Binarized: 0.455\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.33      0.40       104\n",
      "           2       0.31      0.20      0.24       145\n",
      "           3       0.40      0.35      0.37       422\n",
      "           4       0.41      0.40      0.41      1163\n",
      "           5       0.84      0.87      0.85      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.49      0.43      0.46      6043\n",
      "weighted avg       0.71      0.72      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization'] + Frequency\n",
      "F1 Score para Neural Network con ['tokenization'] + Frequency: 0.469\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.55      0.40      0.47       104\n",
      "           2       0.29      0.21      0.24       145\n",
      "           3       0.40      0.36      0.38       422\n",
      "           4       0.42      0.38      0.40      1163\n",
      "           5       0.83      0.88      0.85      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.50      0.45      0.47      6043\n",
      "weighted avg       0.70      0.72      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization'] + TF-IDF\n",
      "F1 Score para Neural Network con ['tokenization'] + TF-IDF: 0.476\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.33      0.42       104\n",
      "           2       0.36      0.26      0.30       145\n",
      "           3       0.41      0.39      0.40       422\n",
      "           4       0.41      0.40      0.41      1163\n",
      "           5       0.84      0.86      0.85      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.52      0.45      0.48      6043\n",
      "weighted avg       0.71      0.72      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization'] + TF-IDF + SVD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dern9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score para Neural Network con ['tokenization'] + TF-IDF + SVD: 0.411\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.31      0.28      0.29       104\n",
      "           2       0.23      0.21      0.22       145\n",
      "           3       0.34      0.33      0.34       422\n",
      "           4       0.37      0.37      0.37      1163\n",
      "           5       0.83      0.84      0.83      4209\n",
      "\n",
      "    accuracy                           0.69      6043\n",
      "   macro avg       0.42      0.41      0.41      6043\n",
      "weighted avg       0.68      0.69      0.68      6043\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words'] + Binarized\n",
      "F1 Score para Neural Network con ['stop_words'] + Binarized: 0.451\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.34      0.40       104\n",
      "           2       0.29      0.19      0.23       145\n",
      "           3       0.39      0.36      0.38       422\n",
      "           4       0.41      0.39      0.40      1163\n",
      "           5       0.84      0.87      0.85      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.48      0.43      0.45      6043\n",
      "weighted avg       0.70      0.72      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words'] + Frequency\n",
      "F1 Score para Neural Network con ['stop_words'] + Frequency: 0.486\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.43      0.50       104\n",
      "           2       0.31      0.26      0.28       145\n",
      "           3       0.40      0.37      0.39       422\n",
      "           4       0.42      0.40      0.41      1163\n",
      "           5       0.84      0.87      0.86      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.51      0.47      0.49      6043\n",
      "weighted avg       0.71      0.72      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words'] + TF-IDF\n",
      "F1 Score para Neural Network con ['stop_words'] + TF-IDF: 0.468\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.34      0.43       104\n",
      "           2       0.33      0.20      0.25       145\n",
      "           3       0.42      0.37      0.40       422\n",
      "           4       0.40      0.42      0.41      1163\n",
      "           5       0.84      0.86      0.85      4209\n",
      "\n",
      "    accuracy                           0.71      6043\n",
      "   macro avg       0.52      0.44      0.47      6043\n",
      "weighted avg       0.71      0.71      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words'] + TF-IDF + SVD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dern9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score para Neural Network con ['stop_words'] + TF-IDF + SVD: 0.392\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.29      0.30      0.29       104\n",
      "           2       0.20      0.18      0.19       145\n",
      "           3       0.32      0.27      0.29       422\n",
      "           4       0.35      0.36      0.36      1163\n",
      "           5       0.82      0.83      0.82      4209\n",
      "\n",
      "    accuracy                           0.68      6043\n",
      "   macro avg       0.40      0.39      0.39      6043\n",
      "weighted avg       0.67      0.68      0.67      6043\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['lemmatization'] + Binarized\n",
      "F1 Score para Neural Network con ['lemmatization'] + Binarized: 0.461\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.52      0.37      0.43       104\n",
      "           2       0.28      0.18      0.22       145\n",
      "           3       0.42      0.40      0.41       422\n",
      "           4       0.41      0.39      0.40      1163\n",
      "           5       0.84      0.87      0.85      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.49      0.44      0.46      6043\n",
      "weighted avg       0.71      0.72      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['lemmatization'] + Frequency\n",
      "F1 Score para Neural Network con ['lemmatization'] + Frequency: 0.490\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.54      0.43      0.48       104\n",
      "           2       0.36      0.29      0.32       145\n",
      "           3       0.40      0.38      0.39       422\n",
      "           4       0.43      0.39      0.41      1163\n",
      "           5       0.84      0.87      0.86      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.51      0.47      0.49      6043\n",
      "weighted avg       0.71      0.72      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['lemmatization'] + TF-IDF\n",
      "F1 Score para Neural Network con ['lemmatization'] + TF-IDF: 0.465\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.57      0.38      0.45       104\n",
      "           2       0.34      0.19      0.24       145\n",
      "           3       0.40      0.37      0.39       422\n",
      "           4       0.40      0.39      0.39      1163\n",
      "           5       0.83      0.86      0.85      4209\n",
      "\n",
      "    accuracy                           0.71      6043\n",
      "   macro avg       0.51      0.44      0.46      6043\n",
      "weighted avg       0.70      0.71      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['lemmatization'] + TF-IDF + SVD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dern9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score para Neural Network con ['lemmatization'] + TF-IDF + SVD: 0.405\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.30      0.34      0.32       104\n",
      "           2       0.23      0.22      0.23       145\n",
      "           3       0.30      0.29      0.30       422\n",
      "           4       0.37      0.34      0.35      1163\n",
      "           5       0.82      0.84      0.83      4209\n",
      "\n",
      "    accuracy                           0.68      6043\n",
      "   macro avg       0.41      0.41      0.41      6043\n",
      "weighted avg       0.68      0.68      0.68      6043\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['text_cleaning'] + Binarized\n",
      "F1 Score para Neural Network con ['text_cleaning'] + Binarized: 0.456\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.38      0.44       104\n",
      "           2       0.29      0.18      0.22       145\n",
      "           3       0.40      0.36      0.38       422\n",
      "           4       0.41      0.38      0.39      1163\n",
      "           5       0.83      0.88      0.85      4209\n",
      "\n",
      "    accuracy                           0.72      6043\n",
      "   macro avg       0.49      0.43      0.46      6043\n",
      "weighted avg       0.70      0.72      0.71      6043\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['text_cleaning'] + Frequency\n",
      "F1 Score para Neural Network con ['text_cleaning'] + Frequency: 0.482\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.41      0.45       104\n",
      "           2       0.36      0.23      0.28       145\n",
      "           3       0.44      0.39      0.41       422\n",
      "           4       0.42      0.39      0.40      1163\n",
      "           5       0.84      0.88      0.86      4209\n",
      "\n",
      "    accuracy                           0.73      6043\n",
      "   macro avg       0.51      0.46      0.48      6043\n",
      "weighted avg       0.71      0.73      0.72      6043\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['text_cleaning'] + TF-IDF\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    results[clf_name] = {}\n",
    "    for com_rep_name, (X_train_rep, X_test_rep) in representaciones.items():\n",
    "        if clf_name == \"Naive Bayes\" and \"TF-IDF\" in com_rep_name:\n",
    "            print(f\"Skipping {clf_name} with {com_rep_name}\")\n",
    "            continue\n",
    "        print(f'\\nEvaluando {clf_name} con combincion:{com_rep_name}')\n",
    "        try:\n",
    "            f1, y_pred = TrainandEvaluate(clf,X_train_rep,X_test_rep,y_train,y_test)\n",
    "            results[clf_name][com_rep_name] = f1\n",
    "            print(f'F1 Score para {clf_name} con {com_rep_name}: {f1:.3f}')\n",
    "            print(f'\\nReporte de Clasificación para {clf_name} con {com_rep_name}:')\n",
    "            print(classification_report(y_test, y_pred, zero_division=0))\n",
    "        except Exception as e:\n",
    "            print(f'Error al evaluar {clf_name} con {com_rep_name}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKs4PULuuiW0"
   },
   "source": [
    "Los 3 mejores resultados por clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOemPBNjuiW0"
   },
   "outputs": [],
   "source": [
    "for clf_name, clf_scores in results.items():\n",
    "    print(f'\\nClasificador: {clf_name}')\n",
    "    top_combos = sorted(clf_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "    for rank, (combo_name, f1) in enumerate(top_combos, start=1):\n",
    "        print(f'{rank+1}. Combinación: {combo_name}, F1 Score: {f1:.3f}')\n",
    "\n",
    "        X_train_rep, X_test_rep = representaciones[combo_name]\n",
    "        clf = classifiers[clf_name]\n",
    "        clf.fit(X_train_rep, y_train)\n",
    "        y_pred = clf.predict(X_test_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNeU-qTBuiW0",
    "outputId": "bedd15ec-50c1-4882-f2a5-c207bca13c42"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Representation</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>['tokenization', 'lemmatization', 'text_cleani...</td>\n",
       "      <td>0.627513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>['tokenization', 'lemmatization', 'text_cleani...</td>\n",
       "      <td>0.610700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>['tokenization', 'lemmatization'] + Binarized</td>\n",
       "      <td>0.609505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>['tokenization', 'stop_words', 'lemmatization'...</td>\n",
       "      <td>0.620603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>['tokenization', 'stop_words', 'lemmatization'...</td>\n",
       "      <td>0.614150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>['tokenization', 'stop_words'] + Binarized</td>\n",
       "      <td>0.605219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>['stop_words', 'lemmatization'] + TF-IDF + SVD</td>\n",
       "      <td>0.510492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>['lemmatization'] + TF-IDF + SVD</td>\n",
       "      <td>0.504589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>['stop_words'] + TF-IDF + SVD</td>\n",
       "      <td>0.493866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SVM</td>\n",
       "      <td>['tokenization', 'stop_words'] + TF-IDF + SVD</td>\n",
       "      <td>0.534118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SVM</td>\n",
       "      <td>['tokenization', 'stop_words', 'text_cleaning'...</td>\n",
       "      <td>0.534118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SVM</td>\n",
       "      <td>['tokenization', 'stop_words', 'lemmatization'...</td>\n",
       "      <td>0.526396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>['stop_words', 'text_cleaning'] + TF-IDF + SVD</td>\n",
       "      <td>0.638667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>['stop_words'] + TF-IDF + SVD</td>\n",
       "      <td>0.635913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>['tokenization', 'text_cleaning'] + TF-IDF + SVD</td>\n",
       "      <td>0.635913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>['tokenization', 'stop_words', 'lemmatization'...</td>\n",
       "      <td>0.562764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>['tokenization', 'lemmatization', 'text_cleani...</td>\n",
       "      <td>0.557068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>['tokenization'] + Frequency</td>\n",
       "      <td>0.554016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Classifier                                     Representation  \\\n",
       "0   Logistic Regression  ['tokenization', 'lemmatization', 'text_cleani...   \n",
       "1   Logistic Regression  ['tokenization', 'lemmatization', 'text_cleani...   \n",
       "2   Logistic Regression      ['tokenization', 'lemmatization'] + Binarized   \n",
       "3           Naive Bayes  ['tokenization', 'stop_words', 'lemmatization'...   \n",
       "4           Naive Bayes  ['tokenization', 'stop_words', 'lemmatization'...   \n",
       "5           Naive Bayes         ['tokenization', 'stop_words'] + Binarized   \n",
       "6         Decision Tree     ['stop_words', 'lemmatization'] + TF-IDF + SVD   \n",
       "7         Decision Tree                   ['lemmatization'] + TF-IDF + SVD   \n",
       "8         Decision Tree                      ['stop_words'] + TF-IDF + SVD   \n",
       "9                   SVM      ['tokenization', 'stop_words'] + TF-IDF + SVD   \n",
       "10                  SVM  ['tokenization', 'stop_words', 'text_cleaning'...   \n",
       "11                  SVM  ['tokenization', 'stop_words', 'lemmatization'...   \n",
       "12       Neural Network     ['stop_words', 'text_cleaning'] + TF-IDF + SVD   \n",
       "13       Neural Network                      ['stop_words'] + TF-IDF + SVD   \n",
       "14       Neural Network   ['tokenization', 'text_cleaning'] + TF-IDF + SVD   \n",
       "15    Gradient Boosting  ['tokenization', 'stop_words', 'lemmatization'...   \n",
       "16    Gradient Boosting  ['tokenization', 'lemmatization', 'text_cleani...   \n",
       "17    Gradient Boosting                       ['tokenization'] + Frequency   \n",
       "\n",
       "    F1 Score  \n",
       "0   0.627513  \n",
       "1   0.610700  \n",
       "2   0.609505  \n",
       "3   0.620603  \n",
       "4   0.614150  \n",
       "5   0.605219  \n",
       "6   0.510492  \n",
       "7   0.504589  \n",
       "8   0.493866  \n",
       "9   0.534118  \n",
       "10  0.534118  \n",
       "11  0.526396  \n",
       "12  0.638667  \n",
       "13  0.635913  \n",
       "14  0.635913  \n",
       "15  0.562764  \n",
       "16  0.557068  \n",
       "17  0.554016  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for clf_name, clf_scores in results.items():\n",
    "    top_combos = sorted(clf_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    for combo_name, f1 in top_combos:\n",
    "        data.append({'Classifier': clf_name, 'Representation': combo_name, 'F1 Score': f1})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "#df_multi = df.set_index(['Classifier', 'Representation', 'F1 Score'])\n",
    "#df_multi\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0kT3WvfZuiW0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
