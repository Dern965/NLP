{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice V Text classification\n",
    "***\n",
    "Rodriguez Nuñez Diego Eduardo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score , ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "      <th>Section</th>\n",
       "      <th>URL</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>La Jornada: Deportes</td>\n",
       "      <td>Récord de 96 mil asistentes para pelea Dubois ...</td>\n",
       "      <td>Londres. Daniel Dubois, nueva estrella del box...</td>\n",
       "      <td>Deportes</td>\n",
       "      <td>https://www.jornada.com.mx/2024/09/21/deportes...</td>\n",
       "      <td>21/09/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>La Jornada: Deportes</td>\n",
       "      <td>América  no está  para formar jugadores, dice ...</td>\n",
       "      <td>Siendo el América uno de los clubes que menos ...</td>\n",
       "      <td>Deportes</td>\n",
       "      <td>https://www.jornada.com.mx/2024/09/21/deportes...</td>\n",
       "      <td>21/09/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>La Jornada: Deportes</td>\n",
       "      <td>Fátima Herrera, sin miedo a nada,  hizo histor...</td>\n",
       "      <td>El surgimiento de referentes en el deporte de ...</td>\n",
       "      <td>Deportes</td>\n",
       "      <td>https://www.jornada.com.mx/2024/09/21/deportes...</td>\n",
       "      <td>21/09/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>La Jornada: Deportes</td>\n",
       "      <td>Pumas femenil deja escapar los tres puntos en CU</td>\n",
       "      <td>Pese a generar diversas oportunidades de gol, ...</td>\n",
       "      <td>Deportes</td>\n",
       "      <td>https://www.jornada.com.mx/2024/09/21/deportes...</td>\n",
       "      <td>21/09/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La Jornada: Deportes</td>\n",
       "      <td>Cae dupla Zverev-Alcaraz</td>\n",
       "      <td>La dupla Carlos Alcaraz-Alexander Zverev cayó ...</td>\n",
       "      <td>Deportes</td>\n",
       "      <td>https://www.jornada.com.mx/2024/09/21/deportes...</td>\n",
       "      <td>21/09/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>Expansión - Tecnología</td>\n",
       "      <td>YouTube se corona como el rey del streaming en...</td>\n",
       "      <td>La plataforma se impuso como la más consumida ...</td>\n",
       "      <td>Tecnología</td>\n",
       "      <td>https://expansion.mx/tecnologia/2024/09/30/you...</td>\n",
       "      <td>30/09/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>Expansión - Tecnología</td>\n",
       "      <td>Qué es y cómo funciona la tarifa dinámica de T...</td>\n",
       "      <td>Los hermanos Gallagher anunciaron que para la ...</td>\n",
       "      <td>Tecnología</td>\n",
       "      <td>https://expansion.mx/tecnologia/2024/09/30/que...</td>\n",
       "      <td>30/09/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>Expansión - Tecnología</td>\n",
       "      <td>Trump advierte que irá contra Google por solo ...</td>\n",
       "      <td>El candidato apuntó que en caso de llegar a la...</td>\n",
       "      <td>Tecnología</td>\n",
       "      <td>https://expansion.mx/tecnologia/2024/09/30/tru...</td>\n",
       "      <td>30/09/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>Expansión - Tecnología</td>\n",
       "      <td>AT&amp;T venderá participación en DirecTV por 7,60...</td>\n",
       "      <td>Esta operación permite a AT&amp;amp;T continuar de...</td>\n",
       "      <td>Tecnología</td>\n",
       "      <td>https://expansion.mx/tecnologia/2024/09/30/att...</td>\n",
       "      <td>30/09/2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>Expansión - Tecnología</td>\n",
       "      <td>La IA es una oportunidad para ser más humanos</td>\n",
       "      <td>Pilar Manchón, directora de estrategia de inve...</td>\n",
       "      <td>Tecnología</td>\n",
       "      <td>https://expansion.mx/tecnologia/2024/09/30/la-...</td>\n",
       "      <td>30/09/2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Source  \\\n",
       "0      La Jornada: Deportes   \n",
       "1      La Jornada: Deportes   \n",
       "2      La Jornada: Deportes   \n",
       "3      La Jornada: Deportes   \n",
       "4      La Jornada: Deportes   \n",
       "..                      ...   \n",
       "393  Expansión - Tecnología   \n",
       "394  Expansión - Tecnología   \n",
       "395  Expansión - Tecnología   \n",
       "396  Expansión - Tecnología   \n",
       "397  Expansión - Tecnología   \n",
       "\n",
       "                                                 Title  \\\n",
       "0    Récord de 96 mil asistentes para pelea Dubois ...   \n",
       "1    América  no está  para formar jugadores, dice ...   \n",
       "2    Fátima Herrera, sin miedo a nada,  hizo histor...   \n",
       "3     Pumas femenil deja escapar los tres puntos en CU   \n",
       "4                             Cae dupla Zverev-Alcaraz   \n",
       "..                                                 ...   \n",
       "393  YouTube se corona como el rey del streaming en...   \n",
       "394  Qué es y cómo funciona la tarifa dinámica de T...   \n",
       "395  Trump advierte que irá contra Google por solo ...   \n",
       "396  AT&T venderá participación en DirecTV por 7,60...   \n",
       "397      La IA es una oportunidad para ser más humanos   \n",
       "\n",
       "                                               Content     Section  \\\n",
       "0    Londres. Daniel Dubois, nueva estrella del box...    Deportes   \n",
       "1    Siendo el América uno de los clubes que menos ...    Deportes   \n",
       "2    El surgimiento de referentes en el deporte de ...    Deportes   \n",
       "3    Pese a generar diversas oportunidades de gol, ...    Deportes   \n",
       "4    La dupla Carlos Alcaraz-Alexander Zverev cayó ...    Deportes   \n",
       "..                                                 ...         ...   \n",
       "393  La plataforma se impuso como la más consumida ...  Tecnología   \n",
       "394  Los hermanos Gallagher anunciaron que para la ...  Tecnología   \n",
       "395  El candidato apuntó que en caso de llegar a la...  Tecnología   \n",
       "396  Esta operación permite a AT&amp;T continuar de...  Tecnología   \n",
       "397  Pilar Manchón, directora de estrategia de inve...  Tecnología   \n",
       "\n",
       "                                                   URL        Date  \n",
       "0    https://www.jornada.com.mx/2024/09/21/deportes...  21/09/2024  \n",
       "1    https://www.jornada.com.mx/2024/09/21/deportes...  21/09/2024  \n",
       "2    https://www.jornada.com.mx/2024/09/21/deportes...  21/09/2024  \n",
       "3    https://www.jornada.com.mx/2024/09/21/deportes...  21/09/2024  \n",
       "4    https://www.jornada.com.mx/2024/09/21/deportes...  21/09/2024  \n",
       "..                                                 ...         ...  \n",
       "393  https://expansion.mx/tecnologia/2024/09/30/you...  30/09/2024  \n",
       "394  https://expansion.mx/tecnologia/2024/09/30/que...  30/09/2024  \n",
       "395  https://expansion.mx/tecnologia/2024/09/30/tru...  30/09/2024  \n",
       "396  https://expansion.mx/tecnologia/2024/09/30/att...  30/09/2024  \n",
       "397  https://expansion.mx/tecnologia/2024/09/30/la-...  30/09/2024  \n",
       "\n",
       "[398 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('raw_data_corpus.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Features'] = data['Title'] + ' ' + data['Content']\n",
    "X = data['Features'].fillna('')\n",
    "y = data['Section']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.str.replace(r'\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Division del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizarTexto(texto, aplicar):\n",
    "    texto = texto.lower() if 'text_cleaning' in aplicar else texto\n",
    "\n",
    "    doc = nlp(texto)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if not token.is_punct and not token.is_space:\n",
    "            if 'stop_words' in aplicar and token.is_stop:\n",
    "                continue\n",
    "            tokens.append(token.lemma_ if 'lemmatization' in aplicar else token.text)\n",
    "    return ' '.join(tokens) if 'tokenization' in aplicar else ' '.join([texto])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando combinacion: ['tokenization']\n",
      "Procesando combinacion: ['stop_words']\n",
      "Procesando combinacion: ['lemmatization']\n",
      "Procesando combinacion: ['text_cleaning']\n",
      "Procesando combinacion: ['tokenization', 'stop_words']\n",
      "Procesando combinacion: ['tokenization', 'lemmatization']\n",
      "Procesando combinacion: ['tokenization', 'text_cleaning']\n",
      "Procesando combinacion: ['stop_words', 'lemmatization']\n",
      "Procesando combinacion: ['stop_words', 'text_cleaning']\n",
      "Procesando combinacion: ['lemmatization', 'text_cleaning']\n",
      "Procesando combinacion: ['tokenization', 'stop_words', 'lemmatization']\n",
      "Procesando combinacion: ['tokenization', 'stop_words', 'text_cleaning']\n",
      "Procesando combinacion: ['tokenization', 'lemmatization', 'text_cleaning']\n",
      "Procesando combinacion: ['stop_words', 'lemmatization', 'text_cleaning']\n",
      "Procesando combinacion: ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning']\n",
      "Combinación: ['tokenization']\n",
      "Train: 3      Pumas femenil deja escapar los tres puntos en ...\n",
      "18     Lun representa en escena la magia de la infanc...\n",
      "376    Llega a su novena edición Cuerpo al Descubiert...\n",
      "248                                                     \n",
      "177    Una jubilación digna se complica cada vez más ...\n",
      "Name: Features, dtype: object\n",
      "Test: 198    Miguel Pineda Negocios y empresas A diferencia...\n",
      "396    AT&T venderá participación en DirecTV por 7,60...\n",
      "33     La CNBV buscará regular reportes de cibersegur...\n",
      "208    Javier Aranda Luna Kundera la novela como desa...\n",
      "93     Malware en Android permite retirar dinero desd...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['stop_words']\n",
      "Train: 3      Pumas femenil deja escapar los tres puntos en ...\n",
      "18     Lun representa en escena la magia de la infanc...\n",
      "376    Llega a su novena edición Cuerpo al Descubiert...\n",
      "248                                                     \n",
      "177    Una jubilación digna se complica cada vez más ...\n",
      "Name: Features, dtype: object\n",
      "Test: 198    Miguel Pineda: Negocios  y empresas A diferenc...\n",
      "396    AT&T venderá participación en DirecTV por 7,60...\n",
      "33     La CNBV buscará regular reportes de cibersegur...\n",
      "208    Javier Aranda Luna: Kundera: la novela como de...\n",
      "93     Malware en Android permite retirar dinero desd...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['lemmatization']\n",
      "Train: 3      Pumas femenil deja escapar los tres puntos en ...\n",
      "18     Lun representa en escena la magia de la infanc...\n",
      "376    Llega a su novena edición Cuerpo al Descubiert...\n",
      "248                                                     \n",
      "177    Una jubilación digna se complica cada vez más ...\n",
      "Name: Features, dtype: object\n",
      "Test: 198    Miguel Pineda: Negocios  y empresas A diferenc...\n",
      "396    AT&T venderá participación en DirecTV por 7,60...\n",
      "33     La CNBV buscará regular reportes de cibersegur...\n",
      "208    Javier Aranda Luna: Kundera: la novela como de...\n",
      "93     Malware en Android permite retirar dinero desd...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['text_cleaning']\n",
      "Train: 3      pumas femenil deja escapar los tres puntos en ...\n",
      "18     lun representa en escena la magia de la infanc...\n",
      "376    llega a su novena edición cuerpo al descubiert...\n",
      "248                                                     \n",
      "177    una jubilación digna se complica cada vez más ...\n",
      "Name: Features, dtype: object\n",
      "Test: 198    miguel pineda: negocios  y empresas a diferenc...\n",
      "396    at&t venderá participación en directv por 7,60...\n",
      "33     la cnbv buscará regular reportes de cibersegur...\n",
      "208    javier aranda luna: kundera: la novela como de...\n",
      "93     malware en android permite retirar dinero desd...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['tokenization', 'stop_words']\n",
      "Train: 3      Pumas femenil deja escapar puntos CU Pese gene...\n",
      "18     Lun representa escena magia infancia enigma ex...\n",
      "376    Llega novena edición Cuerpo Descubierto festiv...\n",
      "248                                                     \n",
      "177    jubilación digna complica mexicanos quinta emp...\n",
      "Name: Features, dtype: object\n",
      "Test: 198    Miguel Pineda Negocios empresas diferencia reg...\n",
      "396    AT&T venderá participación DirecTV 7,600 millo...\n",
      "33     CNBV buscará regular reportes ciberseguridad r...\n",
      "208    Javier Aranda Luna Kundera novela desafío rebe...\n",
      "93     Malware Android permite retirar dinero cajeros...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['tokenization', 'lemmatization']\n",
      "Train: 3      Pumas femenil dejar escapar el tres punto en c...\n",
      "18     Lun representar en escena el magia de el infan...\n",
      "376    llegar a su noveno edición Cuerpo al Descubier...\n",
      "248                                                     \n",
      "177    uno jubilación digno él complicar cada vez más...\n",
      "Name: Features, dtype: object\n",
      "Test: 198    Miguel Pineda Negocios y empresa A diferencia ...\n",
      "396    AT&T vender participación en DirecTV por 7.600...\n",
      "33     el CNBV buscar regular report de cibersegurida...\n",
      "208    Javier Aranda Luna Kundera el novela como desa...\n",
      "93     Malware en Android permitir retirar dinero des...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['tokenization', 'text_cleaning']\n",
      "Train: 3      pumas femenil deja escapar los tres puntos en ...\n",
      "18     lun representa en escena la magia de la infanc...\n",
      "376    llega a su novena edición cuerpo al descubiert...\n",
      "248                                                     \n",
      "177    una jubilación digna se complica cada vez más ...\n",
      "Name: Features, dtype: object\n",
      "Test: 198    miguel pineda negocios y empresas a diferencia...\n",
      "396    at&t venderá participación en directv por 7,60...\n",
      "33     la cnbv buscará regular reportes de cibersegur...\n",
      "208    javier aranda luna kundera la novela como desa...\n",
      "93     malware en android permite retirar dinero desd...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['stop_words', 'lemmatization']\n",
      "Train: 3      Pumas femenil deja escapar los tres puntos en ...\n",
      "18     Lun representa en escena la magia de la infanc...\n",
      "376    Llega a su novena edición Cuerpo al Descubiert...\n",
      "248                                                     \n",
      "177    Una jubilación digna se complica cada vez más ...\n",
      "Name: Features, dtype: object\n",
      "Test: 198    Miguel Pineda: Negocios  y empresas A diferenc...\n",
      "396    AT&T venderá participación en DirecTV por 7,60...\n",
      "33     La CNBV buscará regular reportes de cibersegur...\n",
      "208    Javier Aranda Luna: Kundera: la novela como de...\n",
      "93     Malware en Android permite retirar dinero desd...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['stop_words', 'text_cleaning']\n",
      "Train: 3      pumas femenil deja escapar los tres puntos en ...\n",
      "18     lun representa en escena la magia de la infanc...\n",
      "376    llega a su novena edición cuerpo al descubiert...\n",
      "248                                                     \n",
      "177    una jubilación digna se complica cada vez más ...\n",
      "Name: Features, dtype: object\n",
      "Test: 198    miguel pineda: negocios  y empresas a diferenc...\n",
      "396    at&t venderá participación en directv por 7,60...\n",
      "33     la cnbv buscará regular reportes de cibersegur...\n",
      "208    javier aranda luna: kundera: la novela como de...\n",
      "93     malware en android permite retirar dinero desd...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['lemmatization', 'text_cleaning']\n",
      "Train: 3      pumas femenil deja escapar los tres puntos en ...\n",
      "18     lun representa en escena la magia de la infanc...\n",
      "376    llega a su novena edición cuerpo al descubiert...\n",
      "248                                                     \n",
      "177    una jubilación digna se complica cada vez más ...\n",
      "Name: Features, dtype: object\n",
      "Test: 198    miguel pineda: negocios  y empresas a diferenc...\n",
      "396    at&t venderá participación en directv por 7,60...\n",
      "33     la cnbv buscará regular reportes de cibersegur...\n",
      "208    javier aranda luna: kundera: la novela como de...\n",
      "93     malware en android permite retirar dinero desd...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['tokenization', 'stop_words', 'lemmatization']\n",
      "Train: 3      Pumas femenil dejar escapar punto cu pese gene...\n",
      "18     Lun representar escena magia infancia enigma e...\n",
      "376    llegar noveno edición Cuerpo Descubierto festi...\n",
      "248                                                     \n",
      "177    jubilación digno complicar mexicano quinto emp...\n",
      "Name: Features, dtype: object\n",
      "Test: 198    Miguel Pineda Negocios empresa diferencia regu...\n",
      "396    AT&T vender participación DirecTV 7.600 millón...\n",
      "33     CNBV buscar regular report ciberseguridad regu...\n",
      "208    Javier Aranda Luna Kundera novela desafío rebe...\n",
      "93     Malware Android permitir retirar dinero cajero...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['tokenization', 'stop_words', 'text_cleaning']\n",
      "Train: 3      pumas femenil deja escapar puntos cu pese gene...\n",
      "18     lun representa escena magia infancia enigma ex...\n",
      "376    llega novena edición cuerpo descubierto festiv...\n",
      "248                                                     \n",
      "177    jubilación digna complica mexicanos quinta emp...\n",
      "Name: Features, dtype: object\n",
      "Test: 198    miguel pineda negocios empresas diferencia reg...\n",
      "396    at&t venderá participación directv 7,600 millo...\n",
      "33     cnbv buscará regular reportes ciberseguridad r...\n",
      "208    javier aranda luna kundera novela desafío rebe...\n",
      "93     malware android permite retirar dinero cajeros...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['tokenization', 'lemmatization', 'text_cleaning']\n",
      "Train: 3      puma femenil dejar escapar el tres punto en cu...\n",
      "18     lun representar en escena el magia de el infan...\n",
      "376    llegar a su noveno edición cuerpo al descubier...\n",
      "248                                                     \n",
      "177    uno jubilación digno él complicar cada vez más...\n",
      "Name: Features, dtype: object\n",
      "Test: 198    miguel pineda negocio y empresa a diferencia d...\n",
      "396    at&t vender participación en directv por 7.600...\n",
      "33     el cnbv buscar regular report de cibersegurida...\n",
      "208    javier aranda luna kundera el novela como desa...\n",
      "93     malwarar en android permitir retirar dinero de...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['stop_words', 'lemmatization', 'text_cleaning']\n",
      "Train: 3      pumas femenil deja escapar los tres puntos en ...\n",
      "18     lun representa en escena la magia de la infanc...\n",
      "376    llega a su novena edición cuerpo al descubiert...\n",
      "248                                                     \n",
      "177    una jubilación digna se complica cada vez más ...\n",
      "Name: Features, dtype: object\n",
      "Test: 198    miguel pineda: negocios  y empresas a diferenc...\n",
      "396    at&t venderá participación en directv por 7,60...\n",
      "33     la cnbv buscará regular reportes de cibersegur...\n",
      "208    javier aranda luna: kundera: la novela como de...\n",
      "93     malware en android permite retirar dinero desd...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n",
      "Combinación: ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning']\n",
      "Train: 3      puma femenil dejar escapar punto cu pese gener...\n",
      "18     lun representar escena magia infancia enigma e...\n",
      "376    llegar noveno edición cuerpo descubierto festi...\n",
      "248                                                     \n",
      "177    jubilación digno complicar mexicano quinto emp...\n",
      "Name: Features, dtype: object\n",
      "Test: 198    miguel pineda negocio empresa diferencia regul...\n",
      "396    at&t vender participación directv 7.600 millón...\n",
      "33     cnbv buscar regular report ciberseguridad regu...\n",
      "208    javier aranda luna kundera novela desafío rebe...\n",
      "93     malwarar android permitir retirar dinero cajer...\n",
      "Name: Features, dtype: object\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "procesos = ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning']\n",
    "combos = [list(c) for i in range(1,len(procesos)+1) for c in combinations(procesos, i)]\n",
    "\n",
    "resultados = {}\n",
    "\n",
    "for combo in combos:\n",
    "    print(f'Procesando combinacion: {combo}')\n",
    "    X_train_norm = X_train.apply(lambda x: NormalizarTexto(x, aplicar=combo))\n",
    "    X_test_norm = X_test.apply(lambda x: NormalizarTexto(x, aplicar=combo))\n",
    "    resultados[str(combo)] = (X_train_norm[:5], X_test_norm[:5])\n",
    "\n",
    "for combo, (tran_sample, test_sample) in resultados.items():\n",
    "    print(f'Combinación: {combo}')\n",
    "    print(f'Train: {tran_sample}')\n",
    "    print(f'Test: {test_sample}')\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generando representación para combinación: ['tokenization']\n",
      "Combinacion: ['tokenization'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['tokenization'] + TF-IDF + SVD\n",
      "Preview: [[ 0.21326042  0.20259188 -0.21368175  0.07867552  0.02859727 -0.00275321\n",
      "  -0.1248336   0.05596382  0.04293896  0.13831039  0.12841583  0.12484692\n",
      "   0.0677793  -0.08529728 -0.03492736 -0.00711285 -0.03153037  0.15474409\n",
      "  -0.03299685 -0.06695752  0.05775113 -0.06450361 -0.02013199  0.06130594\n",
      "  -0.04410444 -0.06527795  0.05558361 -0.12583295  0.02887727  0.00171073\n",
      "  -0.02911358 -0.1331649   0.0520549  -0.01818061  0.06857034  0.05112099\n",
      "  -0.06539313 -0.06414931  0.09176535 -0.03745409  0.05818775  0.03900024\n",
      "  -0.02442581  0.00473006 -0.0166907  -0.0737962   0.0273052  -0.02274014\n",
      "   0.0340057   0.08504579  0.03066244  0.03957891  0.03453881  0.00478839\n",
      "  -0.05317812 -0.10252086  0.04325234  0.02453766  0.06456323 -0.03803964\n",
      "   0.10329939  0.02548951  0.01777171 -0.02388314  0.00246067  0.0688039\n",
      "  -0.04600201  0.00936384  0.03926655  0.02688586  0.05831973  0.12498476\n",
      "   0.04365831 -0.00493079 -0.12224943 -0.08818584  0.04128235  0.09130447\n",
      "  -0.01767122 -0.06476928 -0.03360024 -0.04273862  0.03532741  0.00451204\n",
      "   0.00886915  0.04194355 -0.07414165  0.05367172 -0.04814807  0.00107494\n",
      "  -0.06158783 -0.01862862 -0.13223     0.01108101 -0.00864314  0.0079323\n",
      "   0.01496949 -0.02442077 -0.00367864  0.02583097]\n",
      " [ 0.34621338 -0.02939755  0.04868263 -0.02816581 -0.10730069 -0.05142889\n",
      "   0.14897771  0.04162949  0.05365962  0.1070692   0.02665217  0.14181699\n",
      "  -0.02918941  0.15859631  0.00698241 -0.02691928  0.00416156  0.05367499\n",
      "   0.02314217  0.05383213 -0.05132356 -0.05224107 -0.0643321   0.04652485\n",
      "   0.08044191  0.07433883 -0.0305238   0.0391165  -0.00327194 -0.04041965\n",
      "  -0.07310722  0.00078341 -0.0520132  -0.00043555  0.05651734  0.06795205\n",
      "   0.04140125 -0.04565494  0.03847458  0.03012728 -0.03817688 -0.01114713\n",
      "  -0.01042799  0.01234571  0.04554363  0.02752829  0.03706959 -0.02938709\n",
      "  -0.05388914  0.01652773 -0.01050478  0.00719487  0.0333529  -0.00380466\n",
      "  -0.07220509  0.04978221 -0.05112121 -0.01754703 -0.03800682 -0.00574973\n",
      "  -0.09453083 -0.03465525 -0.01456365  0.01243518  0.04321776  0.06166185\n",
      "  -0.01997746 -0.09103676  0.03606536 -0.00566396 -0.01021713  0.06499284\n",
      "  -0.04167228 -0.00509861 -0.03715253  0.04213149 -0.04086709  0.0497998\n",
      "   0.01545585 -0.01792252 -0.01855696  0.03452577 -0.01231621 -0.04229439\n",
      "  -0.02816107 -0.04014865  0.03118942  0.01855923  0.10994298 -0.03785249\n",
      "   0.06053876  0.01988478  0.04933711 -0.02411437 -0.02396052  0.01684509\n",
      "   0.04827461  0.02552248 -0.05746659 -0.01175333]\n",
      " [ 0.22122681  0.01080587 -0.01736925 -0.00239265 -0.06657192  0.02062681\n",
      "   0.05069896  0.07749841  0.00242422  0.00685573 -0.08782363  0.04206634\n",
      "   0.02964068 -0.03710459 -0.02247066  0.09192662  0.00151998 -0.08934295\n",
      "  -0.02009968  0.15131204 -0.05043996 -0.04480287 -0.07168241  0.04037046\n",
      "   0.06409946 -0.13613094 -0.03395718  0.01646445  0.01026309  0.06377382\n",
      "  -0.00937808  0.03633341  0.04421026 -0.05021075 -0.05106104  0.07427502\n",
      "   0.07618595 -0.04372207  0.10979882  0.01844173  0.00069933  0.03140461\n",
      "  -0.03877802  0.02325297  0.09738768 -0.09859693  0.02021401  0.01702582\n",
      "   0.0411398   0.02585184  0.10543447  0.05339288 -0.00543942 -0.0096332\n",
      "   0.10222382  0.11816187 -0.01454704 -0.03900811 -0.07616074 -0.07230291\n",
      "   0.02581291 -0.13676202 -0.00708346  0.00709055  0.0725357   0.01079988\n",
      "  -0.06675631 -0.0461615  -0.09153036  0.0192701  -0.02080745  0.08619303\n",
      "  -0.0213988  -0.02098028 -0.01981382 -0.07315379  0.01778055 -0.02729318\n",
      "  -0.0141503   0.08676872  0.01937674 -0.02242573  0.04429949 -0.05725598\n",
      "   0.02638851  0.0877427  -0.03211635 -0.14165261 -0.031774   -0.05008276\n",
      "   0.03192587 -0.04437524  0.08036954 -0.02692757 -0.06643393  0.00092583\n",
      "  -0.0030864  -0.06637082 -0.01193112 -0.03815641]]\n",
      "\n",
      "Generando representación para combinación: ['stop_words']\n",
      "Combinacion: ['stop_words'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['stop_words'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['stop_words'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['stop_words'] + TF-IDF + SVD\n",
      "Preview: [[ 0.21326235  0.20261865 -0.21368052  0.07862102  0.02849024 -0.0027561\n",
      "  -0.12475158  0.05616941  0.04286994  0.13819202  0.1282607   0.12526847\n",
      "   0.06763953 -0.08520245 -0.03498505 -0.00717128 -0.03151448  0.15472615\n",
      "  -0.03306683 -0.06670136  0.05826755 -0.06428152 -0.02013905  0.06131957\n",
      "  -0.04414126 -0.06528492  0.05562204 -0.12638188  0.02682464  0.00214137\n",
      "  -0.02860331 -0.13305659  0.05234998 -0.01864221  0.06859182  0.05064143\n",
      "  -0.06563622 -0.06429671  0.09166027 -0.03709604  0.05860521  0.03872112\n",
      "  -0.02426133  0.00480755 -0.01663462  0.07357481  0.02758906 -0.02117219\n",
      "   0.03557978  0.08404286  0.03265008  0.04148683  0.0337202   0.00335258\n",
      "  -0.05182902 -0.10265444  0.04069684  0.02738081  0.06582708 -0.03946797\n",
      "   0.10276727  0.02476295  0.01781439 -0.02378977  0.0026437   0.06909638\n",
      "  -0.04611049  0.00944674  0.0383978   0.03025846  0.05681691  0.12489988\n",
      "   0.04451315 -0.00561619 -0.12169536 -0.08819937  0.0391935   0.09211017\n",
      "  -0.0176859  -0.06548463 -0.03312144 -0.0427343   0.03524257  0.00490245\n",
      "   0.00936553  0.04250972 -0.07291881  0.05617808 -0.04762524  0.00450871\n",
      "  -0.06080556 -0.01575135 -0.13275514  0.01088312  0.00709654  0.00736595\n",
      "   0.01498304 -0.02475136 -0.00254001  0.02692702]\n",
      " [ 0.34620765 -0.02939947  0.0486991  -0.02823063 -0.10726361 -0.05145641\n",
      "   0.14891217  0.04116453  0.05403097  0.10705797  0.02665056  0.1416396\n",
      "  -0.02993843  0.15862342  0.00705638 -0.02686639  0.00418529  0.05368456\n",
      "   0.0232263   0.05338121 -0.05168162 -0.05255386 -0.06440988  0.04640635\n",
      "   0.07965456  0.0745653  -0.03045817  0.03901522 -0.00522208 -0.03943786\n",
      "  -0.07464357  0.00052379 -0.05137775 -0.00061662  0.05656252  0.06800433\n",
      "   0.04109303 -0.04586214  0.03832147  0.03039682 -0.03835397 -0.01085573\n",
      "  -0.01086836  0.01215771  0.04525169 -0.02688099  0.03665852 -0.03176941\n",
      "  -0.05309443  0.01628397 -0.00969464  0.00931415  0.03381175 -0.0039615\n",
      "  -0.07279305  0.04909938 -0.04870825 -0.02035522 -0.03975826 -0.0046165\n",
      "  -0.09452628 -0.03436861 -0.01483785  0.0124673   0.04346539  0.0627957\n",
      "  -0.01892511 -0.09065516  0.03564098 -0.00471679 -0.01055832  0.06489652\n",
      "  -0.04137281 -0.00534987 -0.03668121  0.04227054 -0.0411679   0.04987877\n",
      "   0.01563672 -0.01818257 -0.0182099   0.03443216 -0.01246024 -0.04236024\n",
      "  -0.0284305  -0.04058596  0.03102707  0.01550487  0.10953944 -0.04315613\n",
      "   0.059108    0.01765217  0.04875019 -0.02304823  0.02438069  0.01622641\n",
      "   0.04801353  0.02355955 -0.05851124 -0.01156922]\n",
      " [ 0.22122047  0.01081009 -0.01733706 -0.00246773 -0.06654218  0.0205557\n",
      "   0.05082345  0.07746329  0.00254584  0.00683978 -0.08785785  0.04211503\n",
      "   0.02941155 -0.03700957 -0.02253655  0.09197385  0.00145455 -0.08934682\n",
      "  -0.01990482  0.1507421  -0.05146896 -0.04536881 -0.07174796  0.04044132\n",
      "   0.06460146 -0.13612182 -0.03377108  0.01616387  0.01131259  0.06406631\n",
      "  -0.00772786  0.03655459  0.04350979 -0.05020704 -0.05095412  0.07473247\n",
      "   0.07554568 -0.04376315  0.10961037  0.01899534  0.00086957  0.03139186\n",
      "  -0.03876705  0.02322245  0.09733443  0.09848437  0.02050478  0.01876562\n",
      "   0.04056484  0.02299239  0.10639096  0.05224397 -0.00844136 -0.01009173\n",
      "   0.10189643  0.11851419 -0.0117931  -0.04088373 -0.07632745 -0.07083904\n",
      "   0.02548984 -0.136813   -0.00743807  0.00703141  0.07266774  0.00972214\n",
      "  -0.06601731 -0.04783389 -0.09153764  0.01551669 -0.02029947  0.08694364\n",
      "  -0.02089546 -0.02102123 -0.01965621 -0.07314735  0.01748878 -0.02742541\n",
      "  -0.01409314  0.08673558  0.01747065 -0.02178221  0.04579135 -0.05764752\n",
      "   0.02722747  0.08756388 -0.033237   -0.13783892 -0.03892394 -0.05188623\n",
      "   0.03032474 -0.04666438  0.08075232 -0.02495909  0.06831993  0.00120694\n",
      "  -0.00251895 -0.06683    -0.01103844 -0.03728259]]\n",
      "\n",
      "Generando representación para combinación: ['lemmatization']\n",
      "Combinacion: ['lemmatization'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['lemmatization'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['lemmatization'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['lemmatization'] + TF-IDF + SVD\n",
      "Preview: [[ 0.21326235  0.20261865 -0.21368052  0.07862102  0.02849024 -0.0027561\n",
      "  -0.12475158  0.05616941  0.04286994  0.13819202  0.1282607   0.12526847\n",
      "   0.06763953 -0.08520245 -0.03498505 -0.00717128 -0.03151448  0.15472615\n",
      "  -0.03306683 -0.06670136  0.05826755 -0.06428152 -0.02013905  0.06131957\n",
      "  -0.04414126 -0.06528492  0.05562204 -0.12638188  0.02682464  0.00214137\n",
      "  -0.02860331 -0.13305659  0.05234998 -0.01864221  0.06859182  0.05064143\n",
      "  -0.06563622 -0.06429671  0.09166027 -0.03709604  0.05860521  0.03872112\n",
      "  -0.02426133  0.00480755 -0.01663462  0.07357481  0.02758906 -0.02117219\n",
      "   0.03557978  0.08404286  0.03265008  0.04148683  0.0337202   0.00335258\n",
      "  -0.05182902 -0.10265444  0.04069684  0.02738081  0.06582708 -0.03946797\n",
      "   0.10276727  0.02476295  0.01781439 -0.02378977  0.0026437   0.06909638\n",
      "  -0.04611049  0.00944674  0.0383978   0.03025846  0.05681691  0.12489988\n",
      "   0.04451315 -0.00561619 -0.12169536 -0.08819937  0.0391935   0.09211017\n",
      "  -0.0176859  -0.06548463 -0.03312144 -0.0427343   0.03524257  0.00490245\n",
      "   0.00936553  0.04250972 -0.07291881  0.05617808 -0.04762524  0.00450871\n",
      "  -0.06080556 -0.01575135 -0.13275514  0.01088312  0.00709654  0.00736595\n",
      "   0.01498304 -0.02475136 -0.00254001  0.02692702]\n",
      " [ 0.34620765 -0.02939947  0.0486991  -0.02823063 -0.10726361 -0.05145641\n",
      "   0.14891217  0.04116453  0.05403097  0.10705797  0.02665056  0.1416396\n",
      "  -0.02993843  0.15862342  0.00705638 -0.02686639  0.00418529  0.05368456\n",
      "   0.0232263   0.05338121 -0.05168162 -0.05255386 -0.06440988  0.04640635\n",
      "   0.07965456  0.0745653  -0.03045817  0.03901522 -0.00522208 -0.03943786\n",
      "  -0.07464357  0.00052379 -0.05137775 -0.00061662  0.05656252  0.06800433\n",
      "   0.04109303 -0.04586214  0.03832147  0.03039682 -0.03835397 -0.01085573\n",
      "  -0.01086836  0.01215771  0.04525169 -0.02688099  0.03665852 -0.03176941\n",
      "  -0.05309443  0.01628397 -0.00969464  0.00931415  0.03381175 -0.0039615\n",
      "  -0.07279305  0.04909938 -0.04870825 -0.02035522 -0.03975826 -0.0046165\n",
      "  -0.09452628 -0.03436861 -0.01483785  0.0124673   0.04346539  0.0627957\n",
      "  -0.01892511 -0.09065516  0.03564098 -0.00471679 -0.01055832  0.06489652\n",
      "  -0.04137281 -0.00534987 -0.03668121  0.04227054 -0.0411679   0.04987877\n",
      "   0.01563672 -0.01818257 -0.0182099   0.03443216 -0.01246024 -0.04236024\n",
      "  -0.0284305  -0.04058596  0.03102707  0.01550487  0.10953944 -0.04315613\n",
      "   0.059108    0.01765217  0.04875019 -0.02304823  0.02438069  0.01622641\n",
      "   0.04801353  0.02355955 -0.05851124 -0.01156922]\n",
      " [ 0.22122047  0.01081009 -0.01733706 -0.00246773 -0.06654218  0.0205557\n",
      "   0.05082345  0.07746329  0.00254584  0.00683978 -0.08785785  0.04211503\n",
      "   0.02941155 -0.03700957 -0.02253655  0.09197385  0.00145455 -0.08934682\n",
      "  -0.01990482  0.1507421  -0.05146896 -0.04536881 -0.07174796  0.04044132\n",
      "   0.06460146 -0.13612182 -0.03377108  0.01616387  0.01131259  0.06406631\n",
      "  -0.00772786  0.03655459  0.04350979 -0.05020704 -0.05095412  0.07473247\n",
      "   0.07554568 -0.04376315  0.10961037  0.01899534  0.00086957  0.03139186\n",
      "  -0.03876705  0.02322245  0.09733443  0.09848437  0.02050478  0.01876562\n",
      "   0.04056484  0.02299239  0.10639096  0.05224397 -0.00844136 -0.01009173\n",
      "   0.10189643  0.11851419 -0.0117931  -0.04088373 -0.07632745 -0.07083904\n",
      "   0.02548984 -0.136813   -0.00743807  0.00703141  0.07266774  0.00972214\n",
      "  -0.06601731 -0.04783389 -0.09153764  0.01551669 -0.02029947  0.08694364\n",
      "  -0.02089546 -0.02102123 -0.01965621 -0.07314735  0.01748878 -0.02742541\n",
      "  -0.01409314  0.08673558  0.01747065 -0.02178221  0.04579135 -0.05764752\n",
      "   0.02722747  0.08756388 -0.033237   -0.13783892 -0.03892394 -0.05188623\n",
      "   0.03032474 -0.04666438  0.08075232 -0.02495909  0.06831993  0.00120694\n",
      "  -0.00251895 -0.06683    -0.01103844 -0.03728259]]\n",
      "\n",
      "Generando representación para combinación: ['text_cleaning']\n",
      "Combinacion: ['text_cleaning'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['text_cleaning'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['text_cleaning'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['text_cleaning'] + TF-IDF + SVD\n",
      "Preview: [[ 0.21326235  0.20261865 -0.21368052  0.07862102  0.02849024 -0.0027561\n",
      "  -0.12475158  0.05616941  0.04286994  0.13819202  0.1282607   0.12526847\n",
      "   0.06763953 -0.08520245 -0.03498505 -0.00717128 -0.03151448  0.15472615\n",
      "  -0.03306683 -0.06670136  0.05826755 -0.06428152 -0.02013905  0.06131957\n",
      "  -0.04414126 -0.06528492  0.05562204 -0.12638188  0.02682464  0.00214137\n",
      "  -0.02860331 -0.13305659  0.05234998 -0.01864221  0.06859182  0.05064143\n",
      "  -0.06563622 -0.06429671  0.09166027 -0.03709604  0.05860521  0.03872112\n",
      "  -0.02426133  0.00480755 -0.01663462  0.07357481  0.02758906 -0.02117219\n",
      "   0.03557978  0.08404286  0.03265008  0.04148683  0.0337202   0.00335258\n",
      "  -0.05182902 -0.10265444  0.04069684  0.02738081  0.06582708 -0.03946797\n",
      "   0.10276727  0.02476295  0.01781439 -0.02378977  0.0026437   0.06909638\n",
      "  -0.04611049  0.00944674  0.0383978   0.03025846  0.05681691  0.12489988\n",
      "   0.04451315 -0.00561619 -0.12169536 -0.08819937  0.0391935   0.09211017\n",
      "  -0.0176859  -0.06548463 -0.03312144 -0.0427343   0.03524257  0.00490245\n",
      "   0.00936553  0.04250972 -0.07291881  0.05617808 -0.04762524  0.00450871\n",
      "  -0.06080556 -0.01575135 -0.13275514  0.01088312  0.00709654  0.00736595\n",
      "   0.01498304 -0.02475136 -0.00254001  0.02692702]\n",
      " [ 0.34620765 -0.02939947  0.0486991  -0.02823063 -0.10726361 -0.05145641\n",
      "   0.14891217  0.04116453  0.05403097  0.10705797  0.02665056  0.1416396\n",
      "  -0.02993843  0.15862342  0.00705638 -0.02686639  0.00418529  0.05368456\n",
      "   0.0232263   0.05338121 -0.05168162 -0.05255386 -0.06440988  0.04640635\n",
      "   0.07965456  0.0745653  -0.03045817  0.03901522 -0.00522208 -0.03943786\n",
      "  -0.07464357  0.00052379 -0.05137775 -0.00061662  0.05656252  0.06800433\n",
      "   0.04109303 -0.04586214  0.03832147  0.03039682 -0.03835397 -0.01085573\n",
      "  -0.01086836  0.01215771  0.04525169 -0.02688099  0.03665852 -0.03176941\n",
      "  -0.05309443  0.01628397 -0.00969464  0.00931415  0.03381175 -0.0039615\n",
      "  -0.07279305  0.04909938 -0.04870825 -0.02035522 -0.03975826 -0.0046165\n",
      "  -0.09452628 -0.03436861 -0.01483785  0.0124673   0.04346539  0.0627957\n",
      "  -0.01892511 -0.09065516  0.03564098 -0.00471679 -0.01055832  0.06489652\n",
      "  -0.04137281 -0.00534987 -0.03668121  0.04227054 -0.0411679   0.04987877\n",
      "   0.01563672 -0.01818257 -0.0182099   0.03443216 -0.01246024 -0.04236024\n",
      "  -0.0284305  -0.04058596  0.03102707  0.01550487  0.10953944 -0.04315613\n",
      "   0.059108    0.01765217  0.04875019 -0.02304823  0.02438069  0.01622641\n",
      "   0.04801353  0.02355955 -0.05851124 -0.01156922]\n",
      " [ 0.22122047  0.01081009 -0.01733706 -0.00246773 -0.06654218  0.0205557\n",
      "   0.05082345  0.07746329  0.00254584  0.00683978 -0.08785785  0.04211503\n",
      "   0.02941155 -0.03700957 -0.02253655  0.09197385  0.00145455 -0.08934682\n",
      "  -0.01990482  0.1507421  -0.05146896 -0.04536881 -0.07174796  0.04044132\n",
      "   0.06460146 -0.13612182 -0.03377108  0.01616387  0.01131259  0.06406631\n",
      "  -0.00772786  0.03655459  0.04350979 -0.05020704 -0.05095412  0.07473247\n",
      "   0.07554568 -0.04376315  0.10961037  0.01899534  0.00086957  0.03139186\n",
      "  -0.03876705  0.02322245  0.09733443  0.09848437  0.02050478  0.01876562\n",
      "   0.04056484  0.02299239  0.10639096  0.05224397 -0.00844136 -0.01009173\n",
      "   0.10189643  0.11851419 -0.0117931  -0.04088373 -0.07632745 -0.07083904\n",
      "   0.02548984 -0.136813   -0.00743807  0.00703141  0.07266774  0.00972214\n",
      "  -0.06601731 -0.04783389 -0.09153764  0.01551669 -0.02029947  0.08694364\n",
      "  -0.02089546 -0.02102123 -0.01965621 -0.07314735  0.01748878 -0.02742541\n",
      "  -0.01409314  0.08673558  0.01747065 -0.02178221  0.04579135 -0.05764752\n",
      "   0.02722747  0.08756388 -0.033237   -0.13783892 -0.03892394 -0.05188623\n",
      "   0.03032474 -0.04666438  0.08075232 -0.02495909  0.06831993  0.00120694\n",
      "  -0.00251895 -0.06683    -0.01103844 -0.03728259]]\n",
      "\n",
      "Generando representación para combinación: ['tokenization', 'stop_words']\n",
      "Combinacion: ['tokenization', 'stop_words'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'stop_words'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'stop_words'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['tokenization', 'stop_words'] + TF-IDF + SVD\n",
      "Preview: [[ 0.09402547  0.00692753  0.31393842  0.23392335  0.05038961 -0.01224268\n",
      "   0.00057087 -0.08633356 -0.06354378 -0.1470099   0.00472806  0.02713558\n",
      "  -0.05923257 -0.03661082 -0.02669274 -0.03112447 -0.10028648 -0.05366383\n",
      "   0.07223616 -0.08461541 -0.03824699 -0.00850874  0.00521863 -0.16445306\n",
      "   0.07190091 -0.11064683  0.05038461  0.05193078 -0.00120696 -0.03107336\n",
      "   0.01581028 -0.10623045 -0.01627469 -0.00656267  0.0295551  -0.03039038\n",
      "   0.05719325 -0.09301028 -0.03639778  0.01882082 -0.00209754  0.00534052\n",
      "   0.04213184  0.06468845 -0.07539035  0.06668636  0.03524807 -0.05196819\n",
      "   0.03338108  0.02137281  0.00819189 -0.00224891  0.01566754 -0.00337913\n",
      "   0.02108156 -0.00285243  0.00139894 -0.02917207  0.05689974  0.03658664\n",
      "   0.03783785 -0.05956021 -0.0301647   0.00959958  0.02504969  0.063372\n",
      "  -0.07329405  0.01936748 -0.03606439 -0.00374952 -0.07080865 -0.02888877\n",
      "   0.00866113  0.04279426 -0.05893162 -0.00642132 -0.01554745  0.07083611\n",
      "   0.11931234 -0.0024855   0.003204   -0.0747667  -0.01055952 -0.04915571\n",
      "  -0.03536305 -0.02702593  0.04884809  0.01503137  0.06140937 -0.00526077\n",
      "  -0.04305591  0.10432411  0.08802939 -0.01378976 -0.00935489 -0.04182177\n",
      "   0.00466794  0.0393542   0.08112346  0.05188555]\n",
      " [ 0.05603424 -0.00085171  0.01460443 -0.00475542  0.00327792 -0.02695866\n",
      "  -0.07605933  0.19728545  0.16268673 -0.1711805   0.05405431 -0.07910689\n",
      "   0.00073039  0.06027092 -0.03275289  0.00746047  0.0175537   0.04033246\n",
      "  -0.01373645  0.06359159  0.04622598 -0.00332039  0.07000791  0.04031366\n",
      "  -0.00750339 -0.04524755 -0.02337904 -0.03700055 -0.04970855  0.03692911\n",
      "   0.03604244 -0.00490885 -0.07459299 -0.01955878 -0.00690494 -0.0205463\n",
      "   0.05001535  0.07971165  0.01766632 -0.00481986  0.03534542 -0.04895354\n",
      "  -0.02507114 -0.00311711 -0.04661993  0.02177394 -0.06945349  0.03393768\n",
      "   0.00450158  0.01715929  0.02441989  0.00285295  0.07129939  0.00624546\n",
      "  -0.11030167 -0.00179844 -0.10189371 -0.01311105  0.07321596 -0.01075254\n",
      "  -0.00923096 -0.02981249  0.02470521 -0.00854966  0.0345338  -0.03271123\n",
      "   0.03402425 -0.02718855 -0.01502621  0.00052829 -0.13278866  0.0216354\n",
      "   0.1135355   0.07570904  0.01436017  0.14962302  0.03200699  0.0166073\n",
      "   0.09523464  0.03972642  0.06265628 -0.00404762 -0.098736    0.01829004\n",
      "   0.07379946  0.0599238   0.0233394   0.01619401  0.09018041  0.01055124\n",
      "  -0.12237562 -0.03948309 -0.15040783  0.09171915 -0.04848045 -0.00420397\n",
      "  -0.01337636  0.06550225  0.01136331 -0.01010407]\n",
      " [ 0.01980927  0.00172817  0.00762395  0.00057897  0.00972052  0.0125369\n",
      "   0.00157258  0.02145395  0.03323677  0.00713893 -0.01657332  0.02775048\n",
      "   0.00047858 -0.00395746  0.01853868 -0.01299819  0.00722903 -0.0033744\n",
      "   0.00556441  0.01295606 -0.02251843 -0.01029042  0.00931102  0.01966794\n",
      "   0.08543057 -0.04290604  0.02738341  0.02153165 -0.03119157  0.05408853\n",
      "  -0.07073328 -0.02025036  0.04365734 -0.0627968  -0.01519786  0.00186094\n",
      "   0.0429838   0.06186165 -0.01885723 -0.02442339  0.01789415 -0.00238062\n",
      "  -0.07561958 -0.08531162 -0.08541416 -0.00055827  0.06047012 -0.02788266\n",
      "  -0.01886225 -0.06628333  0.09705041  0.01683248  0.03382352  0.12982084\n",
      "  -0.14266635  0.08607279 -0.15368183  0.00106324  0.07267066  0.0102742\n",
      "   0.08075725 -0.00077101  0.02429179 -0.09260508 -0.02678659 -0.0685862\n",
      "   0.07458254  0.02766133 -0.01716094 -0.05271635  0.02304804  0.02362272\n",
      "   0.12116453 -0.09339263 -0.03233298 -0.10588154 -0.0134785   0.11541077\n",
      "  -0.02464533  0.0490434   0.02337861  0.03658819  0.0396435  -0.00485388\n",
      "   0.09434456 -0.04955586  0.0699759   0.03086147  0.01222993  0.10996703\n",
      "  -0.08478178  0.02916333 -0.04453076  0.01760192 -0.04589207  0.05064755\n",
      "  -0.15083728  0.10520632  0.0625905  -0.06302746]]\n",
      "\n",
      "Generando representación para combinación: ['tokenization', 'lemmatization']\n",
      "Combinacion: ['tokenization', 'lemmatization'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'lemmatization'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'lemmatization'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['tokenization', 'lemmatization'] + TF-IDF + SVD\n",
      "Preview: [[ 2.58732308e-01 -2.17699081e-01 -2.30003648e-01  3.58220938e-02\n",
      "   6.43548112e-02 -4.51029672e-02  4.61327696e-02  2.11713407e-01\n",
      "  -1.05611872e-01 -1.25737628e-02 -8.85947269e-03  5.28358472e-02\n",
      "   6.25663683e-02 -1.39671136e-01 -1.19166476e-01  1.07064127e-01\n",
      "   6.00289215e-02 -3.80155094e-02  4.67562059e-02 -3.41495148e-02\n",
      "  -3.14636099e-02 -1.14831857e-01 -7.81652705e-02  4.59356404e-02\n",
      "   1.11325608e-01  1.71189601e-02 -4.83986779e-02 -1.17047104e-01\n",
      "  -2.46381935e-02  1.65801152e-02  1.51678439e-01  4.78740946e-02\n",
      "   4.89319521e-02 -5.79063991e-02 -4.02455528e-02 -3.14529112e-02\n",
      "  -6.57704569e-02  7.69282435e-03  3.70884541e-02 -1.80901182e-03\n",
      "  -2.93379983e-02 -3.76769910e-02  1.54081642e-01  2.29610382e-02\n",
      "   4.78071398e-02 -7.60369263e-02 -9.84335622e-02  2.04214499e-02\n",
      "   3.02262761e-02  8.16424849e-02 -2.59857923e-02 -2.60851133e-02\n",
      "   2.00473720e-02  1.97654364e-02  8.15734202e-03 -7.41696794e-02\n",
      "   2.69439655e-02  4.89535447e-02  9.39000311e-03 -3.66900130e-02\n",
      "   3.68525683e-02  2.89320021e-02 -5.03404537e-02 -2.39990444e-02\n",
      "   8.32093591e-02 -1.05242614e-01  5.93892041e-02  3.78232306e-02\n",
      "   3.35042456e-02  8.53836508e-02  2.24166009e-02 -2.09051544e-02\n",
      "  -1.04679836e-02 -7.89690452e-03 -7.03492808e-02 -8.55702629e-02\n",
      "  -6.76941549e-03  6.58673288e-02 -4.44161202e-02  4.36404642e-02\n",
      "   2.08549674e-02 -8.00137417e-02 -7.74807391e-03 -5.05852121e-03\n",
      "  -5.56419678e-02 -1.10471105e-02  7.06233461e-03  1.85954467e-02\n",
      "  -5.02657582e-02  7.52968211e-02 -2.39175453e-02 -3.21334873e-02\n",
      "   4.45856405e-03  1.11682565e-02 -8.93985268e-03  1.87491911e-02\n",
      "  -1.04329464e-02 -5.24421921e-02 -4.99070595e-03  7.82646516e-02]\n",
      " [ 4.01952094e-01 -5.93411456e-03  1.17069815e-02 -5.78040437e-02\n",
      "  -1.07575096e-01 -3.77414589e-02 -1.42480857e-02 -1.57153350e-01\n",
      "  -5.96854809e-02  9.09757503e-02  1.44866992e-01 -6.25035367e-03\n",
      "   6.47933931e-02  3.85353338e-02 -4.39655180e-02  1.14635887e-01\n",
      "  -6.32007610e-02 -9.97433957e-04  2.79367725e-02 -1.45990915e-02\n",
      "  -5.41689400e-03  1.36378431e-02 -1.64885657e-02  8.23209291e-03\n",
      "   3.17959885e-02 -3.11694008e-02 -1.65869999e-02 -5.04967002e-02\n",
      "  -1.03686046e-02 -7.07872157e-02 -6.71455287e-03  8.11081919e-03\n",
      "  -1.75864898e-02 -3.40438655e-02  1.57351330e-02  5.69560820e-02\n",
      "   4.32474091e-02 -1.19402474e-01 -1.96941874e-02  3.82517086e-02\n",
      "   3.87603796e-02 -4.07893202e-02 -3.95829177e-03 -1.28314409e-02\n",
      "   1.87673800e-02 -2.14875102e-02  3.51389827e-02  2.28263201e-02\n",
      "   5.16703972e-02  1.46940580e-02 -6.70884438e-02 -3.19472555e-04\n",
      "  -3.49481473e-02  3.51393158e-02  4.52439834e-02 -9.62388929e-03\n",
      "   1.12289975e-02  1.40675833e-01  4.03177113e-02 -7.83772563e-02\n",
      "  -5.51651744e-02 -1.88680585e-02 -3.09059014e-03 -2.70455851e-02\n",
      "  -3.52427413e-02 -2.80160428e-02  3.81047415e-02 -5.70451330e-02\n",
      "  -4.94657843e-02 -6.23508354e-02 -3.20225352e-02  1.93663549e-02\n",
      "  -3.49516440e-02 -5.27112250e-02  4.32672639e-03 -1.41095159e-02\n",
      "   6.92447088e-02 -3.78850506e-02  3.61087801e-02 -2.18032047e-02\n",
      "  -1.79404328e-02  5.42497162e-02  3.52687258e-03 -1.67703745e-02\n",
      "  -4.58497630e-02 -3.24594231e-02 -4.69236594e-02  5.94699625e-02\n",
      "   6.43234632e-02 -1.44726102e-02 -2.57463054e-02 -6.40784624e-03\n",
      "  -6.30375094e-02 -7.72960909e-03 -3.88753295e-05 -3.09365149e-02\n",
      "  -3.34603932e-02  9.29563043e-02  5.29171632e-03 -7.19042730e-03]\n",
      " [ 2.29693699e-01 -2.46383506e-02 -3.46250863e-03 -5.11480149e-02\n",
      "  -5.02429699e-02 -9.93964972e-02 -6.90910496e-03 -5.53184194e-02\n",
      "  -3.59752909e-03 -4.21967282e-02  1.42636608e-03  2.94072469e-02\n",
      "  -9.18831987e-02 -7.20884648e-02 -1.83996970e-02 -4.24494638e-02\n",
      "  -3.92893505e-02  1.40673440e-01  4.58892445e-02  2.06516454e-02\n",
      "  -8.58745807e-02  3.22398247e-02 -1.53625329e-02  1.35577271e-01\n",
      "   3.14047602e-02 -8.62411402e-03  6.69628339e-02 -5.68915828e-02\n",
      "   2.20157008e-03 -1.41932834e-02  1.30871946e-03  2.27194886e-02\n",
      "  -3.47752032e-02 -3.84990738e-02 -5.72960288e-02 -2.22979370e-02\n",
      "   1.74942197e-01 -1.07659912e-01 -6.64397911e-03  1.57301823e-02\n",
      "  -1.44695600e-02  1.78953647e-02 -6.55475711e-02  4.73377361e-02\n",
      "   3.24042025e-02  5.66789219e-02  1.29595007e-01  4.52041030e-02\n",
      "   1.29632671e-02  5.45427918e-02  2.14113030e-02 -1.79725203e-02\n",
      "  -5.06735036e-02  7.32185489e-02  9.97859270e-02  2.91529712e-02\n",
      "  -5.88582561e-02  1.01988698e-01 -1.15366763e-01 -1.48195266e-01\n",
      "   9.02138486e-02 -4.57713451e-02  1.14974756e-02  7.41291353e-02\n",
      "  -1.53540622e-03 -3.75431547e-02 -4.37280937e-03 -2.16770056e-02\n",
      "  -6.15454577e-02 -8.84713669e-02 -5.93432784e-02 -1.14666003e-01\n",
      "   1.21600496e-01  9.57421560e-02 -1.21464773e-02 -1.20261988e-02\n",
      "   3.58949365e-02  2.01165432e-02 -2.39764706e-02  1.81709181e-02\n",
      "  -3.65520600e-02 -6.48791093e-02  4.08466375e-02  3.91049849e-02\n",
      "   5.95944243e-03  2.79529594e-04  4.24409597e-02 -1.54595912e-02\n",
      "  -1.63565496e-02 -4.87768218e-02 -1.43652414e-02  1.06765664e-01\n",
      "   4.56250836e-03  5.14250910e-02  2.58115061e-02  9.28431344e-02\n",
      "   8.73446560e-02 -2.14242764e-02  6.60466602e-04  9.76226612e-03]]\n",
      "\n",
      "Generando representación para combinación: ['tokenization', 'text_cleaning']\n",
      "Combinacion: ['tokenization', 'text_cleaning'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'text_cleaning'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'text_cleaning'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['tokenization', 'text_cleaning'] + TF-IDF + SVD\n",
      "Preview: [[ 0.21325535  0.20262095 -0.21367434  0.07863674  0.02851618 -0.00283687\n",
      "  -0.12480647  0.05603082  0.04292183  0.13828595  0.12841309  0.12489723\n",
      "   0.06784272 -0.08523875 -0.03493458 -0.00703262 -0.03154778  0.15474321\n",
      "  -0.03300155 -0.06693806  0.05772574 -0.06454603 -0.0201187   0.0613133\n",
      "  -0.04400252 -0.06536267  0.05558152 -0.12569517  0.02889973  0.00122362\n",
      "  -0.0295148  -0.13323333  0.05198745 -0.01815969  0.06856915  0.05112867\n",
      "  -0.06537077 -0.06415201  0.09177779 -0.03740334  0.05819304  0.0390474\n",
      "  -0.02446375  0.0047581  -0.01671507 -0.07377665  0.02732444 -0.02264419\n",
      "   0.03399866  0.08502541  0.03077346  0.03967727  0.03436822  0.00482437\n",
      "  -0.0531394  -0.10253727  0.04310929  0.02477777  0.06456246 -0.03806365\n",
      "   0.10330049  0.02546871  0.01781539 -0.02384941  0.00245917  0.06866488\n",
      "  -0.04609385  0.00938935  0.03939831  0.02680736  0.05833455  0.12487085\n",
      "   0.04375159 -0.00486925 -0.12234202 -0.08817953  0.04120331  0.0913164\n",
      "  -0.01758572 -0.06478721 -0.03363233 -0.0427479   0.03535574  0.00452454\n",
      "   0.00887671  0.04193615 -0.07412563  0.05373605 -0.04809696  0.00091849\n",
      "  -0.06161282 -0.01859769 -0.1322473   0.01111243 -0.00855193  0.00796585\n",
      "   0.01498276 -0.0244229  -0.00367841  0.02583657]\n",
      " [ 0.3462129  -0.02940427  0.04868958 -0.02822149 -0.10728568 -0.05137275\n",
      "   0.14896526  0.0413588   0.05388535  0.10707663  0.02665271  0.14178602\n",
      "  -0.0294088   0.15859077  0.00699045 -0.02694187  0.00412093  0.05367565\n",
      "   0.02313177  0.0538069  -0.05137618 -0.05218856 -0.06429213  0.04654221\n",
      "   0.08023825  0.07460317 -0.03052931  0.03927068 -0.00324906 -0.04082435\n",
      "  -0.07281237  0.00092213 -0.05204845 -0.00042692  0.05651463  0.06796227\n",
      "   0.04141549 -0.04560565  0.0384891   0.03009031 -0.03817737 -0.01117864\n",
      "  -0.0103093   0.0124571   0.04555999  0.02749905  0.03705253 -0.02943702\n",
      "  -0.05386677  0.01652253 -0.01044872  0.00737511  0.03337353 -0.00379817\n",
      "  -0.07220825  0.04977779 -0.05093683 -0.0178358  -0.03808461 -0.00575202\n",
      "  -0.09453143 -0.03463527 -0.01459273  0.01241598  0.04322255  0.06168013\n",
      "  -0.01997935 -0.09100633  0.03611561 -0.005708   -0.01022918  0.06502812\n",
      "  -0.04160171 -0.00510488 -0.0371468   0.04217403 -0.0408559   0.04978897\n",
      "   0.01551959 -0.01793658 -0.01854964  0.03450999 -0.01230706 -0.04229954\n",
      "  -0.02816949 -0.04012365  0.03119552  0.01844471  0.10997126 -0.03767949\n",
      "   0.06061558  0.01982175  0.04935622 -0.02409115 -0.02401533  0.0168613\n",
      "   0.04827217  0.02552995 -0.05746108 -0.01175797]\n",
      " [ 0.22123036  0.01080333 -0.0173457  -0.00245514 -0.0665878   0.02058005\n",
      "   0.05073015  0.07750261  0.00251195  0.00685553 -0.08782456  0.04207897\n",
      "   0.02963547 -0.03706009 -0.02247692  0.09191226  0.00167657 -0.08935603\n",
      "  -0.02011958  0.15127924 -0.05059158 -0.04484066 -0.07168878  0.04035592\n",
      "   0.06426257 -0.1359773  -0.03397797  0.01654636  0.01024992  0.0635949\n",
      "  -0.00992947  0.03619522  0.04429614 -0.05025552 -0.05107333  0.0743163\n",
      "   0.07620828 -0.04367838  0.10979544  0.01850531  0.00070053  0.03143725\n",
      "  -0.03867391  0.02343615  0.09733244 -0.09866007  0.02024571  0.01704597\n",
      "   0.04114327  0.02570952  0.10548116  0.05326912 -0.00570458 -0.00958249\n",
      "   0.10227903  0.11812509 -0.01422137 -0.03921693 -0.07619216 -0.07226167\n",
      "   0.02581903 -0.13672162 -0.00706499  0.00708629  0.07253747  0.01088276\n",
      "  -0.06669867 -0.04625126 -0.09152716  0.01940969 -0.02078185  0.0861856\n",
      "  -0.02133972 -0.02095256 -0.01987262 -0.07317193  0.01774207 -0.0272782\n",
      "  -0.01425287  0.08672836  0.01938953 -0.02239048  0.04433681 -0.05726407\n",
      "   0.02636694  0.08770235 -0.03216828 -0.14161247 -0.03193046 -0.05006657\n",
      "   0.03202837 -0.04438843  0.08034488 -0.02667834 -0.06658075  0.00091158\n",
      "  -0.00308193 -0.06634074 -0.01190444 -0.03814246]]\n",
      "\n",
      "Generando representación para combinación: ['stop_words', 'lemmatization']\n",
      "Combinacion: ['stop_words', 'lemmatization'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['stop_words', 'lemmatization'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['stop_words', 'lemmatization'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['stop_words', 'lemmatization'] + TF-IDF + SVD\n",
      "Preview: [[ 0.21326235  0.20261865 -0.21368052  0.07862102  0.02849024 -0.0027561\n",
      "  -0.12475158  0.05616941  0.04286994  0.13819202  0.1282607   0.12526847\n",
      "   0.06763953 -0.08520245 -0.03498505 -0.00717128 -0.03151448  0.15472615\n",
      "  -0.03306683 -0.06670136  0.05826755 -0.06428152 -0.02013905  0.06131957\n",
      "  -0.04414126 -0.06528492  0.05562204 -0.12638188  0.02682464  0.00214137\n",
      "  -0.02860331 -0.13305659  0.05234998 -0.01864221  0.06859182  0.05064143\n",
      "  -0.06563622 -0.06429671  0.09166027 -0.03709604  0.05860521  0.03872112\n",
      "  -0.02426133  0.00480755 -0.01663462  0.07357481  0.02758906 -0.02117219\n",
      "   0.03557978  0.08404286  0.03265008  0.04148683  0.0337202   0.00335258\n",
      "  -0.05182902 -0.10265444  0.04069684  0.02738081  0.06582708 -0.03946797\n",
      "   0.10276727  0.02476295  0.01781439 -0.02378977  0.0026437   0.06909638\n",
      "  -0.04611049  0.00944674  0.0383978   0.03025846  0.05681691  0.12489988\n",
      "   0.04451315 -0.00561619 -0.12169536 -0.08819937  0.0391935   0.09211017\n",
      "  -0.0176859  -0.06548463 -0.03312144 -0.0427343   0.03524257  0.00490245\n",
      "   0.00936553  0.04250972 -0.07291881  0.05617808 -0.04762524  0.00450871\n",
      "  -0.06080556 -0.01575135 -0.13275514  0.01088312  0.00709654  0.00736595\n",
      "   0.01498304 -0.02475136 -0.00254001  0.02692702]\n",
      " [ 0.34620765 -0.02939947  0.0486991  -0.02823063 -0.10726361 -0.05145641\n",
      "   0.14891217  0.04116453  0.05403097  0.10705797  0.02665056  0.1416396\n",
      "  -0.02993843  0.15862342  0.00705638 -0.02686639  0.00418529  0.05368456\n",
      "   0.0232263   0.05338121 -0.05168162 -0.05255386 -0.06440988  0.04640635\n",
      "   0.07965456  0.0745653  -0.03045817  0.03901522 -0.00522208 -0.03943786\n",
      "  -0.07464357  0.00052379 -0.05137775 -0.00061662  0.05656252  0.06800433\n",
      "   0.04109303 -0.04586214  0.03832147  0.03039682 -0.03835397 -0.01085573\n",
      "  -0.01086836  0.01215771  0.04525169 -0.02688099  0.03665852 -0.03176941\n",
      "  -0.05309443  0.01628397 -0.00969464  0.00931415  0.03381175 -0.0039615\n",
      "  -0.07279305  0.04909938 -0.04870825 -0.02035522 -0.03975826 -0.0046165\n",
      "  -0.09452628 -0.03436861 -0.01483785  0.0124673   0.04346539  0.0627957\n",
      "  -0.01892511 -0.09065516  0.03564098 -0.00471679 -0.01055832  0.06489652\n",
      "  -0.04137281 -0.00534987 -0.03668121  0.04227054 -0.0411679   0.04987877\n",
      "   0.01563672 -0.01818257 -0.0182099   0.03443216 -0.01246024 -0.04236024\n",
      "  -0.0284305  -0.04058596  0.03102707  0.01550487  0.10953944 -0.04315613\n",
      "   0.059108    0.01765217  0.04875019 -0.02304823  0.02438069  0.01622641\n",
      "   0.04801353  0.02355955 -0.05851124 -0.01156922]\n",
      " [ 0.22122047  0.01081009 -0.01733706 -0.00246773 -0.06654218  0.0205557\n",
      "   0.05082345  0.07746329  0.00254584  0.00683978 -0.08785785  0.04211503\n",
      "   0.02941155 -0.03700957 -0.02253655  0.09197385  0.00145455 -0.08934682\n",
      "  -0.01990482  0.1507421  -0.05146896 -0.04536881 -0.07174796  0.04044132\n",
      "   0.06460146 -0.13612182 -0.03377108  0.01616387  0.01131259  0.06406631\n",
      "  -0.00772786  0.03655459  0.04350979 -0.05020704 -0.05095412  0.07473247\n",
      "   0.07554568 -0.04376315  0.10961037  0.01899534  0.00086957  0.03139186\n",
      "  -0.03876705  0.02322245  0.09733443  0.09848437  0.02050478  0.01876562\n",
      "   0.04056484  0.02299239  0.10639096  0.05224397 -0.00844136 -0.01009173\n",
      "   0.10189643  0.11851419 -0.0117931  -0.04088373 -0.07632745 -0.07083904\n",
      "   0.02548984 -0.136813   -0.00743807  0.00703141  0.07266774  0.00972214\n",
      "  -0.06601731 -0.04783389 -0.09153764  0.01551669 -0.02029947  0.08694364\n",
      "  -0.02089546 -0.02102123 -0.01965621 -0.07314735  0.01748878 -0.02742541\n",
      "  -0.01409314  0.08673558  0.01747065 -0.02178221  0.04579135 -0.05764752\n",
      "   0.02722747  0.08756388 -0.033237   -0.13783892 -0.03892394 -0.05188623\n",
      "   0.03032474 -0.04666438  0.08075232 -0.02495909  0.06831993  0.00120694\n",
      "  -0.00251895 -0.06683    -0.01103844 -0.03728259]]\n",
      "\n",
      "Generando representación para combinación: ['stop_words', 'text_cleaning']\n",
      "Combinacion: ['stop_words', 'text_cleaning'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['stop_words', 'text_cleaning'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['stop_words', 'text_cleaning'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['stop_words', 'text_cleaning'] + TF-IDF + SVD\n",
      "Preview: [[ 0.21326235  0.20261865 -0.21368052  0.07862102  0.02849024 -0.0027561\n",
      "  -0.12475158  0.05616941  0.04286994  0.13819202  0.1282607   0.12526847\n",
      "   0.06763953 -0.08520245 -0.03498505 -0.00717128 -0.03151448  0.15472615\n",
      "  -0.03306683 -0.06670136  0.05826755 -0.06428152 -0.02013905  0.06131957\n",
      "  -0.04414126 -0.06528492  0.05562204 -0.12638188  0.02682464  0.00214137\n",
      "  -0.02860331 -0.13305659  0.05234998 -0.01864221  0.06859182  0.05064143\n",
      "  -0.06563622 -0.06429671  0.09166027 -0.03709604  0.05860521  0.03872112\n",
      "  -0.02426133  0.00480755 -0.01663462  0.07357481  0.02758906 -0.02117219\n",
      "   0.03557978  0.08404286  0.03265008  0.04148683  0.0337202   0.00335258\n",
      "  -0.05182902 -0.10265444  0.04069684  0.02738081  0.06582708 -0.03946797\n",
      "   0.10276727  0.02476295  0.01781439 -0.02378977  0.0026437   0.06909638\n",
      "  -0.04611049  0.00944674  0.0383978   0.03025846  0.05681691  0.12489988\n",
      "   0.04451315 -0.00561619 -0.12169536 -0.08819937  0.0391935   0.09211017\n",
      "  -0.0176859  -0.06548463 -0.03312144 -0.0427343   0.03524257  0.00490245\n",
      "   0.00936553  0.04250972 -0.07291881  0.05617808 -0.04762524  0.00450871\n",
      "  -0.06080556 -0.01575135 -0.13275514  0.01088312  0.00709654  0.00736595\n",
      "   0.01498304 -0.02475136 -0.00254001  0.02692702]\n",
      " [ 0.34620765 -0.02939947  0.0486991  -0.02823063 -0.10726361 -0.05145641\n",
      "   0.14891217  0.04116453  0.05403097  0.10705797  0.02665056  0.1416396\n",
      "  -0.02993843  0.15862342  0.00705638 -0.02686639  0.00418529  0.05368456\n",
      "   0.0232263   0.05338121 -0.05168162 -0.05255386 -0.06440988  0.04640635\n",
      "   0.07965456  0.0745653  -0.03045817  0.03901522 -0.00522208 -0.03943786\n",
      "  -0.07464357  0.00052379 -0.05137775 -0.00061662  0.05656252  0.06800433\n",
      "   0.04109303 -0.04586214  0.03832147  0.03039682 -0.03835397 -0.01085573\n",
      "  -0.01086836  0.01215771  0.04525169 -0.02688099  0.03665852 -0.03176941\n",
      "  -0.05309443  0.01628397 -0.00969464  0.00931415  0.03381175 -0.0039615\n",
      "  -0.07279305  0.04909938 -0.04870825 -0.02035522 -0.03975826 -0.0046165\n",
      "  -0.09452628 -0.03436861 -0.01483785  0.0124673   0.04346539  0.0627957\n",
      "  -0.01892511 -0.09065516  0.03564098 -0.00471679 -0.01055832  0.06489652\n",
      "  -0.04137281 -0.00534987 -0.03668121  0.04227054 -0.0411679   0.04987877\n",
      "   0.01563672 -0.01818257 -0.0182099   0.03443216 -0.01246024 -0.04236024\n",
      "  -0.0284305  -0.04058596  0.03102707  0.01550487  0.10953944 -0.04315613\n",
      "   0.059108    0.01765217  0.04875019 -0.02304823  0.02438069  0.01622641\n",
      "   0.04801353  0.02355955 -0.05851124 -0.01156922]\n",
      " [ 0.22122047  0.01081009 -0.01733706 -0.00246773 -0.06654218  0.0205557\n",
      "   0.05082345  0.07746329  0.00254584  0.00683978 -0.08785785  0.04211503\n",
      "   0.02941155 -0.03700957 -0.02253655  0.09197385  0.00145455 -0.08934682\n",
      "  -0.01990482  0.1507421  -0.05146896 -0.04536881 -0.07174796  0.04044132\n",
      "   0.06460146 -0.13612182 -0.03377108  0.01616387  0.01131259  0.06406631\n",
      "  -0.00772786  0.03655459  0.04350979 -0.05020704 -0.05095412  0.07473247\n",
      "   0.07554568 -0.04376315  0.10961037  0.01899534  0.00086957  0.03139186\n",
      "  -0.03876705  0.02322245  0.09733443  0.09848437  0.02050478  0.01876562\n",
      "   0.04056484  0.02299239  0.10639096  0.05224397 -0.00844136 -0.01009173\n",
      "   0.10189643  0.11851419 -0.0117931  -0.04088373 -0.07632745 -0.07083904\n",
      "   0.02548984 -0.136813   -0.00743807  0.00703141  0.07266774  0.00972214\n",
      "  -0.06601731 -0.04783389 -0.09153764  0.01551669 -0.02029947  0.08694364\n",
      "  -0.02089546 -0.02102123 -0.01965621 -0.07314735  0.01748878 -0.02742541\n",
      "  -0.01409314  0.08673558  0.01747065 -0.02178221  0.04579135 -0.05764752\n",
      "   0.02722747  0.08756388 -0.033237   -0.13783892 -0.03892394 -0.05188623\n",
      "   0.03032474 -0.04666438  0.08075232 -0.02495909  0.06831993  0.00120694\n",
      "  -0.00251895 -0.06683    -0.01103844 -0.03728259]]\n",
      "\n",
      "Generando representación para combinación: ['lemmatization', 'text_cleaning']\n",
      "Combinacion: ['lemmatization', 'text_cleaning'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['lemmatization', 'text_cleaning'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['lemmatization', 'text_cleaning'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "Preview: [[ 0.21326235  0.20261865 -0.21368052  0.07862102  0.02849024 -0.0027561\n",
      "  -0.12475158  0.05616941  0.04286994  0.13819202  0.1282607   0.12526847\n",
      "   0.06763953 -0.08520245 -0.03498505 -0.00717128 -0.03151448  0.15472615\n",
      "  -0.03306683 -0.06670136  0.05826755 -0.06428152 -0.02013905  0.06131957\n",
      "  -0.04414126 -0.06528492  0.05562204 -0.12638188  0.02682464  0.00214137\n",
      "  -0.02860331 -0.13305659  0.05234998 -0.01864221  0.06859182  0.05064143\n",
      "  -0.06563622 -0.06429671  0.09166027 -0.03709604  0.05860521  0.03872112\n",
      "  -0.02426133  0.00480755 -0.01663462  0.07357481  0.02758906 -0.02117219\n",
      "   0.03557978  0.08404286  0.03265008  0.04148683  0.0337202   0.00335258\n",
      "  -0.05182902 -0.10265444  0.04069684  0.02738081  0.06582708 -0.03946797\n",
      "   0.10276727  0.02476295  0.01781439 -0.02378977  0.0026437   0.06909638\n",
      "  -0.04611049  0.00944674  0.0383978   0.03025846  0.05681691  0.12489988\n",
      "   0.04451315 -0.00561619 -0.12169536 -0.08819937  0.0391935   0.09211017\n",
      "  -0.0176859  -0.06548463 -0.03312144 -0.0427343   0.03524257  0.00490245\n",
      "   0.00936553  0.04250972 -0.07291881  0.05617808 -0.04762524  0.00450871\n",
      "  -0.06080556 -0.01575135 -0.13275514  0.01088312  0.00709654  0.00736595\n",
      "   0.01498304 -0.02475136 -0.00254001  0.02692702]\n",
      " [ 0.34620765 -0.02939947  0.0486991  -0.02823063 -0.10726361 -0.05145641\n",
      "   0.14891217  0.04116453  0.05403097  0.10705797  0.02665056  0.1416396\n",
      "  -0.02993843  0.15862342  0.00705638 -0.02686639  0.00418529  0.05368456\n",
      "   0.0232263   0.05338121 -0.05168162 -0.05255386 -0.06440988  0.04640635\n",
      "   0.07965456  0.0745653  -0.03045817  0.03901522 -0.00522208 -0.03943786\n",
      "  -0.07464357  0.00052379 -0.05137775 -0.00061662  0.05656252  0.06800433\n",
      "   0.04109303 -0.04586214  0.03832147  0.03039682 -0.03835397 -0.01085573\n",
      "  -0.01086836  0.01215771  0.04525169 -0.02688099  0.03665852 -0.03176941\n",
      "  -0.05309443  0.01628397 -0.00969464  0.00931415  0.03381175 -0.0039615\n",
      "  -0.07279305  0.04909938 -0.04870825 -0.02035522 -0.03975826 -0.0046165\n",
      "  -0.09452628 -0.03436861 -0.01483785  0.0124673   0.04346539  0.0627957\n",
      "  -0.01892511 -0.09065516  0.03564098 -0.00471679 -0.01055832  0.06489652\n",
      "  -0.04137281 -0.00534987 -0.03668121  0.04227054 -0.0411679   0.04987877\n",
      "   0.01563672 -0.01818257 -0.0182099   0.03443216 -0.01246024 -0.04236024\n",
      "  -0.0284305  -0.04058596  0.03102707  0.01550487  0.10953944 -0.04315613\n",
      "   0.059108    0.01765217  0.04875019 -0.02304823  0.02438069  0.01622641\n",
      "   0.04801353  0.02355955 -0.05851124 -0.01156922]\n",
      " [ 0.22122047  0.01081009 -0.01733706 -0.00246773 -0.06654218  0.0205557\n",
      "   0.05082345  0.07746329  0.00254584  0.00683978 -0.08785785  0.04211503\n",
      "   0.02941155 -0.03700957 -0.02253655  0.09197385  0.00145455 -0.08934682\n",
      "  -0.01990482  0.1507421  -0.05146896 -0.04536881 -0.07174796  0.04044132\n",
      "   0.06460146 -0.13612182 -0.03377108  0.01616387  0.01131259  0.06406631\n",
      "  -0.00772786  0.03655459  0.04350979 -0.05020704 -0.05095412  0.07473247\n",
      "   0.07554568 -0.04376315  0.10961037  0.01899534  0.00086957  0.03139186\n",
      "  -0.03876705  0.02322245  0.09733443  0.09848437  0.02050478  0.01876562\n",
      "   0.04056484  0.02299239  0.10639096  0.05224397 -0.00844136 -0.01009173\n",
      "   0.10189643  0.11851419 -0.0117931  -0.04088373 -0.07632745 -0.07083904\n",
      "   0.02548984 -0.136813   -0.00743807  0.00703141  0.07266774  0.00972214\n",
      "  -0.06601731 -0.04783389 -0.09153764  0.01551669 -0.02029947  0.08694364\n",
      "  -0.02089546 -0.02102123 -0.01965621 -0.07314735  0.01748878 -0.02742541\n",
      "  -0.01409314  0.08673558  0.01747065 -0.02178221  0.04579135 -0.05764752\n",
      "   0.02722747  0.08756388 -0.033237   -0.13783892 -0.03892394 -0.05188623\n",
      "   0.03032474 -0.04666438  0.08075232 -0.02495909  0.06831993  0.00120694\n",
      "  -0.00251895 -0.06683    -0.01103844 -0.03728259]]\n",
      "\n",
      "Generando representación para combinación: ['tokenization', 'stop_words', 'lemmatization']\n",
      "Combinacion: ['tokenization', 'stop_words', 'lemmatization'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'stop_words', 'lemmatization'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD\n",
      "Preview: [[ 1.37067817e-01 -3.62226355e-02  3.84878428e-01  1.22599433e-01\n",
      "   4.59779878e-02  2.87206081e-02  4.70533228e-02  6.22594409e-02\n",
      "  -9.39649668e-02  1.38207126e-01 -2.82132618e-02 -6.97097699e-02\n",
      "  -6.37621684e-02  2.68600218e-02 -7.05758352e-02  3.08019259e-02\n",
      "   1.02277470e-01 -1.49949061e-01  1.41929260e-02  2.22275554e-02\n",
      "  -1.39398700e-01  2.22355566e-02 -7.17977307e-02 -4.57674036e-02\n",
      "   7.78933681e-02 -6.32749520e-02  8.94904580e-03  3.64767605e-02\n",
      "   5.29590825e-02 -2.08468268e-02  5.60298757e-02 -1.82802279e-02\n",
      "   2.64578614e-02 -4.79884454e-02 -2.53307464e-02  4.76007638e-02\n",
      "  -2.64346412e-02 -2.08604328e-02 -6.01670905e-03  3.91917014e-02\n",
      "  -6.72063688e-02  1.01864440e-02 -2.32086442e-02  3.56088647e-02\n",
      "   6.24214193e-02  1.48915550e-03 -6.87528819e-03  4.92097603e-03\n",
      "  -6.04262274e-02 -2.82505299e-02 -5.79167604e-02  4.60545004e-02\n",
      "  -1.58712358e-02 -8.24218140e-02 -4.30234055e-02  1.49242025e-03\n",
      "  -5.18085129e-02 -7.07212977e-02  8.32136908e-03  2.78799442e-02\n",
      "  -1.04335485e-02  6.14007740e-02  3.26351208e-02 -3.49088986e-02\n",
      "  -2.07048787e-02 -8.86712922e-02  7.69724781e-03  1.03439606e-02\n",
      "   6.64800859e-02 -1.29684587e-02 -7.20031234e-02  7.28325269e-02\n",
      "   1.64987910e-02  1.29335970e-03 -7.11836283e-02 -6.42341320e-02\n",
      "   2.33547809e-02  2.97055851e-02 -7.03574355e-02  5.15842899e-02\n",
      "  -1.38470387e-02 -1.34911543e-02  1.81809259e-02 -1.46054878e-02\n",
      "   5.66865345e-02  2.88948112e-02  5.89836713e-02 -7.02261319e-02\n",
      "   1.82290853e-02 -1.23365757e-01 -4.17670509e-02 -3.07351975e-02\n",
      "  -2.84045825e-02 -6.74037918e-03 -6.01272121e-02 -5.82332307e-02\n",
      "   2.41870577e-02  1.72056804e-03 -1.79656678e-02  4.40042182e-02]\n",
      " [ 5.38771030e-02 -1.60429776e-02  1.57777402e-02 -9.75153428e-03\n",
      "  -1.74944888e-02  2.85743306e-02 -4.97311137e-02 -1.03190098e-01\n",
      "   2.33365806e-01  1.86844790e-01 -1.22748141e-02 -3.10306305e-02\n",
      "  -2.88161866e-02 -1.90217822e-02  4.73556466e-02 -3.25509958e-02\n",
      "  -7.60253772e-02  4.56854898e-02 -7.10636949e-03  4.18938495e-02\n",
      "   5.57460081e-03 -2.19746092e-02  5.83575126e-03  5.44799387e-02\n",
      "  -1.98838167e-02 -3.94673964e-02  3.26740974e-02  5.19541131e-02\n",
      "  -5.25203454e-02  1.52364647e-03  8.68390585e-02 -1.20856466e-02\n",
      "  -1.16247507e-02  3.90555696e-02 -2.60624273e-02 -8.67666213e-02\n",
      "   6.30766802e-02 -1.00466357e-03  1.50095784e-02  1.67266633e-02\n",
      "  -1.71657819e-02 -4.23468183e-02 -3.84163791e-02  5.71109010e-02\n",
      "   5.10373356e-02 -4.06982863e-02 -3.80568808e-02  9.47443516e-02\n",
      "  -8.99412676e-03 -7.93903977e-02  3.14817028e-02 -4.51677000e-02\n",
      "  -4.24475836e-02  3.70267661e-02  1.33884166e-02 -1.05212077e-02\n",
      "  -4.56054770e-03 -1.21291460e-01 -4.31152572e-02  2.71161952e-02\n",
      "  -5.03406329e-02 -2.75259524e-02 -1.95178227e-02  1.77768954e-02\n",
      "   9.67911080e-02  2.68261272e-02  3.57949164e-03 -3.95281372e-02\n",
      "   2.29757160e-02  1.49464402e-02 -7.53961469e-03  5.27152734e-02\n",
      "   5.71944319e-03  2.72814596e-02 -3.48150685e-02 -8.32056476e-03\n",
      "   1.01217231e-01 -3.32409487e-03  5.18528705e-02  8.20540323e-02\n",
      "  -2.06421529e-02 -1.00438577e-01  1.38207542e-01 -1.22746596e-02\n",
      "   3.78574474e-02 -9.81546901e-02  1.14494100e-02  6.02652436e-02\n",
      "  -4.29142825e-02  1.33298626e-01 -5.50168358e-02 -5.15573736e-02\n",
      "   2.52883709e-02  8.24019262e-02  1.19004062e-02  1.01628870e-01\n",
      "  -6.23831387e-02 -9.20528814e-02 -7.18895992e-02 -2.88583298e-02]\n",
      " [ 2.41635808e-02 -1.55698154e-02  1.01606727e-02 -1.47576233e-03\n",
      "   8.38455846e-03  3.80218728e-03 -4.07948781e-02 -3.16758486e-02\n",
      "   3.02431449e-02 -2.34064610e-02 -7.14332343e-03 -2.45018078e-02\n",
      "   1.69857455e-02  3.09977481e-02 -1.44724793e-02 -4.40103585e-02\n",
      "  -6.32151125e-03  4.75731417e-03  2.36792928e-02 -1.82960246e-02\n",
      "  -1.65702557e-02 -4.22993419e-02  2.30011688e-02 -2.09820933e-02\n",
      "   1.12166164e-02 -6.49011120e-02  3.80222861e-02  3.90688492e-02\n",
      "   5.87055197e-02  4.42354415e-02 -8.33800864e-03  1.25949064e-02\n",
      "   2.37058070e-02 -5.54531515e-02 -2.51215024e-02 -8.15530100e-02\n",
      "   6.04605447e-02  1.13589463e-02 -4.46318366e-02  4.41982457e-02\n",
      "   5.74486632e-04  2.65497713e-02 -2.34769468e-02  5.31620746e-05\n",
      "  -4.57175070e-02  3.14158946e-02  1.00994470e-01  1.00926686e-01\n",
      "   7.87802628e-02 -1.16402554e-01  1.17161289e-01 -3.19080919e-02\n",
      "  -3.84945733e-02 -3.18479229e-02  3.53076761e-02  4.06072805e-02\n",
      "   5.37364002e-02 -9.43508821e-02 -2.26434088e-02 -3.82693067e-02\n",
      "   2.78907132e-02 -9.90940486e-02 -3.52688041e-02  6.20499307e-02\n",
      "   1.02535216e-02  5.76827733e-04 -2.01555071e-01 -6.61478934e-03\n",
      "  -4.02609694e-03 -7.00066357e-02 -8.83676310e-02 -2.15276513e-02\n",
      "   9.39675353e-02 -1.25760334e-02 -7.36326674e-02 -1.02163671e-02\n",
      "   1.80619244e-02 -5.82575223e-03 -9.77829035e-02 -4.83111377e-02\n",
      "   7.28823143e-02  5.43473040e-02  1.09392811e-02 -8.13671947e-02\n",
      "  -2.96192546e-02  7.19374307e-02 -2.18465606e-02  3.44134495e-02\n",
      "   1.46103870e-02  8.07175743e-02  2.23315020e-02 -4.67501515e-04\n",
      "   4.97693420e-02 -1.60729615e-01  1.50851743e-01 -2.06678657e-02\n",
      "  -3.64654465e-02  3.03801578e-02  3.28105817e-02  5.87526728e-02]]\n",
      "\n",
      "Generando representación para combinación: ['tokenization', 'stop_words', 'text_cleaning']\n",
      "Combinacion: ['tokenization', 'stop_words', 'text_cleaning'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'stop_words', 'text_cleaning'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD\n",
      "Preview: [[ 9.40178561e-02  6.91472891e-03  3.13782261e-01  2.34132388e-01\n",
      "   5.04373102e-02 -1.18693384e-02  1.12509938e-04 -8.64022360e-02\n",
      "  -6.34940179e-02 -1.47000753e-01  4.57947750e-03  2.72011327e-02\n",
      "  -5.92359920e-02 -3.66273034e-02 -2.66815417e-02 -3.11706037e-02\n",
      "  -1.00239520e-01 -5.37167592e-02  7.20914792e-02 -8.47201722e-02\n",
      "  -3.83202593e-02 -8.16063430e-03  5.48834776e-03 -1.64452827e-01\n",
      "   7.15221842e-02 -1.10856918e-01  5.02259724e-02  5.20730237e-02\n",
      "  -1.31498799e-03 -3.10705791e-02  1.54513631e-02 -1.06360544e-01\n",
      "  -1.61901704e-02 -6.47870074e-03  2.95542607e-02 -3.03604643e-02\n",
      "   5.71127993e-02 -9.30615455e-02 -3.64093872e-02  1.88631523e-02\n",
      "  -2.09442591e-03  5.33001265e-03  4.22268717e-02  6.45524501e-02\n",
      "  -7.52737356e-02  6.68332534e-02  3.52183702e-02 -5.20407628e-02\n",
      "   3.33432248e-02  2.13530144e-02  8.19921018e-03 -2.25210998e-03\n",
      "   1.56517952e-02 -3.40741502e-03  2.10920531e-02 -2.85085829e-03\n",
      "   1.37928750e-03 -2.91631638e-02  5.67866999e-02  3.66014661e-02\n",
      "   3.84275401e-02 -5.92315947e-02 -3.02917694e-02  9.60283111e-03\n",
      "   2.50710699e-02  6.32585394e-02 -7.34138963e-02  1.93618526e-02\n",
      "  -3.60114337e-02 -3.68897623e-03 -7.07757194e-02 -2.88844861e-02\n",
      "   8.54422472e-03  4.28518555e-02 -5.89445675e-02 -6.42594250e-03\n",
      "  -1.55358752e-02  7.08732040e-02  1.19242895e-01 -2.59500542e-03\n",
      "   3.31445536e-03 -7.48371140e-02 -1.06710957e-02 -4.91637612e-02\n",
      "  -3.53804966e-02 -2.68697753e-02  4.87742783e-02  1.52355145e-02\n",
      "   6.14396671e-02 -5.27526751e-03 -4.31108587e-02  1.04309176e-01\n",
      "   8.80544956e-02 -1.36292640e-02 -9.35456556e-03 -4.18424012e-02\n",
      "   4.70035514e-03  3.93691326e-02  8.10826341e-02  5.17925952e-02]\n",
      " [ 5.60366387e-02 -8.46413165e-04  1.46098812e-02 -4.75324015e-03\n",
      "   3.49035706e-03 -2.69655429e-02 -7.51610697e-02  1.97665699e-01\n",
      "   1.62488819e-01 -1.71212999e-01  5.44131717e-02 -7.90949568e-02\n",
      "   7.49293878e-04  6.02764960e-02 -3.27420673e-02  7.41773191e-03\n",
      "   1.75312691e-02  4.03443953e-02 -1.37034863e-02  6.34893174e-02\n",
      "   4.64907404e-02 -2.35082998e-03  6.99314549e-02  4.03361416e-02\n",
      "  -7.68140685e-03 -4.52279799e-02 -2.34382692e-02 -3.69452864e-02\n",
      "  -4.97389050e-02  3.69317240e-02  3.59894547e-02 -5.14940423e-03\n",
      "  -7.46139312e-02 -1.95555553e-02 -6.89482373e-03 -2.05783310e-02\n",
      "   5.00810241e-02  7.96774069e-02  1.76714374e-02 -4.83896863e-03\n",
      "   3.53321053e-02 -4.89020392e-02 -2.51067664e-02 -3.15337685e-03\n",
      "  -4.66016088e-02  2.18629054e-02 -6.94429667e-02  3.39070694e-02\n",
      "   4.53628439e-03  1.71750220e-02  2.43831277e-02  2.83345194e-03\n",
      "   7.13627330e-02  6.20582005e-03 -1.10269282e-01 -1.69504163e-03\n",
      "  -1.01918510e-01 -1.30621934e-02  7.31544466e-02 -1.07430235e-02\n",
      "  -8.80444693e-03 -3.00269277e-02  2.47153011e-02 -8.53126509e-03\n",
      "   3.45404211e-02 -3.26927338e-02  3.40369492e-02 -2.71885810e-02\n",
      "  -1.51092983e-02  7.44962318e-04 -1.32779465e-01  2.15091375e-02\n",
      "   1.13411858e-01  7.59429527e-02  1.43548814e-02  1.49628466e-01\n",
      "   3.20245955e-02  1.65815147e-02  9.52152997e-02  3.95410955e-02\n",
      "   6.27189764e-02 -4.06793278e-03 -9.86075440e-02  1.83466522e-02\n",
      "   7.40536299e-02  5.98260389e-02  2.33790927e-02  1.61975193e-02\n",
      "   9.01494205e-02  1.05311584e-02 -1.22297443e-01 -3.95627793e-02\n",
      "  -1.50612209e-01  9.15161523e-02 -4.84340989e-02 -4.23239866e-03\n",
      "  -1.33497105e-02  6.55260752e-02  1.13213623e-02 -9.96095624e-03]\n",
      " [ 1.98096836e-02  1.73005448e-03  7.62412065e-03  5.81591610e-04\n",
      "   9.65946366e-03  1.25814487e-02  1.70212204e-03  2.14781925e-02\n",
      "   3.32322591e-02  7.13823340e-03 -1.66184071e-02  2.76840666e-02\n",
      "   4.70462333e-04 -3.94094063e-03  1.85310799e-02 -1.29224195e-02\n",
      "   7.25017711e-03 -3.38548732e-03  5.55831672e-03  1.29220340e-02\n",
      "  -2.24794506e-02 -9.82618346e-03  9.65519758e-03  1.96553638e-02\n",
      "   8.53331722e-02 -4.31106254e-02  2.73247568e-02  2.16167883e-02\n",
      "  -3.12649312e-02  5.40907026e-02 -7.08002645e-02 -2.00703053e-02\n",
      "   4.36634646e-02 -6.27865795e-02 -1.52074609e-02  1.86725509e-03\n",
      "   4.30103689e-02  6.17730558e-02 -1.88516690e-02 -2.45878481e-02\n",
      "   1.78872495e-02 -2.30537708e-03 -7.57706253e-02 -8.50641800e-02\n",
      "  -8.55158408e-02 -8.02840521e-04  6.04623563e-02 -2.79826304e-02\n",
      "  -1.88679562e-02 -6.62491330e-02  9.70175345e-02  1.68032372e-02\n",
      "   3.40567273e-02  1.29821363e-01 -1.42542400e-01  8.62577633e-02\n",
      "  -1.53701543e-01  1.11627592e-03  7.24750979e-02  1.02738361e-02\n",
      "   8.08757868e-02 -1.29805131e-04  2.42262086e-02 -9.25651979e-02\n",
      "  -2.67544129e-02 -6.85311576e-02  7.45823531e-02  2.76633233e-02\n",
      "  -1.73843839e-02 -5.26164460e-02  2.28812852e-02  2.37159370e-02\n",
      "   1.21346856e-01 -9.32826484e-02 -3.23002817e-02 -1.05932217e-01\n",
      "  -1.33628614e-02  1.15356350e-01 -2.46347521e-02  4.90258368e-02\n",
      "   2.35218668e-02  3.65348525e-02  3.97744766e-02 -4.89427733e-03\n",
      "   9.42169460e-02 -4.97205452e-02  6.99604085e-02  3.09838080e-02\n",
      "   1.22397580e-02  1.09968913e-01 -8.47922451e-02  2.91179116e-02\n",
      "  -4.45450636e-02  1.75397391e-02 -4.58981826e-02  5.06234683e-02\n",
      "  -1.50761206e-01  1.05329137e-01  6.25796878e-02 -6.23281186e-02]]\n",
      "\n",
      "Generando representación para combinación: ['tokenization', 'lemmatization', 'text_cleaning']\n",
      "Combinacion: ['tokenization', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "Preview: [[ 2.59794641e-01 -2.11376580e-01 -2.19710424e-01  2.79133190e-02\n",
      "   9.18453082e-02  3.21351016e-02  1.33412688e-02  2.21041902e-01\n",
      "  -1.19576053e-01 -4.96631537e-02  5.46095002e-02 -5.67955736e-02\n",
      "   1.68602213e-01 -3.54232939e-02  5.60714973e-02 -3.05180973e-02\n",
      "   1.33999663e-01  1.75038580e-02  3.54908696e-02 -2.73766492e-02\n",
      "  -8.94139334e-03  5.16455129e-02 -8.33910897e-02  1.11107678e-01\n",
      "   9.29550504e-02 -1.10371199e-01 -4.21233353e-02 -1.01847011e-01\n",
      "  -2.09886333e-02  3.14440560e-02  8.37118138e-02  1.04792619e-01\n",
      "   6.63101418e-02  3.66606191e-02 -7.52041948e-02 -1.01744157e-01\n",
      "  -4.56230307e-02  1.10120366e-02  2.06669280e-03  3.94302677e-02\n",
      "  -6.89602010e-05  1.08022710e-01 -9.86060512e-02  2.38628206e-02\n",
      "   2.21742462e-02  7.38275582e-02 -6.20844684e-02 -1.91529627e-02\n",
      "  -4.16148408e-02  2.99310499e-02 -7.98222426e-03  3.02843983e-02\n",
      "   5.51817380e-02  7.13885645e-02 -1.75399701e-02  4.06174103e-02\n",
      "   6.41341773e-03  6.81868811e-03  6.94773373e-02  4.66817519e-02\n",
      "  -4.11461875e-02 -1.00447812e-02 -4.51981236e-02 -4.36856218e-02\n",
      "  -3.89190153e-02 -1.02988311e-01 -8.46394117e-02 -2.06419447e-02\n",
      "   2.67713411e-02  3.02906375e-02  5.03071125e-02 -5.83035321e-02\n",
      "  -1.54139738e-02  2.07454622e-02  4.23673375e-02  1.00798471e-01\n",
      "   4.78846159e-02  7.72683000e-02  1.09928060e-02 -2.14869164e-02\n",
      "   8.67121006e-02 -4.46200515e-02  6.87205571e-02 -2.28707240e-02\n",
      "  -5.03739871e-02  1.17704387e-02 -7.44554584e-04 -1.12872211e-02\n",
      "  -6.85451361e-02 -1.53568461e-02  3.62761380e-02 -1.19513209e-02\n",
      "   3.31849243e-02  1.40187614e-02  3.54823103e-02  1.21529984e-02\n",
      "  -3.69843680e-02  3.04450430e-02  2.91376071e-02  2.89950030e-02]\n",
      " [ 4.05688953e-01 -4.43709701e-03  8.11953842e-03 -4.98288058e-02\n",
      "  -1.05212313e-01  2.16963712e-02 -1.21787269e-02 -1.38806929e-01\n",
      "  -8.71174952e-02  5.19667460e-02  3.85716524e-02  5.69632365e-02\n",
      "   1.10168237e-02  5.45128093e-02  1.62679675e-02  9.21003376e-02\n",
      "   5.02394888e-02 -2.86491149e-02  2.97845847e-02 -8.30542207e-03\n",
      "   7.23096565e-03 -1.37089337e-02 -2.43550271e-02  1.92723834e-02\n",
      "   4.45419472e-02 -1.13552670e-02  2.18368619e-02 -6.89253855e-02\n",
      "  -2.85154837e-02 -6.05041516e-02  2.42069101e-02  5.65287436e-03\n",
      "  -3.80241176e-02  3.73518554e-02  4.37296667e-02  7.36811764e-02\n",
      "  -6.92671712e-02 -1.00591512e-01 -2.80524394e-02  2.93625499e-02\n",
      "   3.59147010e-02 -4.51516146e-03 -1.36911256e-02  6.67170525e-02\n",
      "   4.77300803e-02 -1.58196840e-02 -2.39486559e-02  4.00605939e-02\n",
      "  -6.22542899e-03  4.18113538e-02  3.33781733e-02 -3.26934463e-02\n",
      "  -4.50715132e-02  9.75535158e-02 -3.26880717e-02  3.61895273e-02\n",
      "   2.14161520e-02  1.36364062e-01  1.44927892e-02  4.00171408e-02\n",
      "  -4.85886846e-02 -4.73068639e-02 -2.32255258e-02 -2.57384776e-02\n",
      "   9.21765896e-03  5.35414564e-02  3.82398418e-02 -7.26484600e-03\n",
      "  -1.71191267e-02 -6.23245527e-02  6.04265589e-02  6.48991249e-03\n",
      "   2.25404917e-02 -2.65356634e-02  6.82768817e-02 -3.08645044e-02\n",
      "  -6.14840024e-02 -6.21017688e-02  4.72979132e-02  4.41837572e-03\n",
      "  -1.16650723e-01 -8.20229925e-02 -3.59326192e-02  4.07679696e-02\n",
      "  -2.67844765e-02 -6.15821845e-02  7.49963206e-03  6.57876800e-02\n",
      "  -1.36236719e-02 -1.43993814e-02 -1.87854796e-03  3.29865446e-02\n",
      "  -6.24414350e-02 -1.66144776e-02 -7.35861152e-02 -2.99670601e-02\n",
      "  -1.12623517e-02 -3.56625888e-02  1.18081128e-02  3.64221473e-02]\n",
      " [ 2.29961134e-01 -2.58333637e-02 -6.95179202e-03 -5.19366887e-02\n",
      "  -4.60809037e-02  1.00622488e-01 -9.02765726e-03 -5.77890772e-02\n",
      "  -1.05084543e-02 -4.07381381e-02  4.82259470e-02  3.17224140e-02\n",
      "   2.96417450e-02 -7.99705050e-02  3.37343993e-03  1.02513992e-02\n",
      "  -1.06742183e-01  1.00672842e-01  2.08055933e-02  3.62191543e-02\n",
      "   7.90079537e-02 -3.58891037e-02 -2.32469012e-02  1.40605419e-01\n",
      "   2.96464935e-02  8.90851605e-02 -4.28516604e-02 -1.64615151e-02\n",
      "  -7.43495808e-04 -1.48084943e-02  3.92565091e-02  6.19744893e-03\n",
      "  -7.28202306e-02  1.06981805e-01 -4.15480998e-02  6.50982607e-02\n",
      "   5.22290533e-02 -1.60733758e-01  9.27579201e-03  3.53482753e-02\n",
      "   3.65720127e-03 -3.47192145e-02 -1.72290348e-02  4.95321165e-03\n",
      "  -2.30021462e-02 -8.95427587e-02  5.93696972e-02  1.15178670e-01\n",
      "   2.10446446e-02  4.79798787e-02 -8.67600738e-02  2.14286083e-02\n",
      "  -1.20950001e-01  4.44943024e-02  4.65007210e-02 -1.65167036e-02\n",
      "   1.02232500e-01  1.52774418e-01 -3.94939253e-02 -2.60624873e-02\n",
      "   7.63984728e-02  3.23754428e-02 -1.05657525e-01  1.58540185e-02\n",
      "  -3.39199043e-02 -3.30301895e-02  2.77227348e-02  3.86030820e-02\n",
      "  -8.65787847e-02 -1.06729160e-01 -7.02789853e-02 -5.63970232e-02\n",
      "  -8.02529480e-02 -5.83593701e-02 -1.29241145e-02  7.27176161e-02\n",
      "  -4.19093586e-03  7.72111821e-02  7.20127112e-02  4.62098919e-02\n",
      "   1.75301090e-02  4.36929530e-03  7.57035626e-02  4.28957598e-02\n",
      "   8.68001456e-03  2.77329716e-03  3.03790113e-02 -6.53174318e-02\n",
      "   2.78384278e-02  4.76120469e-02 -6.40707634e-02  4.69579682e-02\n",
      "   8.52332058e-02 -4.62796848e-02  4.62394744e-02  6.77700230e-03\n",
      "   8.77964034e-02 -2.98640928e-02  1.81457916e-02  4.09278775e-02]]\n",
      "\n",
      "Generando representación para combinación: ['stop_words', 'lemmatization', 'text_cleaning']\n",
      "Combinacion: ['stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "Preview: [[ 0.21326235  0.20261865 -0.21368052  0.07862102  0.02849024 -0.0027561\n",
      "  -0.12475158  0.05616941  0.04286994  0.13819202  0.1282607   0.12526847\n",
      "   0.06763953 -0.08520245 -0.03498505 -0.00717128 -0.03151448  0.15472615\n",
      "  -0.03306683 -0.06670136  0.05826755 -0.06428152 -0.02013905  0.06131957\n",
      "  -0.04414126 -0.06528492  0.05562204 -0.12638188  0.02682464  0.00214137\n",
      "  -0.02860331 -0.13305659  0.05234998 -0.01864221  0.06859182  0.05064143\n",
      "  -0.06563622 -0.06429671  0.09166027 -0.03709604  0.05860521  0.03872112\n",
      "  -0.02426133  0.00480755 -0.01663462  0.07357481  0.02758906 -0.02117219\n",
      "   0.03557978  0.08404286  0.03265008  0.04148683  0.0337202   0.00335258\n",
      "  -0.05182902 -0.10265444  0.04069684  0.02738081  0.06582708 -0.03946797\n",
      "   0.10276727  0.02476295  0.01781439 -0.02378977  0.0026437   0.06909638\n",
      "  -0.04611049  0.00944674  0.0383978   0.03025846  0.05681691  0.12489988\n",
      "   0.04451315 -0.00561619 -0.12169536 -0.08819937  0.0391935   0.09211017\n",
      "  -0.0176859  -0.06548463 -0.03312144 -0.0427343   0.03524257  0.00490245\n",
      "   0.00936553  0.04250972 -0.07291881  0.05617808 -0.04762524  0.00450871\n",
      "  -0.06080556 -0.01575135 -0.13275514  0.01088312  0.00709654  0.00736595\n",
      "   0.01498304 -0.02475136 -0.00254001  0.02692702]\n",
      " [ 0.34620765 -0.02939947  0.0486991  -0.02823063 -0.10726361 -0.05145641\n",
      "   0.14891217  0.04116453  0.05403097  0.10705797  0.02665056  0.1416396\n",
      "  -0.02993843  0.15862342  0.00705638 -0.02686639  0.00418529  0.05368456\n",
      "   0.0232263   0.05338121 -0.05168162 -0.05255386 -0.06440988  0.04640635\n",
      "   0.07965456  0.0745653  -0.03045817  0.03901522 -0.00522208 -0.03943786\n",
      "  -0.07464357  0.00052379 -0.05137775 -0.00061662  0.05656252  0.06800433\n",
      "   0.04109303 -0.04586214  0.03832147  0.03039682 -0.03835397 -0.01085573\n",
      "  -0.01086836  0.01215771  0.04525169 -0.02688099  0.03665852 -0.03176941\n",
      "  -0.05309443  0.01628397 -0.00969464  0.00931415  0.03381175 -0.0039615\n",
      "  -0.07279305  0.04909938 -0.04870825 -0.02035522 -0.03975826 -0.0046165\n",
      "  -0.09452628 -0.03436861 -0.01483785  0.0124673   0.04346539  0.0627957\n",
      "  -0.01892511 -0.09065516  0.03564098 -0.00471679 -0.01055832  0.06489652\n",
      "  -0.04137281 -0.00534987 -0.03668121  0.04227054 -0.0411679   0.04987877\n",
      "   0.01563672 -0.01818257 -0.0182099   0.03443216 -0.01246024 -0.04236024\n",
      "  -0.0284305  -0.04058596  0.03102707  0.01550487  0.10953944 -0.04315613\n",
      "   0.059108    0.01765217  0.04875019 -0.02304823  0.02438069  0.01622641\n",
      "   0.04801353  0.02355955 -0.05851124 -0.01156922]\n",
      " [ 0.22122047  0.01081009 -0.01733706 -0.00246773 -0.06654218  0.0205557\n",
      "   0.05082345  0.07746329  0.00254584  0.00683978 -0.08785785  0.04211503\n",
      "   0.02941155 -0.03700957 -0.02253655  0.09197385  0.00145455 -0.08934682\n",
      "  -0.01990482  0.1507421  -0.05146896 -0.04536881 -0.07174796  0.04044132\n",
      "   0.06460146 -0.13612182 -0.03377108  0.01616387  0.01131259  0.06406631\n",
      "  -0.00772786  0.03655459  0.04350979 -0.05020704 -0.05095412  0.07473247\n",
      "   0.07554568 -0.04376315  0.10961037  0.01899534  0.00086957  0.03139186\n",
      "  -0.03876705  0.02322245  0.09733443  0.09848437  0.02050478  0.01876562\n",
      "   0.04056484  0.02299239  0.10639096  0.05224397 -0.00844136 -0.01009173\n",
      "   0.10189643  0.11851419 -0.0117931  -0.04088373 -0.07632745 -0.07083904\n",
      "   0.02548984 -0.136813   -0.00743807  0.00703141  0.07266774  0.00972214\n",
      "  -0.06601731 -0.04783389 -0.09153764  0.01551669 -0.02029947  0.08694364\n",
      "  -0.02089546 -0.02102123 -0.01965621 -0.07314735  0.01748878 -0.02742541\n",
      "  -0.01409314  0.08673558  0.01747065 -0.02178221  0.04579135 -0.05764752\n",
      "   0.02722747  0.08756388 -0.033237   -0.13783892 -0.03892394 -0.05188623\n",
      "   0.03032474 -0.04666438  0.08075232 -0.02495909  0.06831993  0.00120694\n",
      "  -0.00251895 -0.06683    -0.01103844 -0.03728259]]\n",
      "\n",
      "Generando representación para combinación: ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning']\n",
      "Combinacion: ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "Preview: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "Combinacion: ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "Preview: [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Combinacion: ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "Preview: [[ 1.38529276e-01 -2.04278856e-02  3.74186731e-01  1.36236835e-01\n",
      "   6.93880187e-02  4.03030375e-02 -4.20306873e-02  3.10408192e-02\n",
      "  -1.62477392e-01 -2.16447903e-02 -1.39119047e-01  6.83305433e-02\n",
      "  -1.00507278e-02  1.08671578e-02 -7.04821039e-02 -8.38204929e-03\n",
      "   2.32264546e-02 -1.14805480e-01 -1.77055236e-01 -2.18578286e-02\n",
      "  -7.82742364e-02 -4.33125316e-02 -9.95267521e-02 -4.86887819e-02\n",
      "   1.26503970e-02  6.92230985e-02  5.65175634e-02 -2.94146594e-02\n",
      "   1.18746079e-02 -1.37723050e-02  1.24096687e-02 -5.73603750e-02\n",
      "  -2.49008781e-02  2.73733623e-02  1.16129913e-02  1.44334522e-02\n",
      "   7.00916515e-02 -2.76907170e-02 -1.90622147e-02  2.13374067e-02\n",
      "  -6.11696166e-02 -2.19228811e-02 -1.17629424e-02  5.35146598e-02\n",
      "  -3.33405905e-02 -2.67828532e-02  7.85881846e-03  1.69356603e-02\n",
      "  -1.55936562e-02 -2.86794371e-02 -5.57280511e-02 -2.27754157e-02\n",
      "   3.64226075e-02 -4.72251481e-02  9.14543255e-02 -5.29203820e-02\n",
      "  -2.99689818e-02  2.67418056e-02 -7.37068004e-03 -4.51907380e-02\n",
      "   1.55752737e-02 -4.20137837e-02  4.58389096e-02 -2.02142248e-03\n",
      "   9.43536093e-02 -9.17269778e-02  2.80056436e-02  2.20344477e-03\n",
      "   1.47733325e-03 -4.64982504e-02  8.68777272e-02 -2.51275222e-04\n",
      "  -5.07539301e-02  1.36046540e-02  2.65777263e-04 -2.19504820e-02\n",
      "   7.72023230e-02  4.70587191e-02 -7.19082154e-02 -3.37069916e-02\n",
      "  -3.99377831e-02  8.51166134e-02 -7.68396815e-02 -7.75125046e-02\n",
      "  -1.03329550e-03 -8.41950669e-02 -1.09707330e-02  7.55064912e-02\n",
      "   2.17001373e-02 -4.59132895e-02 -5.19413047e-03  2.26914668e-02\n",
      "   4.60730356e-03 -6.45208847e-02 -5.22890304e-03 -4.11132092e-02\n",
      "  -5.29306725e-02 -4.76079115e-03  4.19144538e-02 -7.18424616e-02]\n",
      " [ 5.15427812e-02 -8.98076103e-03  1.18079424e-02 -1.78614784e-03\n",
      "  -1.52439706e-02  2.21200116e-02  5.66031850e-02  1.51961831e-02\n",
      "   8.65203367e-02 -1.28344358e-01 -3.54743870e-02  1.25177613e-01\n",
      "  -3.58492581e-02 -4.35096615e-02  5.55554608e-02  5.29001262e-02\n",
      "  -5.81422125e-02  2.45614484e-02  6.73333914e-02  5.98242801e-02\n",
      "   2.08864810e-03 -4.80574315e-02  3.20252605e-02  3.09228566e-02\n",
      "   7.78830712e-02  3.94252173e-02 -8.39109632e-03  1.98390989e-02\n",
      "   6.11340591e-02 -5.96250290e-03  7.55848883e-02 -3.83501188e-02\n",
      "   1.91842392e-02 -6.12762364e-02 -4.13704076e-02 -2.20928813e-02\n",
      "  -6.96610444e-02 -5.37479647e-02 -5.57683730e-02  3.51144806e-02\n",
      "  -1.88918715e-02  4.27449567e-03 -8.12633833e-02  6.54311337e-02\n",
      "   5.52412695e-02  3.59600006e-02  3.57082306e-02 -7.77128539e-02\n",
      "  -9.46526662e-03 -5.38858720e-02 -8.69110657e-02 -3.34825103e-02\n",
      "   3.68923059e-02  4.63066418e-02 -1.55965444e-03  3.45361035e-02\n",
      "  -6.99112298e-02  7.95803667e-02  1.52228220e-02 -4.21422042e-02\n",
      "  -2.67217739e-02  2.27703357e-02  1.17362632e-01  6.27210489e-02\n",
      "  -1.44672374e-01  7.73239398e-02 -1.32072787e-02 -1.51187537e-02\n",
      "  -2.11238098e-02 -2.69584164e-02  5.83914469e-02  3.46184410e-02\n",
      "  -3.06105101e-02  9.07023857e-03 -1.22652031e-02 -7.75309325e-02\n",
      "   2.98695054e-02 -1.00368869e-01  8.45835409e-02 -1.89455183e-01\n",
      "  -9.50711896e-02  3.52898510e-02 -7.46990773e-03 -4.83443203e-03\n",
      "   9.36929930e-02  1.33546687e-01  4.30823987e-02  9.84724031e-03\n",
      "  -1.15985292e-03 -6.64286847e-02  5.67348031e-02 -3.89535448e-02\n",
      "   7.92287895e-02  1.05905986e-01  1.44259827e-02  7.13751056e-02\n",
      "   6.15590617e-02 -1.36618045e-01 -2.95209775e-02  1.44145479e-01]\n",
      " [ 2.45605137e-02 -1.41460768e-02  1.16798965e-02  9.34562451e-04\n",
      "   6.68093240e-03  3.51738323e-03  4.47808170e-02 -2.25688907e-02\n",
      "   5.24700802e-02 -8.18484125e-03 -1.56272289e-03  1.97740685e-02\n",
      "   2.48579226e-02  1.46519380e-02  7.63381130e-03  3.60598380e-02\n",
      "  -3.16951036e-02  1.67112280e-02 -1.23166754e-02 -2.72916411e-02\n",
      "   1.10667111e-03 -1.67867250e-02  2.83184236e-02 -4.33202604e-02\n",
      "   5.40274747e-02  2.99076402e-03  5.23450530e-02 -7.36398287e-02\n",
      "  -6.94828093e-03  1.01534195e-02  2.39347664e-02 -2.98147153e-02\n",
      "  -2.83063433e-03  9.29586663e-03  1.46482731e-02  1.55489719e-02\n",
      "  -7.33364717e-02 -6.00696383e-02 -7.54081584e-02  2.45728684e-02\n",
      "   7.08760323e-04 -2.47925414e-03  3.80867338e-02  1.52005836e-03\n",
      "   4.13382408e-02  1.04156156e-01  1.24925133e-01 -1.19891785e-01\n",
      "   2.47646169e-02  3.82769194e-02 -1.87262201e-03  7.03929208e-02\n",
      "   5.04121288e-03 -1.04167994e-02  2.71362721e-02  3.21425904e-02\n",
      "   3.11031563e-02  1.38065000e-01 -5.04206030e-02  3.73782742e-02\n",
      "  -2.01702200e-02  4.85714465e-02 -7.21543657e-02 -1.10745286e-02\n",
      "  -1.20596968e-01 -1.10812224e-01 -5.72668890e-02 -7.46795285e-02\n",
      "  -1.72514843e-01  5.08963113e-03 -4.25060155e-02  4.63516665e-02\n",
      "  -3.98356890e-03  3.42361156e-02  4.89373652e-02 -4.99476436e-02\n",
      "   1.06936523e-01  1.39614080e-01 -2.66599276e-02 -3.49860934e-03\n",
      "   3.24626945e-02  2.53231618e-02  3.48386292e-02  7.70311001e-02\n",
      "  -7.64371988e-02  1.91281746e-02 -2.25886752e-02 -9.92580549e-02\n",
      "  -4.66210468e-02  1.07801529e-01 -5.70123745e-03  3.23067619e-02\n",
      "  -8.49150455e-02  5.06657978e-02 -5.30988221e-02 -4.48624167e-02\n",
      "  -9.53641738e-02 -8.02686017e-02 -6.90187148e-02  4.23020786e-02]]\n"
     ]
    }
   ],
   "source": [
    "representaciones = {}\n",
    "\n",
    "for combo in combos:\n",
    "    print(f'\\nGenerando representación para combinación: {combo}')\n",
    "\n",
    "    X_test_norm = X_test.apply(lambda x: NormalizarTexto(x, aplicar=combo))\n",
    "    X_train_norm = X_train.apply(lambda x: NormalizarTexto(x, aplicar=combo))\n",
    "\n",
    "    #Binarizada\n",
    "    vectorizer_binary = CountVectorizer(binary=True)\n",
    "    X_train_binary = vectorizer_binary.fit_transform(X_train_norm)\n",
    "    X_test_binary = vectorizer_binary.transform(X_test_norm)\n",
    "    representaciones[f'{combo} + Binarized'] = (X_train_binary, X_test_binary)\n",
    "    print(f'Combinacion: {combo} + Binarized\\nPreview:',X_train_binary[:5].toarray())\n",
    "\n",
    "    #Frecuencia\n",
    "    vectorizer_freq = CountVectorizer()\n",
    "    X_train_freq = vectorizer_freq.fit_transform(X_train_norm)\n",
    "    X_test_freq = vectorizer_freq.transform(X_test_norm)\n",
    "    representaciones[f'{combo} + Frequency'] = (X_train_freq, X_test_freq)\n",
    "    print(f'Combinacion: {combo} + Frequency\\nPreview:',X_train_freq[:5].toarray())\n",
    "\n",
    "    #TF-IDF\n",
    "    vectorizer_tfidf = TfidfVectorizer()\n",
    "    X_train_tfidf = vectorizer_tfidf.fit_transform(X_train_norm)\n",
    "    X_test_tfidf = vectorizer_tfidf.transform(X_test_norm)\n",
    "    representaciones[f'{combo} + TF-IDF'] = (X_train_tfidf, X_test_tfidf)\n",
    "    print(f'Combinacion: {combo} + TF-IDF\\nPreview:',X_train_tfidf[:5].toarray())\n",
    "\n",
    "    #TF-IDF + SVD\n",
    "    svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "    X_train_svd = svd.fit_transform(X_train_tfidf)\n",
    "    X_test_svd = svd.transform(X_test_tfidf)\n",
    "    representaciones[f'{combo} + TF-IDF + SVD'] = (X_train_svd, X_test_svd)\n",
    "    print(f'Combinacion: {combo} + TF-IDF + SVD\\nPreview:',X_train_svd[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar y evaluar clasificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainandEvaluate(classifier,X_train,X_test,y_train,y_test):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    return f1, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers ={\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Naive Bayes': MultinomialNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'SVM': SVC(),\n",
    "    'Neural Network': MLPClassifier(hidden_layer_sizes=(200,100)),\n",
    "    'Gradient Boosting': GradientBoostingClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization'] + Binarized\n",
      "F1 Score para Logistic Regression con ['tokenization'] + Binarized: 0.596\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.71      0.81      0.75        36\n",
      "  Tecnología       0.64      0.64      0.64        25\n",
      "\n",
      "    accuracy                           0.72        80\n",
      "   macro avg       0.65      0.57      0.60        80\n",
      "weighted avg       0.73      0.72      0.72        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization'] + Frequency\n",
      "F1 Score para Logistic Regression con ['tokenization'] + Frequency: 0.580\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.60      0.43      0.50         7\n",
      "    Deportes       0.91      0.91      0.91        11\n",
      "    Economía       0.74      0.81      0.77        36\n",
      "  Tecnología       0.72      0.72      0.72        25\n",
      "\n",
      "    accuracy                           0.75        80\n",
      "   macro avg       0.59      0.57      0.58        80\n",
      "weighted avg       0.74      0.75      0.74        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['tokenization'] + TF-IDF: 0.298\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.27      0.43        11\n",
      "    Economía       0.51      1.00      0.67        36\n",
      "  Tecnología       1.00      0.24      0.39        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.50      0.30      0.30        80\n",
      "weighted avg       0.68      0.56      0.48        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['tokenization'] + TF-IDF + SVD: 0.308\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.51      1.00      0.67        36\n",
      "  Tecnología       1.00      0.20      0.33        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.50      0.31      0.31        80\n",
      "weighted avg       0.68      0.56      0.48        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words'] + Binarized\n",
      "F1 Score para Logistic Regression con ['stop_words'] + Binarized: 0.596\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.71      0.81      0.75        36\n",
      "  Tecnología       0.64      0.64      0.64        25\n",
      "\n",
      "    accuracy                           0.72        80\n",
      "   macro avg       0.65      0.57      0.60        80\n",
      "weighted avg       0.73      0.72      0.72        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words'] + Frequency\n",
      "F1 Score para Logistic Regression con ['stop_words'] + Frequency: 0.580\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.60      0.43      0.50         7\n",
      "    Deportes       0.91      0.91      0.91        11\n",
      "    Economía       0.74      0.81      0.77        36\n",
      "  Tecnología       0.72      0.72      0.72        25\n",
      "\n",
      "    accuracy                           0.75        80\n",
      "   macro avg       0.59      0.57      0.58        80\n",
      "weighted avg       0.74      0.75      0.74        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['stop_words'] + TF-IDF: 0.298\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.27      0.43        11\n",
      "    Economía       0.51      1.00      0.67        36\n",
      "  Tecnología       1.00      0.24      0.39        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.50      0.30      0.30        80\n",
      "weighted avg       0.68      0.56      0.48        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['stop_words'] + TF-IDF + SVD: 0.308\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.51      1.00      0.67        36\n",
      "  Tecnología       1.00      0.20      0.33        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.50      0.31      0.31        80\n",
      "weighted avg       0.68      0.56      0.48        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['lemmatization'] + Binarized\n",
      "F1 Score para Logistic Regression con ['lemmatization'] + Binarized: 0.596\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.71      0.81      0.75        36\n",
      "  Tecnología       0.64      0.64      0.64        25\n",
      "\n",
      "    accuracy                           0.72        80\n",
      "   macro avg       0.65      0.57      0.60        80\n",
      "weighted avg       0.73      0.72      0.72        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['lemmatization'] + Frequency\n",
      "F1 Score para Logistic Regression con ['lemmatization'] + Frequency: 0.580\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.60      0.43      0.50         7\n",
      "    Deportes       0.91      0.91      0.91        11\n",
      "    Economía       0.74      0.81      0.77        36\n",
      "  Tecnología       0.72      0.72      0.72        25\n",
      "\n",
      "    accuracy                           0.75        80\n",
      "   macro avg       0.59      0.57      0.58        80\n",
      "weighted avg       0.74      0.75      0.74        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['lemmatization'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['lemmatization'] + TF-IDF: 0.298\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.27      0.43        11\n",
      "    Economía       0.51      1.00      0.67        36\n",
      "  Tecnología       1.00      0.24      0.39        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.50      0.30      0.30        80\n",
      "weighted avg       0.68      0.56      0.48        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['lemmatization'] + TF-IDF + SVD: 0.308\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.51      1.00      0.67        36\n",
      "  Tecnología       1.00      0.20      0.33        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.50      0.31      0.31        80\n",
      "weighted avg       0.68      0.56      0.48        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['text_cleaning'] + Binarized\n",
      "F1 Score para Logistic Regression con ['text_cleaning'] + Binarized: 0.596\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.71      0.81      0.75        36\n",
      "  Tecnología       0.64      0.64      0.64        25\n",
      "\n",
      "    accuracy                           0.72        80\n",
      "   macro avg       0.65      0.57      0.60        80\n",
      "weighted avg       0.73      0.72      0.72        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['text_cleaning'] + Frequency\n",
      "F1 Score para Logistic Regression con ['text_cleaning'] + Frequency: 0.580\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.60      0.43      0.50         7\n",
      "    Deportes       0.91      0.91      0.91        11\n",
      "    Economía       0.74      0.81      0.77        36\n",
      "  Tecnología       0.72      0.72      0.72        25\n",
      "\n",
      "    accuracy                           0.75        80\n",
      "   macro avg       0.59      0.57      0.58        80\n",
      "weighted avg       0.74      0.75      0.74        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['text_cleaning'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['text_cleaning'] + TF-IDF: 0.298\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.27      0.43        11\n",
      "    Economía       0.51      1.00      0.67        36\n",
      "  Tecnología       1.00      0.24      0.39        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.50      0.30      0.30        80\n",
      "weighted avg       0.68      0.56      0.48        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['text_cleaning'] + TF-IDF + SVD: 0.308\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.51      1.00      0.67        36\n",
      "  Tecnología       1.00      0.20      0.33        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.50      0.31      0.31        80\n",
      "weighted avg       0.68      0.56      0.48        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words'] + Binarized\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words'] + Binarized: 0.569\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       1.00      0.73      0.84        11\n",
      "    Economía       0.66      0.81      0.72        36\n",
      "  Tecnología       0.68      0.68      0.68        25\n",
      "\n",
      "    accuracy                           0.71        80\n",
      "   macro avg       0.67      0.53      0.57        80\n",
      "weighted avg       0.73      0.71      0.71        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words'] + Frequency\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words'] + Frequency: 0.524\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       1.00      0.64      0.78        11\n",
      "    Economía       0.62      0.78      0.69        36\n",
      "  Tecnología       0.69      0.72      0.71        25\n",
      "\n",
      "    accuracy                           0.69        80\n",
      "   macro avg       0.66      0.48      0.52        80\n",
      "weighted avg       0.72      0.69      0.68        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words'] + TF-IDF: 0.299\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.49      0.94      0.65        36\n",
      "  Tecnología       0.71      0.20      0.31        25\n",
      "\n",
      "    accuracy                           0.54        80\n",
      "   macro avg       0.44      0.30      0.30        80\n",
      "weighted avg       0.58      0.54      0.46        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words'] + TF-IDF + SVD: 0.295\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.50      1.00      0.67        36\n",
      "  Tecnología       1.00      0.16      0.28        25\n",
      "\n",
      "    accuracy                           0.55        80\n",
      "   macro avg       0.50      0.30      0.30        80\n",
      "weighted avg       0.68      0.55      0.46        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'lemmatization'] + Binarized\n",
      "F1 Score para Logistic Regression con ['tokenization', 'lemmatization'] + Binarized: 0.610\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.92      1.00      0.96        11\n",
      "    Economía       0.72      0.72      0.72        36\n",
      "  Tecnología       0.61      0.68      0.64        25\n",
      "\n",
      "    accuracy                           0.72        80\n",
      "   macro avg       0.65      0.59      0.61        80\n",
      "weighted avg       0.73      0.72      0.72        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'lemmatization'] + Frequency\n",
      "F1 Score para Logistic Regression con ['tokenization', 'lemmatization'] + Frequency: 0.588\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.75      0.43      0.55         7\n",
      "    Deportes       1.00      0.91      0.95        11\n",
      "    Economía       0.73      0.75      0.74        36\n",
      "  Tecnología       0.66      0.76      0.70        25\n",
      "\n",
      "    accuracy                           0.74        80\n",
      "   macro avg       0.63      0.57      0.59        80\n",
      "weighted avg       0.74      0.74      0.73        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'lemmatization'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['tokenization', 'lemmatization'] + TF-IDF: 0.356\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.45      0.62        11\n",
      "    Economía       0.52      0.92      0.67        36\n",
      "  Tecnología       0.75      0.36      0.49        25\n",
      "\n",
      "    accuracy                           0.59        80\n",
      "   macro avg       0.45      0.35      0.36        80\n",
      "weighted avg       0.61      0.59      0.54        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['tokenization', 'lemmatization'] + TF-IDF + SVD: 0.363\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.55      0.71        11\n",
      "    Economía       0.53      0.97      0.69        36\n",
      "  Tecnología       0.88      0.28      0.42        25\n",
      "\n",
      "    accuracy                           0.60        80\n",
      "   macro avg       0.48      0.36      0.36        80\n",
      "weighted avg       0.65      0.60      0.54        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Logistic Regression con ['tokenization', 'text_cleaning'] + Binarized: 0.596\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.71      0.81      0.75        36\n",
      "  Tecnología       0.64      0.64      0.64        25\n",
      "\n",
      "    accuracy                           0.72        80\n",
      "   macro avg       0.65      0.57      0.60        80\n",
      "weighted avg       0.73      0.72      0.72        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Logistic Regression con ['tokenization', 'text_cleaning'] + Frequency: 0.580\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.60      0.43      0.50         7\n",
      "    Deportes       0.91      0.91      0.91        11\n",
      "    Economía       0.74      0.81      0.77        36\n",
      "  Tecnología       0.72      0.72      0.72        25\n",
      "\n",
      "    accuracy                           0.75        80\n",
      "   macro avg       0.59      0.57      0.58        80\n",
      "weighted avg       0.74      0.75      0.74        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['tokenization', 'text_cleaning'] + TF-IDF: 0.298\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.27      0.43        11\n",
      "    Economía       0.51      1.00      0.67        36\n",
      "  Tecnología       1.00      0.24      0.39        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.50      0.30      0.30        80\n",
      "weighted avg       0.68      0.56      0.48        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['tokenization', 'text_cleaning'] + TF-IDF + SVD: 0.308\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.51      1.00      0.67        36\n",
      "  Tecnología       1.00      0.20      0.33        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.50      0.31      0.31        80\n",
      "weighted avg       0.68      0.56      0.48        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'lemmatization'] + Binarized\n",
      "F1 Score para Logistic Regression con ['stop_words', 'lemmatization'] + Binarized: 0.596\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.71      0.81      0.75        36\n",
      "  Tecnología       0.64      0.64      0.64        25\n",
      "\n",
      "    accuracy                           0.72        80\n",
      "   macro avg       0.65      0.57      0.60        80\n",
      "weighted avg       0.73      0.72      0.72        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'lemmatization'] + Frequency\n",
      "F1 Score para Logistic Regression con ['stop_words', 'lemmatization'] + Frequency: 0.580\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.60      0.43      0.50         7\n",
      "    Deportes       0.91      0.91      0.91        11\n",
      "    Economía       0.74      0.81      0.77        36\n",
      "  Tecnología       0.72      0.72      0.72        25\n",
      "\n",
      "    accuracy                           0.75        80\n",
      "   macro avg       0.59      0.57      0.58        80\n",
      "weighted avg       0.74      0.75      0.74        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'lemmatization'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['stop_words', 'lemmatization'] + TF-IDF: 0.298\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.27      0.43        11\n",
      "    Economía       0.51      1.00      0.67        36\n",
      "  Tecnología       1.00      0.24      0.39        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.50      0.30      0.30        80\n",
      "weighted avg       0.68      0.56      0.48        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['stop_words', 'lemmatization'] + TF-IDF + SVD: 0.308\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.51      1.00      0.67        36\n",
      "  Tecnología       1.00      0.20      0.33        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.50      0.31      0.31        80\n",
      "weighted avg       0.68      0.56      0.48        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'text_cleaning'] + Binarized\n",
      "F1 Score para Logistic Regression con ['stop_words', 'text_cleaning'] + Binarized: 0.596\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.71      0.81      0.75        36\n",
      "  Tecnología       0.64      0.64      0.64        25\n",
      "\n",
      "    accuracy                           0.72        80\n",
      "   macro avg       0.65      0.57      0.60        80\n",
      "weighted avg       0.73      0.72      0.72        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'text_cleaning'] + Frequency\n",
      "F1 Score para Logistic Regression con ['stop_words', 'text_cleaning'] + Frequency: 0.580\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.60      0.43      0.50         7\n",
      "    Deportes       0.91      0.91      0.91        11\n",
      "    Economía       0.74      0.81      0.77        36\n",
      "  Tecnología       0.72      0.72      0.72        25\n",
      "\n",
      "    accuracy                           0.75        80\n",
      "   macro avg       0.59      0.57      0.58        80\n",
      "weighted avg       0.74      0.75      0.74        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['stop_words', 'text_cleaning'] + TF-IDF: 0.298\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.27      0.43        11\n",
      "    Economía       0.51      1.00      0.67        36\n",
      "  Tecnología       1.00      0.24      0.39        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.50      0.30      0.30        80\n",
      "weighted avg       0.68      0.56      0.48        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['stop_words', 'text_cleaning'] + TF-IDF + SVD: 0.308\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.51      1.00      0.67        36\n",
      "  Tecnología       1.00      0.20      0.33        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.50      0.31      0.31        80\n",
      "weighted avg       0.68      0.56      0.48        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Logistic Regression con ['lemmatization', 'text_cleaning'] + Binarized: 0.596\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.71      0.81      0.75        36\n",
      "  Tecnología       0.64      0.64      0.64        25\n",
      "\n",
      "    accuracy                           0.72        80\n",
      "   macro avg       0.65      0.57      0.60        80\n",
      "weighted avg       0.73      0.72      0.72        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Logistic Regression con ['lemmatization', 'text_cleaning'] + Frequency: 0.580\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.60      0.43      0.50         7\n",
      "    Deportes       0.91      0.91      0.91        11\n",
      "    Economía       0.74      0.81      0.77        36\n",
      "  Tecnología       0.72      0.72      0.72        25\n",
      "\n",
      "    accuracy                           0.75        80\n",
      "   macro avg       0.59      0.57      0.58        80\n",
      "weighted avg       0.74      0.75      0.74        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['lemmatization', 'text_cleaning'] + TF-IDF: 0.298\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.27      0.43        11\n",
      "    Economía       0.51      1.00      0.67        36\n",
      "  Tecnología       1.00      0.24      0.39        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.50      0.30      0.30        80\n",
      "weighted avg       0.68      0.56      0.48        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.308\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.51      1.00      0.67        36\n",
      "  Tecnología       1.00      0.20      0.33        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.50      0.31      0.31        80\n",
      "weighted avg       0.68      0.56      0.48        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'lemmatization'] + Binarized\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization'] + Binarized: 0.593\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.83      0.91      0.87        11\n",
      "    Economía       0.73      0.75      0.74        36\n",
      "  Tecnología       0.71      0.80      0.75        25\n",
      "\n",
      "    accuracy                           0.75        80\n",
      "   macro avg       0.66      0.58      0.59        80\n",
      "weighted avg       0.75      0.75      0.74        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'lemmatization'] + Frequency\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization'] + Frequency: 0.511\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.14      0.25         7\n",
      "    Deportes       1.00      0.73      0.84        11\n",
      "    Economía       0.65      0.78      0.71        36\n",
      "  Tecnología       0.71      0.80      0.75        25\n",
      "\n",
      "    accuracy                           0.71        80\n",
      "   macro avg       0.67      0.49      0.51        80\n",
      "weighted avg       0.74      0.71      0.69        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'lemmatization'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF: 0.331\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.52      0.94      0.67        36\n",
      "  Tecnología       0.80      0.32      0.46        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.46      0.33      0.33        80\n",
      "weighted avg       0.62      0.57      0.52        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD: 0.331\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.52      0.94      0.67        36\n",
      "  Tecnología       0.80      0.32      0.46        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.46      0.33      0.33        80\n",
      "weighted avg       0.62      0.57      0.52        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'text_cleaning'] + Binarized\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'text_cleaning'] + Binarized: 0.569\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       1.00      0.73      0.84        11\n",
      "    Economía       0.66      0.81      0.72        36\n",
      "  Tecnología       0.68      0.68      0.68        25\n",
      "\n",
      "    accuracy                           0.71        80\n",
      "   macro avg       0.67      0.53      0.57        80\n",
      "weighted avg       0.73      0.71      0.71        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'text_cleaning'] + Frequency\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'text_cleaning'] + Frequency: 0.524\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       1.00      0.64      0.78        11\n",
      "    Economía       0.62      0.78      0.69        36\n",
      "  Tecnología       0.69      0.72      0.71        25\n",
      "\n",
      "    accuracy                           0.69        80\n",
      "   macro avg       0.66      0.48      0.52        80\n",
      "weighted avg       0.72      0.69      0.68        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF: 0.299\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.49      0.94      0.65        36\n",
      "  Tecnología       0.71      0.20      0.31        25\n",
      "\n",
      "    accuracy                           0.54        80\n",
      "   macro avg       0.44      0.30      0.30        80\n",
      "weighted avg       0.58      0.54      0.46        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD: 0.295\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.50      1.00      0.67        36\n",
      "  Tecnología       1.00      0.16      0.28        25\n",
      "\n",
      "    accuracy                           0.55        80\n",
      "   macro avg       0.50      0.30      0.30        80\n",
      "weighted avg       0.68      0.55      0.46        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Logistic Regression con ['tokenization', 'lemmatization', 'text_cleaning'] + Binarized: 0.611\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       1.00      0.91      0.95        11\n",
      "    Economía       0.69      0.75      0.72        36\n",
      "  Tecnología       0.63      0.68      0.65        25\n",
      "\n",
      "    accuracy                           0.72        80\n",
      "   macro avg       0.66      0.58      0.61        80\n",
      "weighted avg       0.73      0.72      0.72        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Logistic Regression con ['tokenization', 'lemmatization', 'text_cleaning'] + Frequency: 0.628\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.80      0.57      0.67         7\n",
      "    Deportes       1.00      0.91      0.95        11\n",
      "    Economía       0.78      0.78      0.78        36\n",
      "  Tecnología       0.69      0.80      0.74        25\n",
      "\n",
      "    accuracy                           0.78        80\n",
      "   macro avg       0.65      0.61      0.63        80\n",
      "weighted avg       0.77      0.78      0.77        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF: 0.389\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.64      0.78        11\n",
      "    Economía       0.54      0.92      0.68        36\n",
      "  Tecnología       0.75      0.36      0.49        25\n",
      "\n",
      "    accuracy                           0.61        80\n",
      "   macro avg       0.46      0.38      0.39        80\n",
      "weighted avg       0.62      0.61      0.57        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.362\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.86      0.55      0.67        11\n",
      "    Economía       0.54      0.94      0.69        36\n",
      "  Tecnología       0.80      0.32      0.46        25\n",
      "\n",
      "    accuracy                           0.60        80\n",
      "   macro avg       0.44      0.36      0.36        80\n",
      "weighted avg       0.61      0.60      0.54        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Logistic Regression con ['stop_words', 'lemmatization', 'text_cleaning'] + Binarized: 0.596\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.71      0.81      0.75        36\n",
      "  Tecnología       0.64      0.64      0.64        25\n",
      "\n",
      "    accuracy                           0.72        80\n",
      "   macro avg       0.65      0.57      0.60        80\n",
      "weighted avg       0.73      0.72      0.72        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Logistic Regression con ['stop_words', 'lemmatization', 'text_cleaning'] + Frequency: 0.580\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.60      0.43      0.50         7\n",
      "    Deportes       0.91      0.91      0.91        11\n",
      "    Economía       0.74      0.81      0.77        36\n",
      "  Tecnología       0.72      0.72      0.72        25\n",
      "\n",
      "    accuracy                           0.75        80\n",
      "   macro avg       0.59      0.57      0.58        80\n",
      "weighted avg       0.74      0.75      0.74        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF: 0.298\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.27      0.43        11\n",
      "    Economía       0.51      1.00      0.67        36\n",
      "  Tecnología       1.00      0.24      0.39        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.50      0.30      0.30        80\n",
      "weighted avg       0.68      0.56      0.48        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.308\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.51      1.00      0.67        36\n",
      "  Tecnología       1.00      0.20      0.33        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.50      0.31      0.31        80\n",
      "weighted avg       0.68      0.56      0.48        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized: 0.598\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.91      0.91      0.91        11\n",
      "    Economía       0.73      0.75      0.74        36\n",
      "  Tecnología       0.69      0.80      0.74        25\n",
      "\n",
      "    accuracy                           0.75        80\n",
      "   macro avg       0.67      0.58      0.60        80\n",
      "weighted avg       0.76      0.75      0.74        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency: 0.559\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.70      0.78      0.74        36\n",
      "  Tecnología       0.71      0.80      0.75        25\n",
      "\n",
      "    accuracy                           0.74        80\n",
      "   macro avg       0.66      0.54      0.56        80\n",
      "weighted avg       0.75      0.74      0.72        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF: 0.336\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.52      0.92      0.66        36\n",
      "  Tecnología       0.75      0.36      0.49        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.45      0.33      0.34        80\n",
      "weighted avg       0.60      0.57      0.52        80\n",
      "\n",
      "\n",
      "Evaluando Logistic Regression con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.326\n",
      "\n",
      "Reporte de Clasificación para Logistic Regression con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.51      0.92      0.65        36\n",
      "  Tecnología       0.73      0.32      0.44        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.45      0.32      0.33        80\n",
      "weighted avg       0.59      0.56      0.51        80\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization'] + Binarized\n",
      "F1 Score para Naive Bayes con ['tokenization'] + Binarized: 0.520\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.61      0.92      0.73        36\n",
      "  Tecnología       0.79      0.44      0.56        25\n",
      "\n",
      "    accuracy                           0.69        80\n",
      "   macro avg       0.66      0.49      0.52        80\n",
      "weighted avg       0.73      0.69      0.66        80\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization'] + Frequency\n",
      "F1 Score para Naive Bayes con ['tokenization'] + Frequency: 0.503\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       1.00      0.82      0.90        11\n",
      "    Economía       0.58      0.94      0.72        36\n",
      "  Tecnología       0.80      0.32      0.46        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.68      0.47      0.50        80\n",
      "weighted avg       0.73      0.66      0.63        80\n",
      "\n",
      "Skipping Naive Bayes with ['tokenization'] + TF-IDF\n",
      "Skipping Naive Bayes with ['tokenization'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['stop_words'] + Binarized\n",
      "F1 Score para Naive Bayes con ['stop_words'] + Binarized: 0.520\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['stop_words'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.61      0.92      0.73        36\n",
      "  Tecnología       0.79      0.44      0.56        25\n",
      "\n",
      "    accuracy                           0.69        80\n",
      "   macro avg       0.66      0.49      0.52        80\n",
      "weighted avg       0.73      0.69      0.66        80\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['stop_words'] + Frequency\n",
      "F1 Score para Naive Bayes con ['stop_words'] + Frequency: 0.503\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['stop_words'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       1.00      0.82      0.90        11\n",
      "    Economía       0.58      0.94      0.72        36\n",
      "  Tecnología       0.80      0.32      0.46        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.68      0.47      0.50        80\n",
      "weighted avg       0.73      0.66      0.63        80\n",
      "\n",
      "Skipping Naive Bayes with ['stop_words'] + TF-IDF\n",
      "Skipping Naive Bayes with ['stop_words'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['lemmatization'] + Binarized\n",
      "F1 Score para Naive Bayes con ['lemmatization'] + Binarized: 0.520\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.61      0.92      0.73        36\n",
      "  Tecnología       0.79      0.44      0.56        25\n",
      "\n",
      "    accuracy                           0.69        80\n",
      "   macro avg       0.66      0.49      0.52        80\n",
      "weighted avg       0.73      0.69      0.66        80\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['lemmatization'] + Frequency\n",
      "F1 Score para Naive Bayes con ['lemmatization'] + Frequency: 0.503\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       1.00      0.82      0.90        11\n",
      "    Economía       0.58      0.94      0.72        36\n",
      "  Tecnología       0.80      0.32      0.46        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.68      0.47      0.50        80\n",
      "weighted avg       0.73      0.66      0.63        80\n",
      "\n",
      "Skipping Naive Bayes with ['lemmatization'] + TF-IDF\n",
      "Skipping Naive Bayes with ['lemmatization'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['text_cleaning'] + Binarized\n",
      "F1 Score para Naive Bayes con ['text_cleaning'] + Binarized: 0.520\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.61      0.92      0.73        36\n",
      "  Tecnología       0.79      0.44      0.56        25\n",
      "\n",
      "    accuracy                           0.69        80\n",
      "   macro avg       0.66      0.49      0.52        80\n",
      "weighted avg       0.73      0.69      0.66        80\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['text_cleaning'] + Frequency\n",
      "F1 Score para Naive Bayes con ['text_cleaning'] + Frequency: 0.503\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       1.00      0.82      0.90        11\n",
      "    Economía       0.58      0.94      0.72        36\n",
      "  Tecnología       0.80      0.32      0.46        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.68      0.47      0.50        80\n",
      "weighted avg       0.73      0.66      0.63        80\n",
      "\n",
      "Skipping Naive Bayes with ['text_cleaning'] + TF-IDF\n",
      "Skipping Naive Bayes with ['text_cleaning'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'stop_words'] + Binarized\n",
      "F1 Score para Naive Bayes con ['tokenization', 'stop_words'] + Binarized: 0.605\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'stop_words'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.67      0.91      0.77        11\n",
      "    Economía       0.83      0.83      0.83        36\n",
      "  Tecnología       0.81      0.84      0.82        25\n",
      "\n",
      "    accuracy                           0.80        80\n",
      "   macro avg       0.66      0.60      0.61        80\n",
      "weighted avg       0.81      0.80      0.79        80\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'stop_words'] + Frequency\n",
      "F1 Score para Naive Bayes con ['tokenization', 'stop_words'] + Frequency: 0.578\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'stop_words'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.75      0.43      0.55         7\n",
      "    Deportes       0.67      0.91      0.77        11\n",
      "    Economía       0.82      0.75      0.78        36\n",
      "  Tecnología       0.75      0.84      0.79        25\n",
      "\n",
      "    accuracy                           0.76        80\n",
      "   macro avg       0.60      0.59      0.58        80\n",
      "weighted avg       0.76      0.76      0.75        80\n",
      "\n",
      "Skipping Naive Bayes with ['tokenization', 'stop_words'] + TF-IDF\n",
      "Skipping Naive Bayes with ['tokenization', 'stop_words'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'lemmatization'] + Binarized\n",
      "F1 Score para Naive Bayes con ['tokenization', 'lemmatization'] + Binarized: 0.555\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.67      0.89      0.76        36\n",
      "  Tecnología       0.80      0.64      0.71        25\n",
      "\n",
      "    accuracy                           0.74        80\n",
      "   macro avg       0.67      0.53      0.55        80\n",
      "weighted avg       0.76      0.74      0.72        80\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'lemmatization'] + Frequency\n",
      "F1 Score para Naive Bayes con ['tokenization', 'lemmatization'] + Frequency: 0.527\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.91      0.91      0.91        11\n",
      "    Economía       0.60      0.89      0.72        36\n",
      "  Tecnología       0.79      0.44      0.56        25\n",
      "\n",
      "    accuracy                           0.69        80\n",
      "   macro avg       0.66      0.50      0.53        80\n",
      "weighted avg       0.73      0.69      0.66        80\n",
      "\n",
      "Skipping Naive Bayes with ['tokenization', 'lemmatization'] + TF-IDF\n",
      "Skipping Naive Bayes with ['tokenization', 'lemmatization'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Naive Bayes con ['tokenization', 'text_cleaning'] + Binarized: 0.520\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.61      0.92      0.73        36\n",
      "  Tecnología       0.79      0.44      0.56        25\n",
      "\n",
      "    accuracy                           0.69        80\n",
      "   macro avg       0.66      0.49      0.52        80\n",
      "weighted avg       0.73      0.69      0.66        80\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Naive Bayes con ['tokenization', 'text_cleaning'] + Frequency: 0.503\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       1.00      0.82      0.90        11\n",
      "    Economía       0.58      0.94      0.72        36\n",
      "  Tecnología       0.80      0.32      0.46        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.68      0.47      0.50        80\n",
      "weighted avg       0.73      0.66      0.63        80\n",
      "\n",
      "Skipping Naive Bayes with ['tokenization', 'text_cleaning'] + TF-IDF\n",
      "Skipping Naive Bayes with ['tokenization', 'text_cleaning'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['stop_words', 'lemmatization'] + Binarized\n",
      "F1 Score para Naive Bayes con ['stop_words', 'lemmatization'] + Binarized: 0.520\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['stop_words', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.61      0.92      0.73        36\n",
      "  Tecnología       0.79      0.44      0.56        25\n",
      "\n",
      "    accuracy                           0.69        80\n",
      "   macro avg       0.66      0.49      0.52        80\n",
      "weighted avg       0.73      0.69      0.66        80\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['stop_words', 'lemmatization'] + Frequency\n",
      "F1 Score para Naive Bayes con ['stop_words', 'lemmatization'] + Frequency: 0.503\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['stop_words', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       1.00      0.82      0.90        11\n",
      "    Economía       0.58      0.94      0.72        36\n",
      "  Tecnología       0.80      0.32      0.46        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.68      0.47      0.50        80\n",
      "weighted avg       0.73      0.66      0.63        80\n",
      "\n",
      "Skipping Naive Bayes with ['stop_words', 'lemmatization'] + TF-IDF\n",
      "Skipping Naive Bayes with ['stop_words', 'lemmatization'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['stop_words', 'text_cleaning'] + Binarized\n",
      "F1 Score para Naive Bayes con ['stop_words', 'text_cleaning'] + Binarized: 0.520\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['stop_words', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.61      0.92      0.73        36\n",
      "  Tecnología       0.79      0.44      0.56        25\n",
      "\n",
      "    accuracy                           0.69        80\n",
      "   macro avg       0.66      0.49      0.52        80\n",
      "weighted avg       0.73      0.69      0.66        80\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['stop_words', 'text_cleaning'] + Frequency\n",
      "F1 Score para Naive Bayes con ['stop_words', 'text_cleaning'] + Frequency: 0.503\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['stop_words', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       1.00      0.82      0.90        11\n",
      "    Economía       0.58      0.94      0.72        36\n",
      "  Tecnología       0.80      0.32      0.46        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.68      0.47      0.50        80\n",
      "weighted avg       0.73      0.66      0.63        80\n",
      "\n",
      "Skipping Naive Bayes with ['stop_words', 'text_cleaning'] + TF-IDF\n",
      "Skipping Naive Bayes with ['stop_words', 'text_cleaning'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Naive Bayes con ['lemmatization', 'text_cleaning'] + Binarized: 0.520\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.61      0.92      0.73        36\n",
      "  Tecnología       0.79      0.44      0.56        25\n",
      "\n",
      "    accuracy                           0.69        80\n",
      "   macro avg       0.66      0.49      0.52        80\n",
      "weighted avg       0.73      0.69      0.66        80\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Naive Bayes con ['lemmatization', 'text_cleaning'] + Frequency: 0.503\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       1.00      0.82      0.90        11\n",
      "    Economía       0.58      0.94      0.72        36\n",
      "  Tecnología       0.80      0.32      0.46        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.68      0.47      0.50        80\n",
      "weighted avg       0.73      0.66      0.63        80\n",
      "\n",
      "Skipping Naive Bayes with ['lemmatization', 'text_cleaning'] + TF-IDF\n",
      "Skipping Naive Bayes with ['lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'stop_words', 'lemmatization'] + Binarized\n",
      "F1 Score para Naive Bayes con ['tokenization', 'stop_words', 'lemmatization'] + Binarized: 0.602\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'stop_words', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.71      0.91      0.80        11\n",
      "    Economía       0.79      0.83      0.81        36\n",
      "  Tecnología       0.80      0.80      0.80        25\n",
      "\n",
      "    accuracy                           0.79        80\n",
      "   macro avg       0.66      0.59      0.60        80\n",
      "weighted avg       0.79      0.79      0.78        80\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'stop_words', 'lemmatization'] + Frequency\n",
      "F1 Score para Naive Bayes con ['tokenization', 'stop_words', 'lemmatization'] + Frequency: 0.614\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'stop_words', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.73      1.00      0.85        11\n",
      "    Economía       0.83      0.81      0.82        36\n",
      "  Tecnología       0.78      0.84      0.81        25\n",
      "\n",
      "    accuracy                           0.80        80\n",
      "   macro avg       0.67      0.61      0.61        80\n",
      "weighted avg       0.80      0.80      0.79        80\n",
      "\n",
      "Skipping Naive Bayes with ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF\n",
      "Skipping Naive Bayes with ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'stop_words', 'text_cleaning'] + Binarized\n",
      "F1 Score para Naive Bayes con ['tokenization', 'stop_words', 'text_cleaning'] + Binarized: 0.605\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'stop_words', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.67      0.91      0.77        11\n",
      "    Economía       0.83      0.83      0.83        36\n",
      "  Tecnología       0.81      0.84      0.82        25\n",
      "\n",
      "    accuracy                           0.80        80\n",
      "   macro avg       0.66      0.60      0.61        80\n",
      "weighted avg       0.81      0.80      0.79        80\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'stop_words', 'text_cleaning'] + Frequency\n",
      "F1 Score para Naive Bayes con ['tokenization', 'stop_words', 'text_cleaning'] + Frequency: 0.578\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'stop_words', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.75      0.43      0.55         7\n",
      "    Deportes       0.67      0.91      0.77        11\n",
      "    Economía       0.82      0.75      0.78        36\n",
      "  Tecnología       0.75      0.84      0.79        25\n",
      "\n",
      "    accuracy                           0.76        80\n",
      "   macro avg       0.60      0.59      0.58        80\n",
      "weighted avg       0.76      0.76      0.75        80\n",
      "\n",
      "Skipping Naive Bayes with ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF\n",
      "Skipping Naive Bayes with ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Naive Bayes con ['tokenization', 'lemmatization', 'text_cleaning'] + Binarized: 0.594\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.69      0.92      0.79        36\n",
      "  Tecnología       0.84      0.64      0.73        25\n",
      "\n",
      "    accuracy                           0.76        80\n",
      "   macro avg       0.69      0.56      0.59        80\n",
      "weighted avg       0.78      0.76      0.75        80\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Naive Bayes con ['tokenization', 'lemmatization', 'text_cleaning'] + Frequency: 0.547\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.92      1.00      0.96        11\n",
      "    Economía       0.63      0.89      0.74        36\n",
      "  Tecnología       0.80      0.48      0.60        25\n",
      "\n",
      "    accuracy                           0.71        80\n",
      "   macro avg       0.67      0.53      0.55        80\n",
      "weighted avg       0.75      0.71      0.69        80\n",
      "\n",
      "Skipping Naive Bayes with ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "Skipping Naive Bayes with ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Naive Bayes con ['stop_words', 'lemmatization', 'text_cleaning'] + Binarized: 0.520\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['stop_words', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.61      0.92      0.73        36\n",
      "  Tecnología       0.79      0.44      0.56        25\n",
      "\n",
      "    accuracy                           0.69        80\n",
      "   macro avg       0.66      0.49      0.52        80\n",
      "weighted avg       0.73      0.69      0.66        80\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Naive Bayes con ['stop_words', 'lemmatization', 'text_cleaning'] + Frequency: 0.503\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['stop_words', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       1.00      0.82      0.90        11\n",
      "    Economía       0.58      0.94      0.72        36\n",
      "  Tecnología       0.80      0.32      0.46        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.68      0.47      0.50        80\n",
      "weighted avg       0.73      0.66      0.63        80\n",
      "\n",
      "Skipping Naive Bayes with ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "Skipping Naive Bayes with ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Naive Bayes con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized: 0.602\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.71      0.91      0.80        11\n",
      "    Economía       0.79      0.83      0.81        36\n",
      "  Tecnología       0.80      0.80      0.80        25\n",
      "\n",
      "    accuracy                           0.79        80\n",
      "   macro avg       0.66      0.59      0.60        80\n",
      "weighted avg       0.79      0.79      0.78        80\n",
      "\n",
      "\n",
      "Evaluando Naive Bayes con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Naive Bayes con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency: 0.621\n",
      "\n",
      "Reporte de Clasificación para Naive Bayes con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.73      1.00      0.85        11\n",
      "    Economía       0.83      0.83      0.83        36\n",
      "  Tecnología       0.81      0.84      0.82        25\n",
      "\n",
      "    accuracy                           0.81        80\n",
      "   macro avg       0.67      0.62      0.62        80\n",
      "weighted avg       0.82      0.81      0.80        80\n",
      "\n",
      "Skipping Naive Bayes with ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "Skipping Naive Bayes with ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization'] + Binarized\n",
      "F1 Score para Decision Tree con ['tokenization'] + Binarized: 0.449\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.43      0.43      0.43         7\n",
      "    Deportes       0.83      0.45      0.59        11\n",
      "    Economía       0.57      0.78      0.66        36\n",
      "  Tecnología       0.71      0.48      0.57        25\n",
      "\n",
      "    accuracy                           0.60        80\n",
      "   macro avg       0.51      0.43      0.45        80\n",
      "weighted avg       0.63      0.60      0.59        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization'] + Frequency\n",
      "F1 Score para Decision Tree con ['tokenization'] + Frequency: 0.365\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.20      0.14      0.17         7\n",
      "    Deportes       0.45      0.45      0.45        11\n",
      "    Economía       0.60      0.67      0.63        36\n",
      "  Tecnología       0.58      0.56      0.57        25\n",
      "\n",
      "    accuracy                           0.55        80\n",
      "   macro avg       0.37      0.36      0.36        80\n",
      "weighted avg       0.53      0.55      0.54        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization'] + TF-IDF\n",
      "F1 Score para Decision Tree con ['tokenization'] + TF-IDF: 0.407\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.44      0.57      0.50         7\n",
      "    Deportes       0.50      0.45      0.48        11\n",
      "    Economía       0.54      0.61      0.57        36\n",
      "  Tecnología       0.55      0.44      0.49        25\n",
      "\n",
      "    accuracy                           0.53        80\n",
      "   macro avg       0.41      0.42      0.41        80\n",
      "weighted avg       0.52      0.53      0.52        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization'] + TF-IDF + SVD\n",
      "F1 Score para Decision Tree con ['tokenization'] + TF-IDF + SVD: 0.470\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.36      0.57      0.44         7\n",
      "    Deportes       0.69      0.82      0.75        11\n",
      "    Economía       0.69      0.81      0.74        36\n",
      "  Tecnología       0.57      0.32      0.41        25\n",
      "\n",
      "    accuracy                           0.62        80\n",
      "   macro avg       0.46      0.50      0.47        80\n",
      "weighted avg       0.62      0.62      0.60        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['stop_words'] + Binarized\n",
      "F1 Score para Decision Tree con ['stop_words'] + Binarized: 0.423\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['stop_words'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.43      0.43      0.43         7\n",
      "    Deportes       0.71      0.45      0.56        11\n",
      "    Economía       0.54      0.72      0.62        36\n",
      "  Tecnología       0.61      0.44      0.51        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.46      0.41      0.42        80\n",
      "weighted avg       0.57      0.56      0.55        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['stop_words'] + Frequency\n",
      "F1 Score para Decision Tree con ['stop_words'] + Frequency: 0.378\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['stop_words'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.14      0.14      0.14         7\n",
      "    Deportes       0.56      0.45      0.50        11\n",
      "    Economía       0.61      0.64      0.62        36\n",
      "  Tecnología       0.62      0.64      0.63        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.38      0.38      0.38        80\n",
      "weighted avg       0.55      0.56      0.56        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['stop_words'] + TF-IDF\n",
      "F1 Score para Decision Tree con ['stop_words'] + TF-IDF: 0.428\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['stop_words'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.45      0.71      0.56         7\n",
      "    Deportes       0.38      0.45      0.42        11\n",
      "    Economía       0.65      0.56      0.60        36\n",
      "  Tecnología       0.58      0.56      0.57        25\n",
      "\n",
      "    accuracy                           0.55        80\n",
      "   macro avg       0.41      0.46      0.43        80\n",
      "weighted avg       0.57      0.55      0.55        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['stop_words'] + TF-IDF + SVD\n",
      "F1 Score para Decision Tree con ['stop_words'] + TF-IDF + SVD: 0.476\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['stop_words'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.36      0.57      0.44         7\n",
      "    Deportes       0.64      0.64      0.64        11\n",
      "    Economía       0.68      0.78      0.73        36\n",
      "  Tecnología       0.71      0.48      0.57        25\n",
      "\n",
      "    accuracy                           0.64        80\n",
      "   macro avg       0.48      0.49      0.48        80\n",
      "weighted avg       0.65      0.64      0.63        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['lemmatization'] + Binarized\n",
      "F1 Score para Decision Tree con ['lemmatization'] + Binarized: 0.406\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.33      0.29      0.31         7\n",
      "    Deportes       0.71      0.45      0.56        11\n",
      "    Economía       0.54      0.75      0.63        36\n",
      "  Tecnología       0.69      0.44      0.54        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.46      0.39      0.41        80\n",
      "weighted avg       0.59      0.56      0.55        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['lemmatization'] + Frequency\n",
      "F1 Score para Decision Tree con ['lemmatization'] + Frequency: 0.366\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.17      0.14      0.15         7\n",
      "    Deportes       0.56      0.45      0.50        11\n",
      "    Economía       0.56      0.61      0.59        36\n",
      "  Tecnología       0.58      0.60      0.59        25\n",
      "\n",
      "    accuracy                           0.54        80\n",
      "   macro avg       0.37      0.36      0.37        80\n",
      "weighted avg       0.53      0.54      0.53        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['lemmatization'] + TF-IDF\n",
      "F1 Score para Decision Tree con ['lemmatization'] + TF-IDF: 0.388\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.30      0.43      0.35         7\n",
      "    Deportes       0.50      0.45      0.48        11\n",
      "    Economía       0.62      0.58      0.60        36\n",
      "  Tecnología       0.50      0.52      0.51        25\n",
      "\n",
      "    accuracy                           0.53        80\n",
      "   macro avg       0.38      0.40      0.39        80\n",
      "weighted avg       0.53      0.53      0.53        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para Decision Tree con ['lemmatization'] + TF-IDF + SVD: 0.470\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.36      0.57      0.44         7\n",
      "    Deportes       0.69      0.82      0.75        11\n",
      "    Economía       0.67      0.81      0.73        36\n",
      "  Tecnología       0.62      0.32      0.42        25\n",
      "\n",
      "    accuracy                           0.62        80\n",
      "   macro avg       0.47      0.50      0.47        80\n",
      "weighted avg       0.62      0.62      0.60        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['text_cleaning'] + Binarized\n",
      "F1 Score para Decision Tree con ['text_cleaning'] + Binarized: 0.425\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.43      0.43      0.43         7\n",
      "    Deportes       0.83      0.45      0.59        11\n",
      "    Economía       0.55      0.67      0.60        36\n",
      "  Tecnología       0.55      0.48      0.51        25\n",
      "\n",
      "    accuracy                           0.55        80\n",
      "   macro avg       0.47      0.41      0.43        80\n",
      "weighted avg       0.57      0.55      0.55        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['text_cleaning'] + Frequency\n",
      "F1 Score para Decision Tree con ['text_cleaning'] + Frequency: 0.386\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.17      0.14      0.15         7\n",
      "    Deportes       0.56      0.45      0.50        11\n",
      "    Economía       0.61      0.64      0.62        36\n",
      "  Tecnología       0.63      0.68      0.65        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.39      0.38      0.39        80\n",
      "weighted avg       0.56      0.57      0.57        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['text_cleaning'] + TF-IDF\n",
      "F1 Score para Decision Tree con ['text_cleaning'] + TF-IDF: 0.449\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.50      0.71      0.59         7\n",
      "    Deportes       0.45      0.45      0.45        11\n",
      "    Economía       0.58      0.61      0.59        36\n",
      "  Tecnología       0.67      0.56      0.61        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.44      0.47      0.45        80\n",
      "weighted avg       0.58      0.57      0.57        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Decision Tree con ['text_cleaning'] + TF-IDF + SVD: 0.483\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.38      0.71      0.50         7\n",
      "    Deportes       0.67      0.73      0.70        11\n",
      "    Economía       0.69      0.81      0.74        36\n",
      "  Tecnología       0.69      0.36      0.47        25\n",
      "\n",
      "    accuracy                           0.64        80\n",
      "   macro avg       0.49      0.52      0.48        80\n",
      "weighted avg       0.65      0.64      0.62        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'stop_words'] + Binarized\n",
      "F1 Score para Decision Tree con ['tokenization', 'stop_words'] + Binarized: 0.473\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'stop_words'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.80      0.57      0.67         7\n",
      "    Deportes       0.38      0.82      0.51        11\n",
      "    Economía       0.63      0.53      0.58        36\n",
      "  Tecnología       0.67      0.56      0.61        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.49      0.50      0.47        80\n",
      "weighted avg       0.61      0.57      0.58        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'stop_words'] + Frequency\n",
      "F1 Score para Decision Tree con ['tokenization', 'stop_words'] + Frequency: 0.393\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'stop_words'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.50      0.29      0.36         7\n",
      "    Deportes       0.32      0.82      0.46        11\n",
      "    Economía       0.65      0.61      0.63        36\n",
      "  Tecnología       0.71      0.40      0.51        25\n",
      "\n",
      "    accuracy                           0.54        80\n",
      "   macro avg       0.44      0.42      0.39        80\n",
      "weighted avg       0.60      0.54      0.54        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'stop_words'] + TF-IDF\n",
      "F1 Score para Decision Tree con ['tokenization', 'stop_words'] + TF-IDF: 0.433\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'stop_words'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.75      0.43      0.55         7\n",
      "    Deportes       0.32      0.64      0.42        11\n",
      "    Economía       0.66      0.53      0.58        36\n",
      "  Tecnología       0.62      0.60      0.61        25\n",
      "\n",
      "    accuracy                           0.55        80\n",
      "   macro avg       0.47      0.44      0.43        80\n",
      "weighted avg       0.60      0.55      0.56        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'stop_words'] + TF-IDF + SVD\n",
      "F1 Score para Decision Tree con ['tokenization', 'stop_words'] + TF-IDF + SVD: 0.482\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'stop_words'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.60      0.43      0.50         7\n",
      "    Deportes       0.58      0.64      0.61        11\n",
      "    Economía       0.68      0.72      0.70        36\n",
      "  Tecnología       0.60      0.60      0.60        25\n",
      "\n",
      "    accuracy                           0.64        80\n",
      "   macro avg       0.49      0.48      0.48        80\n",
      "weighted avg       0.63      0.64      0.63        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'lemmatization'] + Binarized\n",
      "F1 Score para Decision Tree con ['tokenization', 'lemmatization'] + Binarized: 0.375\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.20      0.14      0.17         7\n",
      "    Deportes       0.62      0.45      0.53        11\n",
      "    Economía       0.57      0.75      0.65        36\n",
      "  Tecnología       0.60      0.48      0.53        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.40      0.37      0.38        80\n",
      "weighted avg       0.55      0.56      0.55        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'lemmatization'] + Frequency\n",
      "F1 Score para Decision Tree con ['tokenization', 'lemmatization'] + Frequency: 0.403\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.67      0.29      0.40         7\n",
      "    Deportes       0.36      0.36      0.36        11\n",
      "    Economía       0.58      0.72      0.64        36\n",
      "  Tecnología       0.67      0.56      0.61        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.45      0.39      0.40        80\n",
      "weighted avg       0.58      0.57      0.56        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'lemmatization'] + TF-IDF\n",
      "F1 Score para Decision Tree con ['tokenization', 'lemmatization'] + TF-IDF: 0.490\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.67      0.57      0.62         7\n",
      "    Deportes       0.67      0.55      0.60        11\n",
      "    Economía       0.57      0.69      0.62        36\n",
      "  Tecnología       0.67      0.56      0.61        25\n",
      "\n",
      "    accuracy                           0.61        80\n",
      "   macro avg       0.51      0.47      0.49        80\n",
      "weighted avg       0.61      0.61      0.61        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para Decision Tree con ['tokenization', 'lemmatization'] + TF-IDF + SVD: 0.378\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.33      0.14      0.20         7\n",
      "    Deportes       0.50      0.64      0.56        11\n",
      "    Economía       0.60      0.69      0.64        36\n",
      "  Tecnología       0.55      0.44      0.49        25\n",
      "\n",
      "    accuracy                           0.55        80\n",
      "   macro avg       0.40      0.38      0.38        80\n",
      "weighted avg       0.54      0.55      0.54        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Decision Tree con ['tokenization', 'text_cleaning'] + Binarized: 0.441\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.40      0.29      0.33         7\n",
      "    Deportes       0.86      0.55      0.67        11\n",
      "    Economía       0.57      0.81      0.67        36\n",
      "  Tecnología       0.69      0.44      0.54        25\n",
      "\n",
      "    accuracy                           0.60        80\n",
      "   macro avg       0.50      0.42      0.44        80\n",
      "weighted avg       0.62      0.60      0.59        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Decision Tree con ['tokenization', 'text_cleaning'] + Frequency: 0.376\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.20      0.14      0.17         7\n",
      "    Deportes       0.56      0.45      0.50        11\n",
      "    Economía       0.60      0.69      0.64        36\n",
      "  Tecnología       0.58      0.56      0.57        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.39      0.37      0.38        80\n",
      "weighted avg       0.54      0.56      0.55        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Decision Tree con ['tokenization', 'text_cleaning'] + TF-IDF: 0.426\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.50      0.57      0.53         7\n",
      "    Deportes       0.36      0.45      0.40        11\n",
      "    Economía       0.59      0.56      0.57        36\n",
      "  Tecnología       0.65      0.60      0.62        25\n",
      "\n",
      "    accuracy                           0.55        80\n",
      "   macro avg       0.42      0.44      0.43        80\n",
      "weighted avg       0.56      0.55      0.55        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Decision Tree con ['tokenization', 'text_cleaning'] + TF-IDF + SVD: 0.465\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.42      0.71      0.53         7\n",
      "    Deportes       0.67      0.73      0.70        11\n",
      "    Economía       0.67      0.81      0.73        36\n",
      "  Tecnología       0.54      0.28      0.37        25\n",
      "\n",
      "    accuracy                           0.61        80\n",
      "   macro avg       0.46      0.51      0.46        80\n",
      "weighted avg       0.60      0.61      0.59        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['stop_words', 'lemmatization'] + Binarized\n",
      "F1 Score para Decision Tree con ['stop_words', 'lemmatization'] + Binarized: 0.424\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['stop_words', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.43      0.43      0.43         7\n",
      "    Deportes       0.83      0.45      0.59        11\n",
      "    Economía       0.53      0.69      0.60        36\n",
      "  Tecnología       0.58      0.44      0.50        25\n",
      "\n",
      "    accuracy                           0.55        80\n",
      "   macro avg       0.47      0.40      0.42        80\n",
      "weighted avg       0.57      0.55      0.55        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['stop_words', 'lemmatization'] + Frequency\n",
      "F1 Score para Decision Tree con ['stop_words', 'lemmatization'] + Frequency: 0.359\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['stop_words', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.20      0.14      0.17         7\n",
      "    Deportes       0.45      0.45      0.45        11\n",
      "    Economía       0.60      0.67      0.63        36\n",
      "  Tecnología       0.57      0.52      0.54        25\n",
      "\n",
      "    accuracy                           0.54        80\n",
      "   macro avg       0.36      0.36      0.36        80\n",
      "weighted avg       0.53      0.54      0.53        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['stop_words', 'lemmatization'] + TF-IDF\n",
      "F1 Score para Decision Tree con ['stop_words', 'lemmatization'] + TF-IDF: 0.429\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['stop_words', 'lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.50      0.57      0.53         7\n",
      "    Deportes       0.55      0.55      0.55        11\n",
      "    Economía       0.55      0.58      0.57        36\n",
      "  Tecnología       0.52      0.48      0.50        25\n",
      "\n",
      "    accuracy                           0.54        80\n",
      "   macro avg       0.42      0.44      0.43        80\n",
      "weighted avg       0.53      0.54      0.53        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['stop_words', 'lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para Decision Tree con ['stop_words', 'lemmatization'] + TF-IDF + SVD: 0.458\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['stop_words', 'lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.27      0.57      0.36         7\n",
      "    Deportes       0.80      0.73      0.76        11\n",
      "    Economía       0.69      0.81      0.74        36\n",
      "  Tecnología       0.62      0.32      0.42        25\n",
      "\n",
      "    accuracy                           0.61        80\n",
      "   macro avg       0.47      0.48      0.46        80\n",
      "weighted avg       0.64      0.61      0.60        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['stop_words', 'text_cleaning'] + Binarized\n",
      "F1 Score para Decision Tree con ['stop_words', 'text_cleaning'] + Binarized: 0.440\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['stop_words', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.43      0.43      0.43         7\n",
      "    Deportes       0.75      0.55      0.63        11\n",
      "    Economía       0.55      0.72      0.63        36\n",
      "  Tecnología       0.61      0.44      0.51        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.47      0.43      0.44        80\n",
      "weighted avg       0.58      0.57      0.57        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['stop_words', 'text_cleaning'] + Frequency\n",
      "F1 Score para Decision Tree con ['stop_words', 'text_cleaning'] + Frequency: 0.394\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['stop_words', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.14      0.14      0.14         7\n",
      "    Deportes       0.60      0.55      0.57        11\n",
      "    Economía       0.62      0.64      0.63        36\n",
      "  Tecnología       0.62      0.64      0.63        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.40      0.39      0.39        80\n",
      "weighted avg       0.57      0.57      0.57        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['stop_words', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Decision Tree con ['stop_words', 'text_cleaning'] + TF-IDF: 0.435\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['stop_words', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.45      0.71      0.56         7\n",
      "    Deportes       0.38      0.45      0.42        11\n",
      "    Economía       0.66      0.58      0.62        36\n",
      "  Tecnología       0.61      0.56      0.58        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.42      0.46      0.43        80\n",
      "weighted avg       0.58      0.56      0.57        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['stop_words', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Decision Tree con ['stop_words', 'text_cleaning'] + TF-IDF + SVD: 0.440\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['stop_words', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.29      0.57      0.38         7\n",
      "    Deportes       0.75      0.55      0.63        11\n",
      "    Economía       0.66      0.81      0.72        36\n",
      "  Tecnología       0.64      0.36      0.46        25\n",
      "\n",
      "    accuracy                           0.60        80\n",
      "   macro avg       0.47      0.46      0.44        80\n",
      "weighted avg       0.63      0.60      0.59        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Decision Tree con ['lemmatization', 'text_cleaning'] + Binarized: 0.447\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.43      0.43      0.43         7\n",
      "    Deportes       0.86      0.55      0.67        11\n",
      "    Economía       0.55      0.72      0.63        36\n",
      "  Tecnología       0.61      0.44      0.51        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.49      0.43      0.45        80\n",
      "weighted avg       0.60      0.57      0.57        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Decision Tree con ['lemmatization', 'text_cleaning'] + Frequency: 0.373\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.20      0.14      0.17         7\n",
      "    Deportes       0.50      0.45      0.48        11\n",
      "    Economía       0.61      0.69      0.65        36\n",
      "  Tecnología       0.58      0.56      0.57        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.38      0.37      0.37        80\n",
      "weighted avg       0.54      0.56      0.55        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Decision Tree con ['lemmatization', 'text_cleaning'] + TF-IDF: 0.429\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.57      0.57      0.57         7\n",
      "    Deportes       0.41      0.64      0.50        11\n",
      "    Economía       0.51      0.50      0.51        36\n",
      "  Tecnología       0.62      0.52      0.57        25\n",
      "\n",
      "    accuracy                           0.53        80\n",
      "   macro avg       0.42      0.45      0.43        80\n",
      "weighted avg       0.53      0.53      0.52        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Decision Tree con ['lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.472\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.38      0.71      0.50         7\n",
      "    Deportes       0.67      0.73      0.70        11\n",
      "    Economía       0.69      0.81      0.74        36\n",
      "  Tecnología       0.62      0.32      0.42        25\n",
      "\n",
      "    accuracy                           0.62        80\n",
      "   macro avg       0.47      0.51      0.47        80\n",
      "weighted avg       0.63      0.62      0.61        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'stop_words', 'lemmatization'] + Binarized\n",
      "F1 Score para Decision Tree con ['tokenization', 'stop_words', 'lemmatization'] + Binarized: 0.429\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'stop_words', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.67      0.29      0.40         7\n",
      "    Deportes       0.40      0.73      0.52        11\n",
      "    Economía       0.63      0.61      0.62        36\n",
      "  Tecnología       0.67      0.56      0.61        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.47      0.44      0.43        80\n",
      "weighted avg       0.60      0.57      0.58        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'stop_words', 'lemmatization'] + Frequency\n",
      "F1 Score para Decision Tree con ['tokenization', 'stop_words', 'lemmatization'] + Frequency: 0.305\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'stop_words', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.30      0.82      0.44        11\n",
      "    Economía       0.59      0.56      0.57        36\n",
      "  Tecnología       0.71      0.40      0.51        25\n",
      "\n",
      "    accuracy                           0.49        80\n",
      "   macro avg       0.32      0.35      0.30        80\n",
      "weighted avg       0.53      0.49      0.48        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'stop_words', 'lemmatization'] + TF-IDF\n",
      "F1 Score para Decision Tree con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF: 0.503\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.83      0.71      0.77         7\n",
      "    Deportes       0.41      0.64      0.50        11\n",
      "    Economía       0.69      0.61      0.65        36\n",
      "  Tecnología       0.60      0.60      0.60        25\n",
      "\n",
      "    accuracy                           0.61        80\n",
      "   macro avg       0.51      0.51      0.50        80\n",
      "weighted avg       0.63      0.61      0.61        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para Decision Tree con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD: 0.413\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.20      0.14      0.17         7\n",
      "    Deportes       0.62      0.73      0.67        11\n",
      "    Economía       0.68      0.64      0.66        36\n",
      "  Tecnología       0.56      0.60      0.58        25\n",
      "\n",
      "    accuracy                           0.59        80\n",
      "   macro avg       0.41      0.42      0.41        80\n",
      "weighted avg       0.58      0.59      0.58        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'stop_words', 'text_cleaning'] + Binarized\n",
      "F1 Score para Decision Tree con ['tokenization', 'stop_words', 'text_cleaning'] + Binarized: 0.419\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'stop_words', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.50      0.43      0.46         7\n",
      "    Deportes       0.33      0.73      0.46        11\n",
      "    Economía       0.61      0.61      0.61        36\n",
      "  Tecnología       0.79      0.44      0.56        25\n",
      "\n",
      "    accuracy                           0.55        80\n",
      "   macro avg       0.45      0.44      0.42        80\n",
      "weighted avg       0.61      0.55      0.55        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'stop_words', 'text_cleaning'] + Frequency\n",
      "F1 Score para Decision Tree con ['tokenization', 'stop_words', 'text_cleaning'] + Frequency: 0.439\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'stop_words', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.67      0.57      0.62         7\n",
      "    Deportes       0.35      0.82      0.49        11\n",
      "    Economía       0.61      0.53      0.57        36\n",
      "  Tecnología       0.65      0.44      0.52        25\n",
      "\n",
      "    accuracy                           0.54        80\n",
      "   macro avg       0.45      0.47      0.44        80\n",
      "weighted avg       0.58      0.54      0.54        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Decision Tree con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF: 0.470\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.75      0.43      0.55         7\n",
      "    Deportes       0.43      0.82      0.56        11\n",
      "    Economía       0.69      0.56      0.62        36\n",
      "  Tecnología       0.62      0.64      0.63        25\n",
      "\n",
      "    accuracy                           0.60        80\n",
      "   macro avg       0.50      0.49      0.47        80\n",
      "weighted avg       0.63      0.60      0.60        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Decision Tree con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD: 0.480\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.43      0.43      0.43         7\n",
      "    Deportes       0.70      0.64      0.67        11\n",
      "    Economía       0.68      0.72      0.70        36\n",
      "  Tecnología       0.60      0.60      0.60        25\n",
      "\n",
      "    accuracy                           0.64        80\n",
      "   macro avg       0.48      0.48      0.48        80\n",
      "weighted avg       0.63      0.64      0.63        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Decision Tree con ['tokenization', 'lemmatization', 'text_cleaning'] + Binarized: 0.425\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.25      0.14      0.18         7\n",
      "    Deportes       0.88      0.64      0.74        11\n",
      "    Economía       0.57      0.69      0.62        36\n",
      "  Tecnología       0.61      0.56      0.58        25\n",
      "\n",
      "    accuracy                           0.59        80\n",
      "   macro avg       0.46      0.41      0.43        80\n",
      "weighted avg       0.59      0.59      0.58        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Decision Tree con ['tokenization', 'lemmatization', 'text_cleaning'] + Frequency: 0.400\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.33      0.29      0.31         7\n",
      "    Deportes       0.56      0.45      0.50        11\n",
      "    Economía       0.57      0.69      0.62        36\n",
      "  Tecnología       0.62      0.52      0.57        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.42      0.39      0.40        80\n",
      "weighted avg       0.55      0.56      0.55        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Decision Tree con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF: 0.430\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.43      0.43      0.43         7\n",
      "    Deportes       0.71      0.45      0.56        11\n",
      "    Economía       0.61      0.56      0.58        36\n",
      "  Tecnología       0.52      0.68      0.59        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.45      0.42      0.43        80\n",
      "weighted avg       0.57      0.56      0.56        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Decision Tree con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.427\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.14      0.25         7\n",
      "    Deportes       0.60      0.82      0.69        11\n",
      "    Economía       0.61      0.78      0.68        36\n",
      "  Tecnología       0.61      0.44      0.51        25\n",
      "\n",
      "    accuracy                           0.61        80\n",
      "   macro avg       0.56      0.44      0.43        80\n",
      "weighted avg       0.63      0.61      0.58        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Decision Tree con ['stop_words', 'lemmatization', 'text_cleaning'] + Binarized: 0.412\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['stop_words', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.40      0.29      0.33         7\n",
      "    Deportes       0.83      0.45      0.59        11\n",
      "    Economía       0.54      0.75      0.63        36\n",
      "  Tecnología       0.61      0.44      0.51        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.48      0.39      0.41        80\n",
      "weighted avg       0.58      0.56      0.55        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Decision Tree con ['stop_words', 'lemmatization', 'text_cleaning'] + Frequency: 0.365\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['stop_words', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.17      0.14      0.15         7\n",
      "    Deportes       0.56      0.45      0.50        11\n",
      "    Economía       0.56      0.64      0.60        36\n",
      "  Tecnología       0.58      0.56      0.57        25\n",
      "\n",
      "    accuracy                           0.54        80\n",
      "   macro avg       0.37      0.36      0.36        80\n",
      "weighted avg       0.53      0.54      0.53        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Decision Tree con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF: 0.390\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.23      0.43      0.30         7\n",
      "    Deportes       0.41      0.64      0.50        11\n",
      "    Economía       0.64      0.64      0.64        36\n",
      "  Tecnología       0.71      0.40      0.51        25\n",
      "\n",
      "    accuracy                           0.54        80\n",
      "   macro avg       0.40      0.42      0.39        80\n",
      "weighted avg       0.59      0.54      0.54        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Decision Tree con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.501\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.36      0.71      0.48         7\n",
      "    Deportes       0.75      0.82      0.78        11\n",
      "    Economía       0.72      0.78      0.75        36\n",
      "  Tecnología       0.67      0.40      0.50        25\n",
      "\n",
      "    accuracy                           0.65        80\n",
      "   macro avg       0.50      0.54      0.50        80\n",
      "weighted avg       0.67      0.65      0.64        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Decision Tree con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized: 0.361\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.33      0.14      0.20         7\n",
      "    Deportes       0.33      0.64      0.44        11\n",
      "    Economía       0.60      0.58      0.59        36\n",
      "  Tecnología       0.65      0.52      0.58        25\n",
      "\n",
      "    accuracy                           0.53        80\n",
      "   macro avg       0.38      0.38      0.36        80\n",
      "weighted avg       0.55      0.53      0.52        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Decision Tree con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency: 0.334\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.33      0.14      0.20         7\n",
      "    Deportes       0.28      0.64      0.39        11\n",
      "    Economía       0.59      0.56      0.57        36\n",
      "  Tecnología       0.61      0.44      0.51        25\n",
      "\n",
      "    accuracy                           0.49        80\n",
      "   macro avg       0.36      0.35      0.33        80\n",
      "weighted avg       0.52      0.49      0.49        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Decision Tree con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF: 0.479\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.80      0.57      0.67         7\n",
      "    Deportes       0.37      0.64      0.47        11\n",
      "    Economía       0.67      0.61      0.64        36\n",
      "  Tecnología       0.65      0.60      0.62        25\n",
      "\n",
      "    accuracy                           0.60        80\n",
      "   macro avg       0.50      0.48      0.48        80\n",
      "weighted avg       0.62      0.60      0.60        80\n",
      "\n",
      "\n",
      "Evaluando Decision Tree con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Decision Tree con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.446\n",
      "\n",
      "Reporte de Clasificación para Decision Tree con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.31      0.71      0.43         7\n",
      "    Deportes       0.55      0.55      0.55        11\n",
      "    Economía       0.70      0.72      0.71        36\n",
      "  Tecnología       0.69      0.44      0.54        25\n",
      "\n",
      "    accuracy                           0.60        80\n",
      "   macro avg       0.45      0.48      0.45        80\n",
      "weighted avg       0.63      0.60      0.60        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization'] + Binarized\n",
      "F1 Score para SVM con ['tokenization'] + Binarized: 0.335\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.83      0.45      0.59        11\n",
      "    Economía       0.52      0.94      0.67        36\n",
      "  Tecnología       0.78      0.28      0.41        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.43      0.34      0.33        80\n",
      "weighted avg       0.59      0.57      0.51        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization'] + Frequency\n",
      "F1 Score para SVM con ['tokenization'] + Frequency: 0.312\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.86      0.55      0.67        11\n",
      "    Economía       0.51      1.00      0.68        36\n",
      "  Tecnología       1.00      0.12      0.21        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.47      0.33      0.31        80\n",
      "weighted avg       0.66      0.56      0.46        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization'] + TF-IDF\n",
      "F1 Score para SVM con ['tokenization'] + TF-IDF: 0.204\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.18      0.31        11\n",
      "    Economía       0.47      1.00      0.64        36\n",
      "  Tecnología       1.00      0.04      0.08        25\n",
      "\n",
      "    accuracy                           0.49        80\n",
      "   macro avg       0.49      0.24      0.20        80\n",
      "weighted avg       0.66      0.49      0.35        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization'] + TF-IDF + SVD\n",
      "F1 Score para SVM con ['tokenization'] + TF-IDF + SVD: 0.513\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.73      1.00      0.85        11\n",
      "    Economía       0.63      0.94      0.76        36\n",
      "  Tecnología       0.75      0.24      0.36        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.62      0.52      0.51        80\n",
      "weighted avg       0.71      0.68      0.62        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['stop_words'] + Binarized\n",
      "F1 Score para SVM con ['stop_words'] + Binarized: 0.335\n",
      "\n",
      "Reporte de Clasificación para SVM con ['stop_words'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.83      0.45      0.59        11\n",
      "    Economía       0.52      0.94      0.67        36\n",
      "  Tecnología       0.78      0.28      0.41        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.43      0.34      0.33        80\n",
      "weighted avg       0.59      0.57      0.51        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['stop_words'] + Frequency\n",
      "F1 Score para SVM con ['stop_words'] + Frequency: 0.312\n",
      "\n",
      "Reporte de Clasificación para SVM con ['stop_words'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.86      0.55      0.67        11\n",
      "    Economía       0.51      1.00      0.68        36\n",
      "  Tecnología       1.00      0.12      0.21        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.47      0.33      0.31        80\n",
      "weighted avg       0.66      0.56      0.46        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['stop_words'] + TF-IDF\n",
      "F1 Score para SVM con ['stop_words'] + TF-IDF: 0.204\n",
      "\n",
      "Reporte de Clasificación para SVM con ['stop_words'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.18      0.31        11\n",
      "    Economía       0.47      1.00      0.64        36\n",
      "  Tecnología       1.00      0.04      0.08        25\n",
      "\n",
      "    accuracy                           0.49        80\n",
      "   macro avg       0.49      0.24      0.20        80\n",
      "weighted avg       0.66      0.49      0.35        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['stop_words'] + TF-IDF + SVD\n",
      "F1 Score para SVM con ['stop_words'] + TF-IDF + SVD: 0.513\n",
      "\n",
      "Reporte de Clasificación para SVM con ['stop_words'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.73      1.00      0.85        11\n",
      "    Economía       0.63      0.94      0.76        36\n",
      "  Tecnología       0.75      0.24      0.36        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.62      0.52      0.51        80\n",
      "weighted avg       0.71      0.68      0.62        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['lemmatization'] + Binarized\n",
      "F1 Score para SVM con ['lemmatization'] + Binarized: 0.335\n",
      "\n",
      "Reporte de Clasificación para SVM con ['lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.83      0.45      0.59        11\n",
      "    Economía       0.52      0.94      0.67        36\n",
      "  Tecnología       0.78      0.28      0.41        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.43      0.34      0.33        80\n",
      "weighted avg       0.59      0.57      0.51        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['lemmatization'] + Frequency\n",
      "F1 Score para SVM con ['lemmatization'] + Frequency: 0.312\n",
      "\n",
      "Reporte de Clasificación para SVM con ['lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.86      0.55      0.67        11\n",
      "    Economía       0.51      1.00      0.68        36\n",
      "  Tecnología       1.00      0.12      0.21        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.47      0.33      0.31        80\n",
      "weighted avg       0.66      0.56      0.46        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['lemmatization'] + TF-IDF\n",
      "F1 Score para SVM con ['lemmatization'] + TF-IDF: 0.204\n",
      "\n",
      "Reporte de Clasificación para SVM con ['lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.18      0.31        11\n",
      "    Economía       0.47      1.00      0.64        36\n",
      "  Tecnología       1.00      0.04      0.08        25\n",
      "\n",
      "    accuracy                           0.49        80\n",
      "   macro avg       0.49      0.24      0.20        80\n",
      "weighted avg       0.66      0.49      0.35        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para SVM con ['lemmatization'] + TF-IDF + SVD: 0.513\n",
      "\n",
      "Reporte de Clasificación para SVM con ['lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.73      1.00      0.85        11\n",
      "    Economía       0.63      0.94      0.76        36\n",
      "  Tecnología       0.75      0.24      0.36        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.62      0.52      0.51        80\n",
      "weighted avg       0.71      0.68      0.62        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['text_cleaning'] + Binarized\n",
      "F1 Score para SVM con ['text_cleaning'] + Binarized: 0.335\n",
      "\n",
      "Reporte de Clasificación para SVM con ['text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.83      0.45      0.59        11\n",
      "    Economía       0.52      0.94      0.67        36\n",
      "  Tecnología       0.78      0.28      0.41        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.43      0.34      0.33        80\n",
      "weighted avg       0.59      0.57      0.51        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['text_cleaning'] + Frequency\n",
      "F1 Score para SVM con ['text_cleaning'] + Frequency: 0.312\n",
      "\n",
      "Reporte de Clasificación para SVM con ['text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.86      0.55      0.67        11\n",
      "    Economía       0.51      1.00      0.68        36\n",
      "  Tecnología       1.00      0.12      0.21        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.47      0.33      0.31        80\n",
      "weighted avg       0.66      0.56      0.46        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['text_cleaning'] + TF-IDF\n",
      "F1 Score para SVM con ['text_cleaning'] + TF-IDF: 0.204\n",
      "\n",
      "Reporte de Clasificación para SVM con ['text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.18      0.31        11\n",
      "    Economía       0.47      1.00      0.64        36\n",
      "  Tecnología       1.00      0.04      0.08        25\n",
      "\n",
      "    accuracy                           0.49        80\n",
      "   macro avg       0.49      0.24      0.20        80\n",
      "weighted avg       0.66      0.49      0.35        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para SVM con ['text_cleaning'] + TF-IDF + SVD: 0.513\n",
      "\n",
      "Reporte de Clasificación para SVM con ['text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.73      1.00      0.85        11\n",
      "    Economía       0.63      0.94      0.76        36\n",
      "  Tecnología       0.75      0.24      0.36        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.62      0.52      0.51        80\n",
      "weighted avg       0.71      0.68      0.62        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'stop_words'] + Binarized\n",
      "F1 Score para SVM con ['tokenization', 'stop_words'] + Binarized: 0.371\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'stop_words'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.27      0.43        11\n",
      "    Economía       0.60      0.83      0.70        36\n",
      "  Tecnología       0.70      0.76      0.73        25\n",
      "\n",
      "    accuracy                           0.65        80\n",
      "   macro avg       0.46      0.37      0.37        80\n",
      "weighted avg       0.63      0.65      0.60        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'stop_words'] + Frequency\n",
      "F1 Score para SVM con ['tokenization', 'stop_words'] + Frequency: 0.424\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'stop_words'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.45      0.62        11\n",
      "    Economía       0.64      0.81      0.72        36\n",
      "  Tecnología       0.72      0.84      0.78        25\n",
      "\n",
      "    accuracy                           0.69        80\n",
      "   macro avg       0.47      0.42      0.42        80\n",
      "weighted avg       0.65      0.69      0.65        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'stop_words'] + TF-IDF\n",
      "F1 Score para SVM con ['tokenization', 'stop_words'] + TF-IDF: 0.234\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'stop_words'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.18      0.31        11\n",
      "    Economía       0.48      1.00      0.65        36\n",
      "  Tecnología       1.00      0.12      0.21        25\n",
      "\n",
      "    accuracy                           0.51        80\n",
      "   macro avg       0.50      0.26      0.23        80\n",
      "weighted avg       0.67      0.51      0.40        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'stop_words'] + TF-IDF + SVD\n",
      "F1 Score para SVM con ['tokenization', 'stop_words'] + TF-IDF + SVD: 0.534\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'stop_words'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.56      0.71      0.62         7\n",
      "    Deportes       1.00      0.64      0.78        11\n",
      "    Economía       0.62      0.89      0.73        36\n",
      "  Tecnología       0.83      0.40      0.54        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.60      0.53      0.53        80\n",
      "weighted avg       0.72      0.68      0.66        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'lemmatization'] + Binarized\n",
      "F1 Score para SVM con ['tokenization', 'lemmatization'] + Binarized: 0.360\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.71      0.45      0.56        11\n",
      "    Economía       0.55      0.86      0.67        36\n",
      "  Tecnología       0.71      0.48      0.57        25\n",
      "\n",
      "    accuracy                           0.60        80\n",
      "   macro avg       0.39      0.36      0.36        80\n",
      "weighted avg       0.57      0.60      0.56        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'lemmatization'] + Frequency\n",
      "F1 Score para SVM con ['tokenization', 'lemmatization'] + Frequency: 0.300\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.71      0.45      0.56        11\n",
      "    Economía       0.51      0.97      0.67        36\n",
      "  Tecnología       1.00      0.16      0.28        25\n",
      "\n",
      "    accuracy                           0.55        80\n",
      "   macro avg       0.44      0.32      0.30        80\n",
      "weighted avg       0.64      0.55      0.46        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'lemmatization'] + TF-IDF\n",
      "F1 Score para SVM con ['tokenization', 'lemmatization'] + TF-IDF: 0.267\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.49      1.00      0.65        36\n",
      "  Tecnología       1.00      0.08      0.15        25\n",
      "\n",
      "    accuracy                           0.53        80\n",
      "   macro avg       0.50      0.29      0.27        80\n",
      "weighted avg       0.67      0.53      0.41        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para SVM con ['tokenization', 'lemmatization'] + TF-IDF + SVD: 0.477\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.65      1.00      0.79        11\n",
      "    Economía       0.65      0.97      0.78        36\n",
      "  Tecnología       0.86      0.24      0.38        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.63      0.50      0.48        80\n",
      "weighted avg       0.74      0.68      0.61        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'text_cleaning'] + Binarized\n",
      "F1 Score para SVM con ['tokenization', 'text_cleaning'] + Binarized: 0.335\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.83      0.45      0.59        11\n",
      "    Economía       0.52      0.94      0.67        36\n",
      "  Tecnología       0.78      0.28      0.41        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.43      0.34      0.33        80\n",
      "weighted avg       0.59      0.57      0.51        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'text_cleaning'] + Frequency\n",
      "F1 Score para SVM con ['tokenization', 'text_cleaning'] + Frequency: 0.312\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.86      0.55      0.67        11\n",
      "    Economía       0.51      1.00      0.68        36\n",
      "  Tecnología       1.00      0.12      0.21        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.47      0.33      0.31        80\n",
      "weighted avg       0.66      0.56      0.46        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para SVM con ['tokenization', 'text_cleaning'] + TF-IDF: 0.204\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.18      0.31        11\n",
      "    Economía       0.47      1.00      0.64        36\n",
      "  Tecnología       1.00      0.04      0.08        25\n",
      "\n",
      "    accuracy                           0.49        80\n",
      "   macro avg       0.49      0.24      0.20        80\n",
      "weighted avg       0.66      0.49      0.35        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para SVM con ['tokenization', 'text_cleaning'] + TF-IDF + SVD: 0.513\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.73      1.00      0.85        11\n",
      "    Economía       0.63      0.94      0.76        36\n",
      "  Tecnología       0.75      0.24      0.36        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.62      0.52      0.51        80\n",
      "weighted avg       0.71      0.68      0.62        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['stop_words', 'lemmatization'] + Binarized\n",
      "F1 Score para SVM con ['stop_words', 'lemmatization'] + Binarized: 0.335\n",
      "\n",
      "Reporte de Clasificación para SVM con ['stop_words', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.83      0.45      0.59        11\n",
      "    Economía       0.52      0.94      0.67        36\n",
      "  Tecnología       0.78      0.28      0.41        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.43      0.34      0.33        80\n",
      "weighted avg       0.59      0.57      0.51        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['stop_words', 'lemmatization'] + Frequency\n",
      "F1 Score para SVM con ['stop_words', 'lemmatization'] + Frequency: 0.312\n",
      "\n",
      "Reporte de Clasificación para SVM con ['stop_words', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.86      0.55      0.67        11\n",
      "    Economía       0.51      1.00      0.68        36\n",
      "  Tecnología       1.00      0.12      0.21        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.47      0.33      0.31        80\n",
      "weighted avg       0.66      0.56      0.46        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['stop_words', 'lemmatization'] + TF-IDF\n",
      "F1 Score para SVM con ['stop_words', 'lemmatization'] + TF-IDF: 0.204\n",
      "\n",
      "Reporte de Clasificación para SVM con ['stop_words', 'lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.18      0.31        11\n",
      "    Economía       0.47      1.00      0.64        36\n",
      "  Tecnología       1.00      0.04      0.08        25\n",
      "\n",
      "    accuracy                           0.49        80\n",
      "   macro avg       0.49      0.24      0.20        80\n",
      "weighted avg       0.66      0.49      0.35        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['stop_words', 'lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para SVM con ['stop_words', 'lemmatization'] + TF-IDF + SVD: 0.513\n",
      "\n",
      "Reporte de Clasificación para SVM con ['stop_words', 'lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.73      1.00      0.85        11\n",
      "    Economía       0.63      0.94      0.76        36\n",
      "  Tecnología       0.75      0.24      0.36        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.62      0.52      0.51        80\n",
      "weighted avg       0.71      0.68      0.62        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['stop_words', 'text_cleaning'] + Binarized\n",
      "F1 Score para SVM con ['stop_words', 'text_cleaning'] + Binarized: 0.335\n",
      "\n",
      "Reporte de Clasificación para SVM con ['stop_words', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.83      0.45      0.59        11\n",
      "    Economía       0.52      0.94      0.67        36\n",
      "  Tecnología       0.78      0.28      0.41        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.43      0.34      0.33        80\n",
      "weighted avg       0.59      0.57      0.51        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['stop_words', 'text_cleaning'] + Frequency\n",
      "F1 Score para SVM con ['stop_words', 'text_cleaning'] + Frequency: 0.312\n",
      "\n",
      "Reporte de Clasificación para SVM con ['stop_words', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.86      0.55      0.67        11\n",
      "    Economía       0.51      1.00      0.68        36\n",
      "  Tecnología       1.00      0.12      0.21        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.47      0.33      0.31        80\n",
      "weighted avg       0.66      0.56      0.46        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['stop_words', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para SVM con ['stop_words', 'text_cleaning'] + TF-IDF: 0.204\n",
      "\n",
      "Reporte de Clasificación para SVM con ['stop_words', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.18      0.31        11\n",
      "    Economía       0.47      1.00      0.64        36\n",
      "  Tecnología       1.00      0.04      0.08        25\n",
      "\n",
      "    accuracy                           0.49        80\n",
      "   macro avg       0.49      0.24      0.20        80\n",
      "weighted avg       0.66      0.49      0.35        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['stop_words', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para SVM con ['stop_words', 'text_cleaning'] + TF-IDF + SVD: 0.513\n",
      "\n",
      "Reporte de Clasificación para SVM con ['stop_words', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.73      1.00      0.85        11\n",
      "    Economía       0.63      0.94      0.76        36\n",
      "  Tecnología       0.75      0.24      0.36        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.62      0.52      0.51        80\n",
      "weighted avg       0.71      0.68      0.62        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para SVM con ['lemmatization', 'text_cleaning'] + Binarized: 0.335\n",
      "\n",
      "Reporte de Clasificación para SVM con ['lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.83      0.45      0.59        11\n",
      "    Economía       0.52      0.94      0.67        36\n",
      "  Tecnología       0.78      0.28      0.41        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.43      0.34      0.33        80\n",
      "weighted avg       0.59      0.57      0.51        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para SVM con ['lemmatization', 'text_cleaning'] + Frequency: 0.312\n",
      "\n",
      "Reporte de Clasificación para SVM con ['lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.86      0.55      0.67        11\n",
      "    Economía       0.51      1.00      0.68        36\n",
      "  Tecnología       1.00      0.12      0.21        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.47      0.33      0.31        80\n",
      "weighted avg       0.66      0.56      0.46        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para SVM con ['lemmatization', 'text_cleaning'] + TF-IDF: 0.204\n",
      "\n",
      "Reporte de Clasificación para SVM con ['lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.18      0.31        11\n",
      "    Economía       0.47      1.00      0.64        36\n",
      "  Tecnología       1.00      0.04      0.08        25\n",
      "\n",
      "    accuracy                           0.49        80\n",
      "   macro avg       0.49      0.24      0.20        80\n",
      "weighted avg       0.66      0.49      0.35        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para SVM con ['lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.513\n",
      "\n",
      "Reporte de Clasificación para SVM con ['lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.73      1.00      0.85        11\n",
      "    Economía       0.63      0.94      0.76        36\n",
      "  Tecnología       0.75      0.24      0.36        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.62      0.52      0.51        80\n",
      "weighted avg       0.71      0.68      0.62        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'stop_words', 'lemmatization'] + Binarized\n",
      "F1 Score para SVM con ['tokenization', 'stop_words', 'lemmatization'] + Binarized: 0.407\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'stop_words', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.45      0.62        11\n",
      "    Economía       0.61      0.83      0.71        36\n",
      "  Tecnología       0.69      0.72      0.71        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.46      0.40      0.41        80\n",
      "weighted avg       0.63      0.66      0.62        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'stop_words', 'lemmatization'] + Frequency\n",
      "F1 Score para SVM con ['tokenization', 'stop_words', 'lemmatization'] + Frequency: 0.423\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'stop_words', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.55      0.71        11\n",
      "    Economía       0.62      0.78      0.69        36\n",
      "  Tecnología       0.68      0.76      0.72        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.46      0.42      0.42        80\n",
      "weighted avg       0.63      0.66      0.63        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'stop_words', 'lemmatization'] + TF-IDF\n",
      "F1 Score para SVM con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF: 0.245\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.27      0.43        11\n",
      "    Economía       0.48      1.00      0.65        36\n",
      "  Tecnología       1.00      0.08      0.15        25\n",
      "\n",
      "    accuracy                           0.51        80\n",
      "   macro avg       0.50      0.27      0.25        80\n",
      "weighted avg       0.67      0.51      0.40        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para SVM con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD: 0.497\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.36      0.71      0.48         7\n",
      "    Deportes       1.00      0.73      0.84        11\n",
      "    Economía       0.65      0.89      0.75        36\n",
      "  Tecnología       0.78      0.28      0.41        25\n",
      "\n",
      "    accuracy                           0.65        80\n",
      "   macro avg       0.56      0.52      0.50        80\n",
      "weighted avg       0.71      0.65      0.62        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'stop_words', 'text_cleaning'] + Binarized\n",
      "F1 Score para SVM con ['tokenization', 'stop_words', 'text_cleaning'] + Binarized: 0.371\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'stop_words', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.27      0.43        11\n",
      "    Economía       0.60      0.83      0.70        36\n",
      "  Tecnología       0.70      0.76      0.73        25\n",
      "\n",
      "    accuracy                           0.65        80\n",
      "   macro avg       0.46      0.37      0.37        80\n",
      "weighted avg       0.63      0.65      0.60        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'stop_words', 'text_cleaning'] + Frequency\n",
      "F1 Score para SVM con ['tokenization', 'stop_words', 'text_cleaning'] + Frequency: 0.424\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'stop_words', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.45      0.62        11\n",
      "    Economía       0.64      0.81      0.72        36\n",
      "  Tecnología       0.72      0.84      0.78        25\n",
      "\n",
      "    accuracy                           0.69        80\n",
      "   macro avg       0.47      0.42      0.42        80\n",
      "weighted avg       0.65      0.69      0.65        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para SVM con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF: 0.234\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.18      0.31        11\n",
      "    Economía       0.48      1.00      0.65        36\n",
      "  Tecnología       1.00      0.12      0.21        25\n",
      "\n",
      "    accuracy                           0.51        80\n",
      "   macro avg       0.50      0.26      0.23        80\n",
      "weighted avg       0.67      0.51      0.40        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para SVM con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD: 0.534\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.56      0.71      0.62         7\n",
      "    Deportes       1.00      0.64      0.78        11\n",
      "    Economía       0.62      0.89      0.73        36\n",
      "  Tecnología       0.83      0.40      0.54        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.60      0.53      0.53        80\n",
      "weighted avg       0.72      0.68      0.66        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para SVM con ['tokenization', 'lemmatization', 'text_cleaning'] + Binarized: 0.383\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.75      0.55      0.63        11\n",
      "    Economía       0.57      0.89      0.70        36\n",
      "  Tecnología       0.75      0.48      0.59        25\n",
      "\n",
      "    accuracy                           0.62        80\n",
      "   macro avg       0.41      0.38      0.38        80\n",
      "weighted avg       0.59      0.62      0.58        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para SVM con ['tokenization', 'lemmatization', 'text_cleaning'] + Frequency: 0.316\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.75      0.55      0.63        11\n",
      "    Economía       0.51      0.97      0.67        36\n",
      "  Tecnología       1.00      0.16      0.28        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.45      0.34      0.32        80\n",
      "weighted avg       0.65      0.56      0.48        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para SVM con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF: 0.267\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.49      1.00      0.65        36\n",
      "  Tecnología       1.00      0.08      0.15        25\n",
      "\n",
      "    accuracy                           0.53        80\n",
      "   macro avg       0.50      0.29      0.27        80\n",
      "weighted avg       0.67      0.53      0.41        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para SVM con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.443\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.58      1.00      0.73        11\n",
      "    Economía       0.63      0.89      0.74        36\n",
      "  Tecnología       0.62      0.20      0.30        25\n",
      "\n",
      "    accuracy                           0.62        80\n",
      "   macro avg       0.57      0.47      0.44        80\n",
      "weighted avg       0.64      0.62      0.57        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para SVM con ['stop_words', 'lemmatization', 'text_cleaning'] + Binarized: 0.335\n",
      "\n",
      "Reporte de Clasificación para SVM con ['stop_words', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.83      0.45      0.59        11\n",
      "    Economía       0.52      0.94      0.67        36\n",
      "  Tecnología       0.78      0.28      0.41        25\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.43      0.34      0.33        80\n",
      "weighted avg       0.59      0.57      0.51        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para SVM con ['stop_words', 'lemmatization', 'text_cleaning'] + Frequency: 0.312\n",
      "\n",
      "Reporte de Clasificación para SVM con ['stop_words', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.86      0.55      0.67        11\n",
      "    Economía       0.51      1.00      0.68        36\n",
      "  Tecnología       1.00      0.12      0.21        25\n",
      "\n",
      "    accuracy                           0.56        80\n",
      "   macro avg       0.47      0.33      0.31        80\n",
      "weighted avg       0.66      0.56      0.46        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para SVM con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF: 0.204\n",
      "\n",
      "Reporte de Clasificación para SVM con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.18      0.31        11\n",
      "    Economía       0.47      1.00      0.64        36\n",
      "  Tecnología       1.00      0.04      0.08        25\n",
      "\n",
      "    accuracy                           0.49        80\n",
      "   macro avg       0.49      0.24      0.20        80\n",
      "weighted avg       0.66      0.49      0.35        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para SVM con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.513\n",
      "\n",
      "Reporte de Clasificación para SVM con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.73      1.00      0.85        11\n",
      "    Economía       0.63      0.94      0.76        36\n",
      "  Tecnología       0.75      0.24      0.36        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.62      0.52      0.51        80\n",
      "weighted avg       0.71      0.68      0.62        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para SVM con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized: 0.400\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.36      0.53        11\n",
      "    Economía       0.62      0.83      0.71        36\n",
      "  Tecnología       0.71      0.80      0.75        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.47      0.40      0.40        80\n",
      "weighted avg       0.64      0.68      0.63        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para SVM con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency: 0.429\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.55      0.71        11\n",
      "    Economía       0.63      0.81      0.71        36\n",
      "  Tecnología       0.70      0.76      0.73        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.47      0.42      0.43        80\n",
      "weighted avg       0.64      0.68      0.64        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para SVM con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF: 0.245\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       1.00      0.27      0.43        11\n",
      "    Economía       0.48      1.00      0.65        36\n",
      "  Tecnología       1.00      0.08      0.15        25\n",
      "\n",
      "    accuracy                           0.51        80\n",
      "   macro avg       0.50      0.27      0.25        80\n",
      "weighted avg       0.67      0.51      0.40        80\n",
      "\n",
      "\n",
      "Evaluando SVM con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para SVM con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.526\n",
      "\n",
      "Reporte de Clasificación para SVM con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.45      0.71      0.56         7\n",
      "    Deportes       0.91      0.91      0.91        11\n",
      "    Economía       0.64      0.83      0.72        36\n",
      "  Tecnología       0.73      0.32      0.44        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.55      0.56      0.53        80\n",
      "weighted avg       0.68      0.66      0.64        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization'] + Binarized\n",
      "F1 Score para Neural Network con ['tokenization'] + Binarized: 0.500\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.14      0.25         7\n",
      "    Deportes       0.77      0.91      0.83        11\n",
      "    Economía       0.66      0.97      0.79        36\n",
      "  Tecnología       0.92      0.48      0.63        25\n",
      "\n",
      "    accuracy                           0.72        80\n",
      "   macro avg       0.67      0.50      0.50        80\n",
      "weighted avg       0.78      0.72      0.69        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization'] + Frequency\n",
      "F1 Score para Neural Network con ['tokenization'] + Frequency: 0.451\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.61      1.00      0.76        11\n",
      "    Economía       0.73      0.89      0.80        36\n",
      "  Tecnología       0.83      0.60      0.70        25\n",
      "\n",
      "    accuracy                           0.72        80\n",
      "   macro avg       0.43      0.50      0.45        80\n",
      "weighted avg       0.67      0.72      0.68        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization'] + TF-IDF\n",
      "F1 Score para Neural Network con ['tokenization'] + TF-IDF: 0.581\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.91      0.91      0.91        11\n",
      "    Economía       0.70      0.92      0.80        36\n",
      "  Tecnología       0.85      0.68      0.76        25\n",
      "\n",
      "    accuracy                           0.78        80\n",
      "   macro avg       0.69      0.56      0.58        80\n",
      "weighted avg       0.79      0.78      0.76        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization'] + TF-IDF + SVD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dern9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score para Neural Network con ['tokenization'] + TF-IDF + SVD: 0.630\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.71      0.71      0.71         7\n",
      "    Deportes       0.83      0.91      0.87        11\n",
      "    Economía       0.81      0.81      0.81        36\n",
      "  Tecnología       0.76      0.76      0.76        25\n",
      "\n",
      "    accuracy                           0.79        80\n",
      "   macro avg       0.62      0.64      0.63        80\n",
      "weighted avg       0.78      0.79      0.78        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words'] + Binarized\n",
      "F1 Score para Neural Network con ['stop_words'] + Binarized: 0.533\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.73      1.00      0.85        11\n",
      "    Economía       0.67      0.94      0.78        36\n",
      "  Tecnología       0.92      0.44      0.59        25\n",
      "\n",
      "    accuracy                           0.72        80\n",
      "   macro avg       0.66      0.53      0.53        80\n",
      "weighted avg       0.77      0.72      0.69        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words'] + Frequency\n",
      "F1 Score para Neural Network con ['stop_words'] + Frequency: 0.505\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.14      0.25         7\n",
      "    Deportes       0.85      1.00      0.92        11\n",
      "    Economía       0.65      0.92      0.76        36\n",
      "  Tecnología       0.80      0.48      0.60        25\n",
      "\n",
      "    accuracy                           0.71        80\n",
      "   macro avg       0.66      0.51      0.51        80\n",
      "weighted avg       0.75      0.71      0.68        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words'] + TF-IDF\n",
      "F1 Score para Neural Network con ['stop_words'] + TF-IDF: 0.615\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.85      1.00      0.92        11\n",
      "    Economía       0.80      0.92      0.86        36\n",
      "  Tecnología       0.88      0.84      0.86        25\n",
      "\n",
      "    accuracy                           0.84        80\n",
      "   macro avg       0.71      0.61      0.62        80\n",
      "weighted avg       0.84      0.84      0.82        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words'] + TF-IDF + SVD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dern9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score para Neural Network con ['stop_words'] + TF-IDF + SVD: 0.636\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.67      0.86      0.75         7\n",
      "    Deportes       0.83      0.91      0.87        11\n",
      "    Economía       0.82      0.78      0.80        36\n",
      "  Tecnología       0.76      0.76      0.76        25\n",
      "\n",
      "    accuracy                           0.79        80\n",
      "   macro avg       0.62      0.66      0.64        80\n",
      "weighted avg       0.78      0.79      0.78        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['lemmatization'] + Binarized\n",
      "F1 Score para Neural Network con ['lemmatization'] + Binarized: 0.564\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       1.00      0.91      0.95        11\n",
      "    Economía       0.65      0.92      0.76        36\n",
      "  Tecnología       0.82      0.56      0.67        25\n",
      "\n",
      "    accuracy                           0.74        80\n",
      "   macro avg       0.69      0.53      0.56        80\n",
      "weighted avg       0.77      0.74      0.72        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['lemmatization'] + Frequency\n",
      "F1 Score para Neural Network con ['lemmatization'] + Frequency: 0.562\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.90      0.82      0.86        11\n",
      "    Economía       0.68      0.89      0.77        36\n",
      "  Tecnología       0.81      0.68      0.74        25\n",
      "\n",
      "    accuracy                           0.75        80\n",
      "   macro avg       0.68      0.53      0.56        80\n",
      "weighted avg       0.77      0.75      0.73        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['lemmatization'] + TF-IDF\n",
      "F1 Score para Neural Network con ['lemmatization'] + TF-IDF: 0.575\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.83      0.91      0.87        11\n",
      "    Economía       0.72      0.92      0.80        36\n",
      "  Tecnología       0.85      0.68      0.76        25\n",
      "\n",
      "    accuracy                           0.78        80\n",
      "   macro avg       0.68      0.56      0.57        80\n",
      "weighted avg       0.79      0.78      0.76        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para Neural Network con ['lemmatization'] + TF-IDF + SVD: 0.630\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.71      0.71      0.71         7\n",
      "    Deportes       0.83      0.91      0.87        11\n",
      "    Economía       0.81      0.81      0.81        36\n",
      "  Tecnología       0.76      0.76      0.76        25\n",
      "\n",
      "    accuracy                           0.79        80\n",
      "   macro avg       0.62      0.64      0.63        80\n",
      "weighted avg       0.78      0.79      0.78        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['text_cleaning'] + Binarized\n",
      "F1 Score para Neural Network con ['text_cleaning'] + Binarized: 0.511\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.14      0.25         7\n",
      "    Deportes       0.92      1.00      0.96        11\n",
      "    Economía       0.63      0.92      0.75        36\n",
      "  Tecnología       0.80      0.48      0.60        25\n",
      "\n",
      "    accuracy                           0.71        80\n",
      "   macro avg       0.67      0.51      0.51        80\n",
      "weighted avg       0.75      0.71      0.68        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['text_cleaning'] + Frequency\n",
      "F1 Score para Neural Network con ['text_cleaning'] + Frequency: 0.533\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.14      0.25         7\n",
      "    Deportes       0.91      0.91      0.91        11\n",
      "    Economía       0.67      0.92      0.78        36\n",
      "  Tecnología       0.84      0.64      0.73        25\n",
      "\n",
      "    accuracy                           0.75        80\n",
      "   macro avg       0.68      0.52      0.53        80\n",
      "weighted avg       0.78      0.75      0.72        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['text_cleaning'] + TF-IDF\n",
      "F1 Score para Neural Network con ['text_cleaning'] + TF-IDF: 0.519\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.14      0.25         7\n",
      "    Deportes       0.83      0.91      0.87        11\n",
      "    Economía       0.67      0.92      0.78        36\n",
      "  Tecnología       0.83      0.60      0.70        25\n",
      "\n",
      "    accuracy                           0.74        80\n",
      "   macro avg       0.67      0.51      0.52        80\n",
      "weighted avg       0.77      0.74      0.71        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Neural Network con ['text_cleaning'] + TF-IDF + SVD: 0.636\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.67      0.86      0.75         7\n",
      "    Deportes       0.83      0.91      0.87        11\n",
      "    Economía       0.82      0.78      0.80        36\n",
      "  Tecnología       0.76      0.76      0.76        25\n",
      "\n",
      "    accuracy                           0.79        80\n",
      "   macro avg       0.62      0.66      0.64        80\n",
      "weighted avg       0.78      0.79      0.78        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'stop_words'] + Binarized\n",
      "F1 Score para Neural Network con ['tokenization', 'stop_words'] + Binarized: 0.573\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'stop_words'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.62      0.91      0.74        11\n",
      "    Economía       0.74      0.86      0.79        36\n",
      "  Tecnología       0.84      0.64      0.73        25\n",
      "\n",
      "    accuracy                           0.75        80\n",
      "   macro avg       0.64      0.57      0.57        80\n",
      "weighted avg       0.77      0.75      0.74        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'stop_words'] + Frequency\n",
      "F1 Score para Neural Network con ['tokenization', 'stop_words'] + Frequency: 0.601\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'stop_words'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.64      0.82      0.72        11\n",
      "    Economía       0.79      0.86      0.83        36\n",
      "  Tecnología       0.88      0.84      0.86        25\n",
      "\n",
      "    accuracy                           0.80        80\n",
      "   macro avg       0.66      0.59      0.60        80\n",
      "weighted avg       0.81      0.80      0.79        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'stop_words'] + TF-IDF\n",
      "F1 Score para Neural Network con ['tokenization', 'stop_words'] + TF-IDF: 0.570\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'stop_words'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.69      0.82      0.75        11\n",
      "    Economía       0.76      0.89      0.82        36\n",
      "  Tecnología       0.87      0.80      0.83        25\n",
      "\n",
      "    accuracy                           0.79        80\n",
      "   macro avg       0.66      0.56      0.57        80\n",
      "weighted avg       0.80      0.79      0.77        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'stop_words'] + TF-IDF + SVD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dern9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score para Neural Network con ['tokenization', 'stop_words'] + TF-IDF + SVD: 0.596\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'stop_words'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.71      0.91      0.80        11\n",
      "    Economía       0.78      0.81      0.79        36\n",
      "  Tecnología       0.77      0.80      0.78        25\n",
      "\n",
      "    accuracy                           0.78        80\n",
      "   macro avg       0.65      0.59      0.60        80\n",
      "weighted avg       0.78      0.78      0.77        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'lemmatization'] + Binarized\n",
      "F1 Score para Neural Network con ['tokenization', 'lemmatization'] + Binarized: 0.572\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.73      1.00      0.85        11\n",
      "    Economía       0.76      0.86      0.81        36\n",
      "  Tecnología       0.82      0.72      0.77        25\n",
      "\n",
      "    accuracy                           0.78        80\n",
      "   macro avg       0.66      0.57      0.57        80\n",
      "weighted avg       0.78      0.78      0.76        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'lemmatization'] + Frequency\n",
      "F1 Score para Neural Network con ['tokenization', 'lemmatization'] + Frequency: 0.581\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.85      1.00      0.92        11\n",
      "    Economía       0.74      0.86      0.79        36\n",
      "  Tecnología       0.78      0.72      0.75        25\n",
      "\n",
      "    accuracy                           0.78        80\n",
      "   macro avg       0.67      0.57      0.58        80\n",
      "weighted avg       0.78      0.78      0.76        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'lemmatization'] + TF-IDF\n",
      "F1 Score para Neural Network con ['tokenization', 'lemmatization'] + TF-IDF: 0.569\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.67      0.29      0.40         7\n",
      "    Deportes       0.83      0.91      0.87        11\n",
      "    Economía       0.74      0.89      0.81        36\n",
      "  Tecnología       0.82      0.72      0.77        25\n",
      "\n",
      "    accuracy                           0.78        80\n",
      "   macro avg       0.61      0.56      0.57        80\n",
      "weighted avg       0.76      0.78      0.76        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para Neural Network con ['tokenization', 'lemmatization'] + TF-IDF + SVD: 0.580\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.75      0.43      0.55         7\n",
      "    Deportes       0.71      0.91      0.80        11\n",
      "    Economía       0.82      0.78      0.80        36\n",
      "  Tecnología       0.71      0.80      0.75        25\n",
      "\n",
      "    accuracy                           0.76        80\n",
      "   macro avg       0.60      0.58      0.58        80\n",
      "weighted avg       0.76      0.76      0.75        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Neural Network con ['tokenization', 'text_cleaning'] + Binarized: 0.488\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.14      0.25         7\n",
      "    Deportes       0.77      0.91      0.83        11\n",
      "    Economía       0.65      0.92      0.76        36\n",
      "  Tecnología       0.80      0.48      0.60        25\n",
      "\n",
      "    accuracy                           0.70        80\n",
      "   macro avg       0.64      0.49      0.49        80\n",
      "weighted avg       0.73      0.70      0.67        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Neural Network con ['tokenization', 'text_cleaning'] + Frequency: 0.531\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.14      0.25         7\n",
      "    Deportes       0.69      1.00      0.81        11\n",
      "    Economía       0.80      0.89      0.84        36\n",
      "  Tecnología       0.78      0.72      0.75        25\n",
      "\n",
      "    accuracy                           0.78        80\n",
      "   macro avg       0.65      0.55      0.53        80\n",
      "weighted avg       0.79      0.78      0.75        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Neural Network con ['tokenization', 'text_cleaning'] + TF-IDF: 0.533\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.14      0.25         7\n",
      "    Deportes       0.91      0.91      0.91        11\n",
      "    Economía       0.67      0.92      0.78        36\n",
      "  Tecnología       0.84      0.64      0.73        25\n",
      "\n",
      "    accuracy                           0.75        80\n",
      "   macro avg       0.68      0.52      0.53        80\n",
      "weighted avg       0.78      0.75      0.72        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'text_cleaning'] + TF-IDF + SVD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dern9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score para Neural Network con ['tokenization', 'text_cleaning'] + TF-IDF + SVD: 0.623\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.71      0.71      0.71         7\n",
      "    Deportes       0.83      0.91      0.87        11\n",
      "    Economía       0.78      0.81      0.79        36\n",
      "  Tecnología       0.75      0.72      0.73        25\n",
      "\n",
      "    accuracy                           0.78        80\n",
      "   macro avg       0.62      0.63      0.62        80\n",
      "weighted avg       0.76      0.78      0.77        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words', 'lemmatization'] + Binarized\n",
      "F1 Score para Neural Network con ['stop_words', 'lemmatization'] + Binarized: 0.520\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.14      0.25         7\n",
      "    Deportes       0.92      1.00      0.96        11\n",
      "    Economía       0.65      0.92      0.76        36\n",
      "  Tecnología       0.81      0.52      0.63        25\n",
      "\n",
      "    accuracy                           0.72        80\n",
      "   macro avg       0.68      0.52      0.52        80\n",
      "weighted avg       0.76      0.72      0.69        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words', 'lemmatization'] + Frequency\n",
      "F1 Score para Neural Network con ['stop_words', 'lemmatization'] + Frequency: 0.478\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.92      1.00      0.96        11\n",
      "    Economía       0.65      0.89      0.75        36\n",
      "  Tecnología       0.79      0.60      0.68        25\n",
      "\n",
      "    accuracy                           0.72        80\n",
      "   macro avg       0.47      0.50      0.48        80\n",
      "weighted avg       0.67      0.72      0.68        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words', 'lemmatization'] + TF-IDF\n",
      "F1 Score para Neural Network con ['stop_words', 'lemmatization'] + TF-IDF: 0.568\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words', 'lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.79      1.00      0.88        11\n",
      "    Economía       0.71      0.89      0.79        36\n",
      "  Tecnología       0.84      0.64      0.73        25\n",
      "\n",
      "    accuracy                           0.76        80\n",
      "   macro avg       0.67      0.56      0.57        80\n",
      "weighted avg       0.78      0.76      0.74        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words', 'lemmatization'] + TF-IDF + SVD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dern9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score para Neural Network con ['stop_words', 'lemmatization'] + TF-IDF + SVD: 0.644\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words', 'lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.83      0.71      0.77         7\n",
      "    Deportes       0.83      0.91      0.87        11\n",
      "    Economía       0.81      0.83      0.82        36\n",
      "  Tecnología       0.76      0.76      0.76        25\n",
      "\n",
      "    accuracy                           0.80        80\n",
      "   macro avg       0.65      0.64      0.64        80\n",
      "weighted avg       0.79      0.80      0.79        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words', 'text_cleaning'] + Binarized\n",
      "F1 Score para Neural Network con ['stop_words', 'text_cleaning'] + Binarized: 0.573\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.91      0.91      0.91        11\n",
      "    Economía       0.69      0.92      0.79        36\n",
      "  Tecnología       0.84      0.64      0.73        25\n",
      "\n",
      "    accuracy                           0.76        80\n",
      "   macro avg       0.69      0.55      0.57        80\n",
      "weighted avg       0.79      0.76      0.74        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words', 'text_cleaning'] + Frequency\n",
      "F1 Score para Neural Network con ['stop_words', 'text_cleaning'] + Frequency: 0.507\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.14      0.25         7\n",
      "    Deportes       0.65      1.00      0.79        11\n",
      "    Economía       0.73      0.89      0.80        36\n",
      "  Tecnología       0.83      0.60      0.70        25\n",
      "\n",
      "    accuracy                           0.74        80\n",
      "   macro avg       0.64      0.53      0.51        80\n",
      "weighted avg       0.76      0.74      0.71        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Neural Network con ['stop_words', 'text_cleaning'] + TF-IDF: 0.565\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.79      1.00      0.88        11\n",
      "    Economía       0.72      0.92      0.80        36\n",
      "  Tecnología       0.83      0.60      0.70        25\n",
      "\n",
      "    accuracy                           0.76        80\n",
      "   macro avg       0.67      0.56      0.57        80\n",
      "weighted avg       0.78      0.76      0.74        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words', 'text_cleaning'] + TF-IDF + SVD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dern9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score para Neural Network con ['stop_words', 'text_cleaning'] + TF-IDF + SVD: 0.636\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.67      0.86      0.75         7\n",
      "    Deportes       0.83      0.91      0.87        11\n",
      "    Economía       0.82      0.78      0.80        36\n",
      "  Tecnología       0.76      0.76      0.76        25\n",
      "\n",
      "    accuracy                           0.79        80\n",
      "   macro avg       0.62      0.66      0.64        80\n",
      "weighted avg       0.78      0.79      0.78        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Neural Network con ['lemmatization', 'text_cleaning'] + Binarized: 0.543\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.92      1.00      0.96        11\n",
      "    Economía       0.63      0.92      0.75        36\n",
      "  Tecnología       0.79      0.44      0.56        25\n",
      "\n",
      "    accuracy                           0.71        80\n",
      "   macro avg       0.67      0.53      0.54        80\n",
      "weighted avg       0.74      0.71      0.68        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Neural Network con ['lemmatization', 'text_cleaning'] + Frequency: 0.526\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.14      0.25         7\n",
      "    Deportes       0.85      1.00      0.92        11\n",
      "    Economía       0.69      0.86      0.77        36\n",
      "  Tecnología       0.76      0.64      0.70        25\n",
      "\n",
      "    accuracy                           0.74        80\n",
      "   macro avg       0.66      0.53      0.53        80\n",
      "weighted avg       0.75      0.74      0.71        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Neural Network con ['lemmatization', 'text_cleaning'] + TF-IDF: 0.559\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.83      0.91      0.87        11\n",
      "    Economía       0.69      0.92      0.79        36\n",
      "  Tecnología       0.83      0.60      0.70        25\n",
      "\n",
      "    accuracy                           0.75        80\n",
      "   macro avg       0.67      0.54      0.56        80\n",
      "weighted avg       0.77      0.75      0.73        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Neural Network con ['lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.617\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.62      0.71      0.67         7\n",
      "    Deportes       0.83      0.91      0.87        11\n",
      "    Economía       0.80      0.78      0.79        36\n",
      "  Tecnología       0.76      0.76      0.76        25\n",
      "\n",
      "    accuracy                           0.78        80\n",
      "   macro avg       0.60      0.63      0.62        80\n",
      "weighted avg       0.77      0.78      0.77        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'stop_words', 'lemmatization'] + Binarized\n",
      "F1 Score para Neural Network con ['tokenization', 'stop_words', 'lemmatization'] + Binarized: 0.617\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'stop_words', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.79      1.00      0.88        11\n",
      "    Economía       0.76      0.89      0.82        36\n",
      "  Tecnología       0.86      0.72      0.78        25\n",
      "\n",
      "    accuracy                           0.80        80\n",
      "   macro avg       0.68      0.61      0.62        80\n",
      "weighted avg       0.81      0.80      0.79        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'stop_words', 'lemmatization'] + Frequency\n",
      "F1 Score para Neural Network con ['tokenization', 'stop_words', 'lemmatization'] + Frequency: 0.610\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'stop_words', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.91      0.91      0.91        11\n",
      "    Economía       0.70      0.86      0.78        36\n",
      "  Tecnología       0.82      0.72      0.77        25\n",
      "\n",
      "    accuracy                           0.78        80\n",
      "   macro avg       0.69      0.58      0.61        80\n",
      "weighted avg       0.79      0.78      0.77        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'stop_words', 'lemmatization'] + TF-IDF\n",
      "F1 Score para Neural Network con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF: 0.576\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.75      0.43      0.55         7\n",
      "    Deportes       0.59      0.91      0.71        11\n",
      "    Economía       0.82      0.86      0.84        36\n",
      "  Tecnología       0.86      0.72      0.78        25\n",
      "\n",
      "    accuracy                           0.78        80\n",
      "   macro avg       0.60      0.58      0.58        80\n",
      "weighted avg       0.78      0.78      0.77        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dern9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score para Neural Network con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD: 0.564\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.75      0.43      0.55         7\n",
      "    Deportes       0.59      0.91      0.71        11\n",
      "    Economía       0.82      0.78      0.80        36\n",
      "  Tecnología       0.76      0.76      0.76        25\n",
      "\n",
      "    accuracy                           0.75        80\n",
      "   macro avg       0.58      0.58      0.56        80\n",
      "weighted avg       0.75      0.75      0.74        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'stop_words', 'text_cleaning'] + Binarized\n",
      "F1 Score para Neural Network con ['tokenization', 'stop_words', 'text_cleaning'] + Binarized: 0.548\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'stop_words', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.62      0.91      0.74        11\n",
      "    Economía       0.69      0.86      0.77        36\n",
      "  Tecnología       0.81      0.52      0.63        25\n",
      "\n",
      "    accuracy                           0.71        80\n",
      "   macro avg       0.63      0.54      0.55        80\n",
      "weighted avg       0.74      0.71      0.70        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'stop_words', 'text_cleaning'] + Frequency\n",
      "F1 Score para Neural Network con ['tokenization', 'stop_words', 'text_cleaning'] + Frequency: 0.603\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'stop_words', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.69      0.82      0.75        11\n",
      "    Economía       0.78      0.89      0.83        36\n",
      "  Tecnología       0.87      0.80      0.83        25\n",
      "\n",
      "    accuracy                           0.80        80\n",
      "   macro avg       0.67      0.59      0.60        80\n",
      "weighted avg       0.81      0.80      0.79        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Neural Network con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF: 0.565\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.67      0.91      0.77        11\n",
      "    Economía       0.76      0.86      0.81        36\n",
      "  Tecnología       0.86      0.76      0.81        25\n",
      "\n",
      "    accuracy                           0.78        80\n",
      "   macro avg       0.66      0.56      0.57        80\n",
      "weighted avg       0.79      0.78      0.76        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dern9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score para Neural Network con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD: 0.612\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.69      1.00      0.81        11\n",
      "    Economía       0.79      0.75      0.77        36\n",
      "  Tecnología       0.73      0.76      0.75        25\n",
      "\n",
      "    accuracy                           0.76        80\n",
      "   macro avg       0.64      0.62      0.61        80\n",
      "weighted avg       0.77      0.76      0.76        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Neural Network con ['tokenization', 'lemmatization', 'text_cleaning'] + Binarized: 0.539\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.14      0.25         7\n",
      "    Deportes       0.65      1.00      0.79        11\n",
      "    Economía       0.79      0.86      0.83        36\n",
      "  Tecnología       0.87      0.80      0.83        25\n",
      "\n",
      "    accuracy                           0.79        80\n",
      "   macro avg       0.66      0.56      0.54        80\n",
      "weighted avg       0.81      0.79      0.76        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Neural Network con ['tokenization', 'lemmatization', 'text_cleaning'] + Frequency: 0.587\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.73      1.00      0.85        11\n",
      "    Economía       0.81      0.83      0.82        36\n",
      "  Tecnología       0.81      0.84      0.82        25\n",
      "\n",
      "    accuracy                           0.80        80\n",
      "   macro avg       0.67      0.59      0.59        80\n",
      "weighted avg       0.81      0.80      0.78        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Neural Network con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF: 0.569\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.67      0.29      0.40         7\n",
      "    Deportes       0.71      0.91      0.80        11\n",
      "    Economía       0.79      0.86      0.83        36\n",
      "  Tecnología       0.83      0.80      0.82        25\n",
      "\n",
      "    accuracy                           0.79        80\n",
      "   macro avg       0.60      0.57      0.57        80\n",
      "weighted avg       0.77      0.79      0.77        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Neural Network con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.576\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.60      0.43      0.50         7\n",
      "    Deportes       0.83      0.91      0.87        11\n",
      "    Economía       0.74      0.81      0.77        36\n",
      "  Tecnología       0.75      0.72      0.73        25\n",
      "\n",
      "    accuracy                           0.75        80\n",
      "   macro avg       0.59      0.57      0.58        80\n",
      "weighted avg       0.74      0.75      0.74        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Neural Network con ['stop_words', 'lemmatization', 'text_cleaning'] + Binarized: 0.512\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.14      0.25         7\n",
      "    Deportes       0.73      1.00      0.85        11\n",
      "    Economía       0.70      0.92      0.80        36\n",
      "  Tecnología       0.82      0.56      0.67        25\n",
      "\n",
      "    accuracy                           0.74        80\n",
      "   macro avg       0.65      0.52      0.51        80\n",
      "weighted avg       0.76      0.74      0.70        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Neural Network con ['stop_words', 'lemmatization', 'text_cleaning'] + Frequency: 0.577\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.92      1.00      0.96        11\n",
      "    Economía       0.69      0.92      0.79        36\n",
      "  Tecnología       0.83      0.60      0.70        25\n",
      "\n",
      "    accuracy                           0.76        80\n",
      "   macro avg       0.69      0.56      0.58        80\n",
      "weighted avg       0.78      0.76      0.74        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Neural Network con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF: 0.594\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.29      0.44         7\n",
      "    Deportes       0.85      1.00      0.92        11\n",
      "    Economía       0.75      0.92      0.82        36\n",
      "  Tecnología       0.86      0.72      0.78        25\n",
      "\n",
      "    accuracy                           0.80        80\n",
      "   macro avg       0.69      0.58      0.59        80\n",
      "weighted avg       0.81      0.80      0.78        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dern9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score para Neural Network con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.619\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.71      0.71      0.71         7\n",
      "    Deportes       0.77      0.91      0.83        11\n",
      "    Economía       0.80      0.78      0.79        36\n",
      "  Tecnología       0.76      0.76      0.76        25\n",
      "\n",
      "    accuracy                           0.78        80\n",
      "   macro avg       0.61      0.63      0.62        80\n",
      "weighted avg       0.77      0.78      0.77        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Neural Network con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized: 0.625\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.79      1.00      0.88        11\n",
      "    Economía       0.79      0.86      0.83        36\n",
      "  Tecnología       0.83      0.80      0.82        25\n",
      "\n",
      "    accuracy                           0.81        80\n",
      "   macro avg       0.68      0.62      0.62        80\n",
      "weighted avg       0.81      0.81      0.80        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Neural Network con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency: 0.607\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.73      1.00      0.85        11\n",
      "    Economía       0.76      0.86      0.81        36\n",
      "  Tecnología       0.86      0.72      0.78        25\n",
      "\n",
      "    accuracy                           0.79        80\n",
      "   macro avg       0.67      0.60      0.61        80\n",
      "weighted avg       0.80      0.79      0.78        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Neural Network con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF: 0.557\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.67      0.29      0.40         7\n",
      "    Deportes       0.67      0.91      0.77        11\n",
      "    Economía       0.78      0.89      0.83        36\n",
      "  Tecnología       0.86      0.72      0.78        25\n",
      "\n",
      "    accuracy                           0.78        80\n",
      "   macro avg       0.59      0.56      0.56        80\n",
      "weighted avg       0.77      0.78      0.76        80\n",
      "\n",
      "\n",
      "Evaluando Neural Network con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Neural Network con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.588\n",
      "\n",
      "Reporte de Clasificación para Neural Network con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.65      1.00      0.79        11\n",
      "    Economía       0.82      0.75      0.78        36\n",
      "  Tecnología       0.74      0.80      0.77        25\n",
      "\n",
      "    accuracy                           0.76        80\n",
      "   macro avg       0.64      0.60      0.59        80\n",
      "weighted avg       0.78      0.76      0.75        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization'] + Binarized\n",
      "F1 Score para Gradient Boosting con ['tokenization'] + Binarized: 0.542\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.75      0.55      0.63        11\n",
      "    Economía       0.62      0.81      0.70        36\n",
      "  Tecnología       0.71      0.60      0.65        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.62      0.50      0.54        80\n",
      "weighted avg       0.69      0.68      0.67        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization'] + Frequency\n",
      "F1 Score para Gradient Boosting con ['tokenization'] + Frequency: 0.540\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.75      0.55      0.63        11\n",
      "    Economía       0.61      0.83      0.71        36\n",
      "  Tecnología       0.74      0.56      0.64        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.62      0.50      0.54        80\n",
      "weighted avg       0.70      0.68      0.67        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization'] + TF-IDF\n",
      "F1 Score para Gradient Boosting con ['tokenization'] + TF-IDF: 0.500\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.75      0.43      0.55         7\n",
      "    Deportes       0.60      0.55      0.57        11\n",
      "    Economía       0.64      0.78      0.70        36\n",
      "  Tecnología       0.73      0.64      0.68        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.54      0.48      0.50        80\n",
      "weighted avg       0.66      0.66      0.65        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization'] + TF-IDF + SVD\n",
      "F1 Score para Gradient Boosting con ['tokenization'] + TF-IDF + SVD: 0.537\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.67      0.57      0.62         7\n",
      "    Deportes       0.78      0.64      0.70        11\n",
      "    Economía       0.65      0.83      0.73        36\n",
      "  Tecnología       0.74      0.56      0.64        25\n",
      "\n",
      "    accuracy                           0.69        80\n",
      "   macro avg       0.57      0.52      0.54        80\n",
      "weighted avg       0.69      0.69      0.68        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['stop_words'] + Binarized\n",
      "F1 Score para Gradient Boosting con ['stop_words'] + Binarized: 0.550\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['stop_words'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.78      0.64      0.70        11\n",
      "    Economía       0.62      0.81      0.70        36\n",
      "  Tecnología       0.70      0.56      0.62        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.62      0.51      0.55        80\n",
      "weighted avg       0.69      0.68      0.67        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['stop_words'] + Frequency\n",
      "F1 Score para Gradient Boosting con ['stop_words'] + Frequency: 0.554\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['stop_words'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.75      0.55      0.63        11\n",
      "    Economía       0.63      0.86      0.73        36\n",
      "  Tecnología       0.79      0.60      0.68        25\n",
      "\n",
      "    accuracy                           0.70        80\n",
      "   macro avg       0.63      0.52      0.55        80\n",
      "weighted avg       0.72      0.70      0.69        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['stop_words'] + TF-IDF\n",
      "F1 Score para Gradient Boosting con ['stop_words'] + TF-IDF: 0.443\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['stop_words'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.67      0.29      0.40         7\n",
      "    Deportes       0.62      0.45      0.53        11\n",
      "    Economía       0.59      0.72      0.65        36\n",
      "  Tecnología       0.64      0.64      0.64        25\n",
      "\n",
      "    accuracy                           0.61        80\n",
      "   macro avg       0.50      0.42      0.44        80\n",
      "weighted avg       0.61      0.61      0.60        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['stop_words'] + TF-IDF + SVD\n",
      "F1 Score para Gradient Boosting con ['stop_words'] + TF-IDF + SVD: 0.456\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['stop_words'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.33      0.14      0.20         7\n",
      "    Deportes       0.78      0.64      0.70        11\n",
      "    Economía       0.62      0.83      0.71        36\n",
      "  Tecnología       0.75      0.60      0.67        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.50      0.44      0.46        80\n",
      "weighted avg       0.65      0.66      0.64        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['lemmatization'] + Binarized\n",
      "F1 Score para Gradient Boosting con ['lemmatization'] + Binarized: 0.523\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.71      0.45      0.56        11\n",
      "    Economía       0.60      0.83      0.70        36\n",
      "  Tecnología       0.74      0.56      0.64        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.61      0.48      0.52        80\n",
      "weighted avg       0.69      0.66      0.65        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['lemmatization'] + Frequency\n",
      "F1 Score para Gradient Boosting con ['lemmatization'] + Frequency: 0.540\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.75      0.55      0.63        11\n",
      "    Economía       0.61      0.83      0.71        36\n",
      "  Tecnología       0.74      0.56      0.64        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.62      0.50      0.54        80\n",
      "weighted avg       0.70      0.68      0.67        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['lemmatization'] + TF-IDF\n",
      "F1 Score para Gradient Boosting con ['lemmatization'] + TF-IDF: 0.443\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.67      0.29      0.40         7\n",
      "    Deportes       0.62      0.45      0.53        11\n",
      "    Economía       0.59      0.72      0.65        36\n",
      "  Tecnología       0.64      0.64      0.64        25\n",
      "\n",
      "    accuracy                           0.61        80\n",
      "   macro avg       0.50      0.42      0.44        80\n",
      "weighted avg       0.61      0.61      0.60        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para Gradient Boosting con ['lemmatization'] + TF-IDF + SVD: 0.477\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.50      0.29      0.36         7\n",
      "    Deportes       0.78      0.64      0.70        11\n",
      "    Economía       0.62      0.81      0.70        36\n",
      "  Tecnología       0.70      0.56      0.62        25\n",
      "\n",
      "    accuracy                           0.65        80\n",
      "   macro avg       0.52      0.46      0.48        80\n",
      "weighted avg       0.65      0.65      0.64        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['text_cleaning'] + Binarized\n",
      "F1 Score para Gradient Boosting con ['text_cleaning'] + Binarized: 0.517\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.71      0.45      0.56        11\n",
      "    Economía       0.59      0.81      0.68        36\n",
      "  Tecnología       0.70      0.56      0.62        25\n",
      "\n",
      "    accuracy                           0.65        80\n",
      "   macro avg       0.60      0.48      0.52        80\n",
      "weighted avg       0.67      0.65      0.64        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['text_cleaning'] + Frequency\n",
      "F1 Score para Gradient Boosting con ['text_cleaning'] + Frequency: 0.540\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.75      0.55      0.63        11\n",
      "    Economía       0.61      0.83      0.71        36\n",
      "  Tecnología       0.74      0.56      0.64        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.62      0.50      0.54        80\n",
      "weighted avg       0.70      0.68      0.67        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['text_cleaning'] + TF-IDF\n",
      "F1 Score para Gradient Boosting con ['text_cleaning'] + TF-IDF: 0.497\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.75      0.43      0.55         7\n",
      "    Deportes       0.67      0.55      0.60        11\n",
      "    Economía       0.61      0.75      0.68        36\n",
      "  Tecnología       0.70      0.64      0.67        25\n",
      "\n",
      "    accuracy                           0.65        80\n",
      "   macro avg       0.55      0.47      0.50        80\n",
      "weighted avg       0.65      0.65      0.64        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Gradient Boosting con ['text_cleaning'] + TF-IDF + SVD: 0.477\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.50      0.29      0.36         7\n",
      "    Deportes       0.78      0.64      0.70        11\n",
      "    Economía       0.62      0.81      0.70        36\n",
      "  Tecnología       0.70      0.56      0.62        25\n",
      "\n",
      "    accuracy                           0.65        80\n",
      "   macro avg       0.52      0.46      0.48        80\n",
      "weighted avg       0.65      0.65      0.64        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'stop_words'] + Binarized\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'stop_words'] + Binarized: 0.523\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'stop_words'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.80      0.36      0.50        11\n",
      "    Economía       0.60      0.86      0.70        36\n",
      "  Tecnología       0.79      0.60      0.68        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.64      0.48      0.52        80\n",
      "weighted avg       0.71      0.68      0.66        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'stop_words'] + Frequency\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'stop_words'] + Frequency: 0.515\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'stop_words'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.80      0.36      0.50        11\n",
      "    Economía       0.58      0.86      0.70        36\n",
      "  Tecnología       0.78      0.56      0.65        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.63      0.47      0.52        80\n",
      "weighted avg       0.70      0.66      0.65        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'stop_words'] + TF-IDF\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'stop_words'] + TF-IDF: 0.449\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'stop_words'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.60      0.43      0.50         7\n",
      "    Deportes       1.00      0.27      0.43        11\n",
      "    Economía       0.57      0.81      0.67        36\n",
      "  Tecnología       0.71      0.60      0.65        25\n",
      "\n",
      "    accuracy                           0.62        80\n",
      "   macro avg       0.58      0.42      0.45        80\n",
      "weighted avg       0.67      0.62      0.61        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'stop_words'] + TF-IDF + SVD\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'stop_words'] + TF-IDF + SVD: 0.439\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'stop_words'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.50      0.14      0.22         7\n",
      "    Deportes       0.64      0.64      0.64        11\n",
      "    Economía       0.64      0.78      0.70        36\n",
      "  Tecnología       0.68      0.60      0.64        25\n",
      "\n",
      "    accuracy                           0.64        80\n",
      "   macro avg       0.49      0.43      0.44        80\n",
      "weighted avg       0.63      0.64      0.62        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'lemmatization'] + Binarized\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'lemmatization'] + Binarized: 0.506\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.75      0.43      0.55         7\n",
      "    Deportes       0.75      0.55      0.63        11\n",
      "    Economía       0.62      0.81      0.70        36\n",
      "  Tecnología       0.71      0.60      0.65        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.57      0.48      0.51        80\n",
      "weighted avg       0.67      0.66      0.65        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'lemmatization'] + Frequency\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'lemmatization'] + Frequency: 0.504\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.75      0.55      0.63        11\n",
      "    Economía       0.57      0.75      0.65        36\n",
      "  Tecnología       0.68      0.60      0.64        25\n",
      "\n",
      "    accuracy                           0.64        80\n",
      "   macro avg       0.60      0.46      0.50        80\n",
      "weighted avg       0.66      0.64      0.63        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'lemmatization'] + TF-IDF\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'lemmatization'] + TF-IDF: 0.482\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.67      0.29      0.40         7\n",
      "    Deportes       0.86      0.55      0.67        11\n",
      "    Economía       0.58      0.81      0.67        36\n",
      "  Tecnología       0.75      0.60      0.67        25\n",
      "\n",
      "    accuracy                           0.65        80\n",
      "   macro avg       0.57      0.45      0.48        80\n",
      "weighted avg       0.67      0.65      0.64        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'lemmatization'] + TF-IDF + SVD: 0.424\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.50      0.14      0.22         7\n",
      "    Deportes       0.56      0.82      0.67        11\n",
      "    Economía       0.61      0.75      0.68        36\n",
      "  Tecnología       0.67      0.48      0.56        25\n",
      "\n",
      "    accuracy                           0.61        80\n",
      "   macro avg       0.47      0.44      0.42        80\n",
      "weighted avg       0.61      0.61      0.59        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'text_cleaning'] + Binarized: 0.517\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.71      0.45      0.56        11\n",
      "    Economía       0.59      0.81      0.68        36\n",
      "  Tecnología       0.70      0.56      0.62        25\n",
      "\n",
      "    accuracy                           0.65        80\n",
      "   macro avg       0.60      0.48      0.52        80\n",
      "weighted avg       0.67      0.65      0.64        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'text_cleaning'] + Frequency: 0.555\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.75      0.55      0.63        11\n",
      "    Economía       0.64      0.83      0.72        36\n",
      "  Tecnología       0.76      0.64      0.70        25\n",
      "\n",
      "    accuracy                           0.70        80\n",
      "   macro avg       0.63      0.52      0.56        80\n",
      "weighted avg       0.72      0.70      0.69        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'text_cleaning'] + TF-IDF: 0.506\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.75      0.43      0.55         7\n",
      "    Deportes       0.70      0.64      0.67        11\n",
      "    Economía       0.62      0.72      0.67        36\n",
      "  Tecnología       0.67      0.64      0.65        25\n",
      "\n",
      "    accuracy                           0.65        80\n",
      "   macro avg       0.55      0.49      0.51        80\n",
      "weighted avg       0.65      0.65      0.64        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'text_cleaning'] + TF-IDF + SVD: 0.551\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.67      0.57      0.62         7\n",
      "    Deportes       0.80      0.73      0.76        11\n",
      "    Economía       0.67      0.83      0.74        36\n",
      "  Tecnología       0.74      0.56      0.64        25\n",
      "\n",
      "    accuracy                           0.70        80\n",
      "   macro avg       0.57      0.54      0.55        80\n",
      "weighted avg       0.70      0.70      0.69        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['stop_words', 'lemmatization'] + Binarized\n",
      "F1 Score para Gradient Boosting con ['stop_words', 'lemmatization'] + Binarized: 0.499\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['stop_words', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.67      0.36      0.47        11\n",
      "    Economía       0.58      0.81      0.67        36\n",
      "  Tecnología       0.70      0.56      0.62        25\n",
      "\n",
      "    accuracy                           0.64        80\n",
      "   macro avg       0.59      0.46      0.50        80\n",
      "weighted avg       0.66      0.64      0.63        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['stop_words', 'lemmatization'] + Frequency\n",
      "F1 Score para Gradient Boosting con ['stop_words', 'lemmatization'] + Frequency: 0.554\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['stop_words', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.75      0.55      0.63        11\n",
      "    Economía       0.63      0.86      0.73        36\n",
      "  Tecnología       0.79      0.60      0.68        25\n",
      "\n",
      "    accuracy                           0.70        80\n",
      "   macro avg       0.63      0.52      0.55        80\n",
      "weighted avg       0.72      0.70      0.69        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['stop_words', 'lemmatization'] + TF-IDF\n",
      "F1 Score para Gradient Boosting con ['stop_words', 'lemmatization'] + TF-IDF: 0.487\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['stop_words', 'lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.75      0.43      0.55         7\n",
      "    Deportes       0.60      0.55      0.57        11\n",
      "    Economía       0.62      0.72      0.67        36\n",
      "  Tecnología       0.67      0.64      0.65        25\n",
      "\n",
      "    accuracy                           0.64        80\n",
      "   macro avg       0.53      0.47      0.49        80\n",
      "weighted avg       0.64      0.64      0.63        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['stop_words', 'lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para Gradient Boosting con ['stop_words', 'lemmatization'] + TF-IDF + SVD: 0.450\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['stop_words', 'lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.33      0.14      0.20         7\n",
      "    Deportes       0.78      0.64      0.70        11\n",
      "    Economía       0.62      0.81      0.70        36\n",
      "  Tecnología       0.71      0.60      0.65        25\n",
      "\n",
      "    accuracy                           0.65        80\n",
      "   macro avg       0.49      0.44      0.45        80\n",
      "weighted avg       0.64      0.65      0.63        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['stop_words', 'text_cleaning'] + Binarized\n",
      "F1 Score para Gradient Boosting con ['stop_words', 'text_cleaning'] + Binarized: 0.540\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['stop_words', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.75      0.55      0.63        11\n",
      "    Economía       0.61      0.83      0.71        36\n",
      "  Tecnología       0.74      0.56      0.64        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.62      0.50      0.54        80\n",
      "weighted avg       0.70      0.68      0.67        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['stop_words', 'text_cleaning'] + Frequency\n",
      "F1 Score para Gradient Boosting con ['stop_words', 'text_cleaning'] + Frequency: 0.523\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['stop_words', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.71      0.45      0.56        11\n",
      "    Economía       0.60      0.83      0.70        36\n",
      "  Tecnología       0.74      0.56      0.64        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.61      0.48      0.52        80\n",
      "weighted avg       0.69      0.66      0.65        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['stop_words', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Gradient Boosting con ['stop_words', 'text_cleaning'] + TF-IDF: 0.487\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['stop_words', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.67      0.29      0.40         7\n",
      "    Deportes       0.70      0.64      0.67        11\n",
      "    Economía       0.64      0.78      0.70        36\n",
      "  Tecnología       0.70      0.64      0.67        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.54      0.47      0.49        80\n",
      "weighted avg       0.66      0.66      0.65        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['stop_words', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Gradient Boosting con ['stop_words', 'text_cleaning'] + TF-IDF + SVD: 0.451\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['stop_words', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.33      0.14      0.20         7\n",
      "    Deportes       0.70      0.64      0.67        11\n",
      "    Economía       0.64      0.83      0.72        36\n",
      "  Tecnología       0.75      0.60      0.67        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.48      0.44      0.45        80\n",
      "weighted avg       0.65      0.66      0.64        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Gradient Boosting con ['lemmatization', 'text_cleaning'] + Binarized: 0.523\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.71      0.45      0.56        11\n",
      "    Economía       0.60      0.83      0.70        36\n",
      "  Tecnología       0.74      0.56      0.64        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.61      0.48      0.52        80\n",
      "weighted avg       0.69      0.66      0.65        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Gradient Boosting con ['lemmatization', 'text_cleaning'] + Frequency: 0.555\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.75      0.55      0.63        11\n",
      "    Economía       0.64      0.83      0.72        36\n",
      "  Tecnología       0.76      0.64      0.70        25\n",
      "\n",
      "    accuracy                           0.70        80\n",
      "   macro avg       0.63      0.52      0.56        80\n",
      "weighted avg       0.72      0.70      0.69        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Gradient Boosting con ['lemmatization', 'text_cleaning'] + TF-IDF: 0.461\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.67      0.29      0.40         7\n",
      "    Deportes       0.67      0.55      0.60        11\n",
      "    Economía       0.59      0.72      0.65        36\n",
      "  Tecnología       0.67      0.64      0.65        25\n",
      "\n",
      "    accuracy                           0.62        80\n",
      "   macro avg       0.52      0.44      0.46        80\n",
      "weighted avg       0.62      0.62      0.61        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Gradient Boosting con ['lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.450\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.33      0.14      0.20         7\n",
      "    Deportes       0.78      0.64      0.70        11\n",
      "    Economía       0.62      0.81      0.70        36\n",
      "  Tecnología       0.71      0.60      0.65        25\n",
      "\n",
      "    accuracy                           0.65        80\n",
      "   macro avg       0.49      0.44      0.45        80\n",
      "weighted avg       0.64      0.65      0.63        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'stop_words', 'lemmatization'] + Binarized\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'stop_words', 'lemmatization'] + Binarized: 0.445\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'stop_words', 'lemmatization'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.75      0.43      0.55         7\n",
      "    Deportes       0.57      0.36      0.44        11\n",
      "    Economía       0.57      0.78      0.66        36\n",
      "  Tecnología       0.65      0.52      0.58        25\n",
      "\n",
      "    accuracy                           0.60        80\n",
      "   macro avg       0.51      0.42      0.45        80\n",
      "weighted avg       0.60      0.60      0.59        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'stop_words', 'lemmatization'] + Frequency\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'stop_words', 'lemmatization'] + Frequency: 0.464\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'stop_words', 'lemmatization'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.75      0.43      0.55         7\n",
      "    Deportes       0.62      0.45      0.53        11\n",
      "    Economía       0.57      0.78      0.66        36\n",
      "  Tecnología       0.68      0.52      0.59        25\n",
      "\n",
      "    accuracy                           0.61        80\n",
      "   macro avg       0.53      0.44      0.46        80\n",
      "weighted avg       0.62      0.61      0.60        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'stop_words', 'lemmatization'] + TF-IDF\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF: 0.455\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.60      0.43      0.50         7\n",
      "    Deportes       0.80      0.36      0.50        11\n",
      "    Economía       0.58      0.78      0.67        36\n",
      "  Tecnología       0.67      0.56      0.61        25\n",
      "\n",
      "    accuracy                           0.61        80\n",
      "   macro avg       0.53      0.43      0.46        80\n",
      "weighted avg       0.63      0.61      0.60        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD: 0.417\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.00      0.00      0.00         7\n",
      "    Deportes       0.88      0.64      0.74        11\n",
      "    Economía       0.65      0.78      0.71        36\n",
      "  Tecnología       0.64      0.64      0.64        25\n",
      "\n",
      "    accuracy                           0.64        80\n",
      "   macro avg       0.43      0.41      0.42        80\n",
      "weighted avg       0.61      0.64      0.62        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'stop_words', 'text_cleaning'] + Binarized\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'stop_words', 'text_cleaning'] + Binarized: 0.506\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'stop_words', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.67      0.36      0.47        11\n",
      "    Economía       0.58      0.83      0.68        36\n",
      "  Tecnología       0.78      0.56      0.65        25\n",
      "\n",
      "    accuracy                           0.65        80\n",
      "   macro avg       0.60      0.47      0.51        80\n",
      "weighted avg       0.68      0.65      0.64        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'stop_words', 'text_cleaning'] + Frequency\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'stop_words', 'text_cleaning'] + Frequency: 0.501\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'stop_words', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.67      0.36      0.47        11\n",
      "    Economía       0.59      0.83      0.69        36\n",
      "  Tecnología       0.76      0.52      0.62        25\n",
      "\n",
      "    accuracy                           0.64        80\n",
      "   macro avg       0.60      0.46      0.50        80\n",
      "weighted avg       0.68      0.64      0.63        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF: 0.444\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.60      0.43      0.50         7\n",
      "    Deportes       1.00      0.27      0.43        11\n",
      "    Economía       0.56      0.78      0.65        36\n",
      "  Tecnología       0.68      0.60      0.64        25\n",
      "\n",
      "    accuracy                           0.61        80\n",
      "   macro avg       0.57      0.42      0.44        80\n",
      "weighted avg       0.66      0.61      0.60        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD: 0.438\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.50      0.14      0.22         7\n",
      "    Deportes       0.64      0.64      0.64        11\n",
      "    Economía       0.62      0.78      0.69        36\n",
      "  Tecnología       0.68      0.60      0.64        25\n",
      "\n",
      "    accuracy                           0.64        80\n",
      "   macro avg       0.49      0.43      0.44        80\n",
      "weighted avg       0.62      0.64      0.62        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'lemmatization', 'text_cleaning'] + Binarized: 0.535\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.73      0.73      0.73        11\n",
      "    Economía       0.63      0.75      0.68        36\n",
      "  Tecnología       0.70      0.64      0.67        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.61      0.51      0.54        80\n",
      "weighted avg       0.69      0.68      0.67        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'lemmatization', 'text_cleaning'] + Frequency: 0.544\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.89      0.73      0.80        11\n",
      "    Economía       0.61      0.78      0.68        36\n",
      "  Tecnología       0.68      0.60      0.64        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.64      0.51      0.54        80\n",
      "weighted avg       0.70      0.68      0.67        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF: 0.511\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.75      0.43      0.55         7\n",
      "    Deportes       0.86      0.55      0.67        11\n",
      "    Economía       0.60      0.81      0.69        36\n",
      "  Tecnología       0.71      0.60      0.65        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.59      0.48      0.51        80\n",
      "weighted avg       0.68      0.66      0.65        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.420\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.14      0.25         7\n",
      "    Deportes       0.64      0.82      0.72        11\n",
      "    Economía       0.58      0.72      0.64        36\n",
      "  Tecnología       0.55      0.44      0.49        25\n",
      "\n",
      "    accuracy                           0.59        80\n",
      "   macro avg       0.55      0.42      0.42        80\n",
      "weighted avg       0.61      0.59      0.56        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Gradient Boosting con ['stop_words', 'lemmatization', 'text_cleaning'] + Binarized: 0.540\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['stop_words', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.75      0.55      0.63        11\n",
      "    Economía       0.61      0.83      0.71        36\n",
      "  Tecnología       0.74      0.56      0.64        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.62      0.50      0.54        80\n",
      "weighted avg       0.70      0.68      0.67        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Gradient Boosting con ['stop_words', 'lemmatization', 'text_cleaning'] + Frequency: 0.531\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['stop_words', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.57      0.73         7\n",
      "    Deportes       0.71      0.45      0.56        11\n",
      "    Economía       0.61      0.83      0.71        36\n",
      "  Tecnología       0.75      0.60      0.67        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.62      0.49      0.53        80\n",
      "weighted avg       0.70      0.68      0.67        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Gradient Boosting con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF: 0.476\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.67      0.29      0.40         7\n",
      "    Deportes       0.75      0.55      0.63        11\n",
      "    Economía       0.61      0.78      0.68        36\n",
      "  Tecnología       0.70      0.64      0.67        25\n",
      "\n",
      "    accuracy                           0.65        80\n",
      "   macro avg       0.54      0.45      0.48        80\n",
      "weighted avg       0.65      0.65      0.64        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Gradient Boosting con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.456\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.33      0.14      0.20         7\n",
      "    Deportes       0.78      0.64      0.70        11\n",
      "    Economía       0.62      0.83      0.71        36\n",
      "  Tecnología       0.75      0.60      0.67        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.50      0.44      0.46        80\n",
      "weighted avg       0.65      0.66      0.64        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized: 0.523\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Binarized:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.78      0.64      0.70        11\n",
      "    Economía       0.60      0.81      0.69        36\n",
      "  Tecnología       0.70      0.56      0.62        25\n",
      "\n",
      "    accuracy                           0.66        80\n",
      "   macro avg       0.62      0.49      0.52        80\n",
      "weighted avg       0.69      0.66      0.65        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency: 0.530\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       1.00      0.43      0.60         7\n",
      "    Deportes       0.78      0.64      0.70        11\n",
      "    Economía       0.62      0.81      0.70        36\n",
      "  Tecnología       0.71      0.60      0.65        25\n",
      "\n",
      "    accuracy                           0.68        80\n",
      "   macro avg       0.62      0.49      0.53        80\n",
      "weighted avg       0.70      0.68      0.67        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF: 0.499\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.67      0.57      0.62         7\n",
      "    Deportes       0.83      0.45      0.59        11\n",
      "    Economía       0.58      0.78      0.67        36\n",
      "  Tecnología       0.70      0.56      0.62        25\n",
      "\n",
      "    accuracy                           0.64        80\n",
      "   macro avg       0.56      0.47      0.50        80\n",
      "weighted avg       0.65      0.64      0.63        80\n",
      "\n",
      "\n",
      "Evaluando Gradient Boosting con combincion:['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD\n",
      "F1 Score para Gradient Boosting con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD: 0.563\n",
      "\n",
      "Reporte de Clasificación para Gradient Boosting con ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Ciencias       0.00      0.00      0.00         1\n",
      "     Cultura       0.75      0.43      0.55         7\n",
      "    Deportes       0.83      0.91      0.87        11\n",
      "    Economía       0.67      0.78      0.72        36\n",
      "  Tecnología       0.73      0.64      0.68        25\n",
      "\n",
      "    accuracy                           0.71        80\n",
      "   macro avg       0.60      0.55      0.56        80\n",
      "weighted avg       0.71      0.71      0.70        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for clf_name, clf in classifiers.items():\n",
    "    results[clf_name] = {}\n",
    "    for com_rep_name, (X_train_rep, X_test_rep) in representaciones.items():\n",
    "        if clf_name == \"Naive Bayes\" and \"TF-IDF\" in com_rep_name:\n",
    "            print(f\"Skipping {clf_name} with {com_rep_name}\")\n",
    "            continue\n",
    "        print(f'\\nEvaluando {clf_name} con combincion:{com_rep_name}')\n",
    "        try:\n",
    "            f1, y_pred = TrainandEvaluate(clf,X_train_rep,X_test_rep,y_train,y_test)\n",
    "            results[clf_name][com_rep_name] = f1\n",
    "            print(f'F1 Score para {clf_name} con {com_rep_name}: {f1:.3f}')\n",
    "            print(f'\\nReporte de Clasificación para {clf_name} con {com_rep_name}:')\n",
    "            print(classification_report(y_test, y_pred, zero_division=0))\n",
    "        except Exception as e:\n",
    "            print(f'Error al evaluar {clf_name} con {com_rep_name}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los 3 mejores resultados por clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clasificador: Logistic Regression\n",
      "2. Combinación: ['tokenization', 'lemmatization', 'text_cleaning'] + Frequency, F1 Score: 0.628\n",
      "3. Combinación: ['tokenization', 'lemmatization', 'text_cleaning'] + Binarized, F1 Score: 0.611\n",
      "4. Combinación: ['tokenization', 'lemmatization'] + Binarized, F1 Score: 0.610\n",
      "\n",
      "Clasificador: Naive Bayes\n",
      "2. Combinación: ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + Frequency, F1 Score: 0.621\n",
      "3. Combinación: ['tokenization', 'stop_words', 'lemmatization'] + Frequency, F1 Score: 0.614\n",
      "4. Combinación: ['tokenization', 'stop_words'] + Binarized, F1 Score: 0.605\n",
      "\n",
      "Clasificador: Decision Tree\n",
      "2. Combinación: ['tokenization', 'stop_words', 'lemmatization'] + TF-IDF, F1 Score: 0.503\n",
      "3. Combinación: ['stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD, F1 Score: 0.501\n",
      "4. Combinación: ['tokenization', 'lemmatization'] + TF-IDF, F1 Score: 0.490\n",
      "\n",
      "Clasificador: SVM\n",
      "2. Combinación: ['tokenization', 'stop_words'] + TF-IDF + SVD, F1 Score: 0.534\n",
      "3. Combinación: ['tokenization', 'stop_words', 'text_cleaning'] + TF-IDF + SVD, F1 Score: 0.534\n",
      "4. Combinación: ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD, F1 Score: 0.526\n",
      "\n",
      "Clasificador: Neural Network\n",
      "2. Combinación: ['stop_words', 'lemmatization'] + TF-IDF + SVD, F1 Score: 0.644\n",
      "3. Combinación: ['stop_words'] + TF-IDF + SVD, F1 Score: 0.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dern9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Combinación: ['text_cleaning'] + TF-IDF + SVD, F1 Score: 0.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dern9\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clasificador: Gradient Boosting\n",
      "2. Combinación: ['tokenization', 'stop_words', 'lemmatization', 'text_cleaning'] + TF-IDF + SVD, F1 Score: 0.563\n",
      "3. Combinación: ['tokenization', 'text_cleaning'] + Frequency, F1 Score: 0.555\n",
      "4. Combinación: ['lemmatization', 'text_cleaning'] + Frequency, F1 Score: 0.555\n"
     ]
    }
   ],
   "source": [
    "for clf_name, clf_scores in results.items():\n",
    "    print(f'\\nClasificador: {clf_name}')\n",
    "    top_combos = sorted(clf_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "\n",
    "    for rank, (combo_name, f1) in enumerate(top_combos, start=1):\n",
    "        print(f'{rank+1}. Combinación: {combo_name}, F1 Score: {f1:.3f}')\n",
    "\n",
    "        X_train_rep, X_test_rep = representaciones[combo_name]\n",
    "        clf = classifiers[clf_name]\n",
    "        clf.fit(X_train_rep, y_train)\n",
    "        y_pred = clf.predict(X_test_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
