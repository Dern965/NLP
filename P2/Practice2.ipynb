{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 2 Text Normalization\n",
    "***\n",
    "Rodriguez Nuñez Diego Eduardo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect information from different sources\n",
    "• Get news using RSS feeds from La Jornada and Expansión platforms using the following URLs:\n",
    "\n",
    "    • https://www.jornada.com.mx/v7.0/cgi/rss.php\n",
    "\n",
    "    • https://expansion.mx/canales-rss\n",
    "    \n",
    "• The data collection should be done once a day during 5 days at agreed time\n",
    "\n",
    "• News can be repeated from one day to the next, so you must avoid collecting it again\n",
    "\n",
    "• For each news article extract:\n",
    "\n",
    "    • Title (<title>)\n",
    "    • Content summary (<description>)\n",
    "    • Section\n",
    "    • URL (<link>)\n",
    "    • Date of publication (<pubDate>)\n",
    "\n",
    "• Section of interest are:\n",
    "\n",
    "    • Sports\n",
    "    • Economy\n",
    "    • Science and technology\n",
    "    • Culture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = {\n",
    "    \"Jan\": \"01\", \"Feb\": \"02\", \"Mar\": \"03\", \"Apr\": \"04\", \"May\": \"05\", \"Jun\": \"06\",\n",
    "    \"Jul\": \"07\", \"Aug\": \"08\", \"Sep\": \"09\", \"Oct\": \"10\", \"Nov\": \"11\", \"Dec\": \"12\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Source', 'Title', 'Content', 'Section', 'URL', 'Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_date(date):\n",
    "    match = re.match(r\"\\w{3}, (\\d{2}) (\\w{3}) (\\d{4})\", date)\n",
    "    if match:\n",
    "        day = match.group(1)    # Extrae el día\n",
    "        month_str = match.group(2)  # Extrae el mes como texto\n",
    "        year = match.group(3)   # Extrae el año\n",
    "\n",
    "        # Convertir el mes de texto a su correspondiente número\n",
    "        month = months.get(month_str, \"00\")  # Si no encuentra el mes, devuelve \"00\"\n",
    "        \n",
    "        # Formatear en dd/mm/yyyy\n",
    "        formatted_date = f\"{day}/{month}/{year}\"\n",
    "        return formatted_date\n",
    "    else:\n",
    "        return None # Retorna None si no se puede extraer la fecha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "la_jornada_urls = [\"https://www.jornada.com.mx/rss/deportes.xml?v=1\",\"https://www.jornada.com.mx/rss/economia.xml?v=1\",\"https://www.jornada.com.mx/rss/ciencias.xml?v=1\",\"https://www.jornada.com.mx/rss/cultura.xml?v=1\"]\n",
    "expansion_urls = [\"https://www.expansion.mx/rss/economia\",\"https://www.expansion.mx/rss/tecnologia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news(url):\n",
    "    feed = feedparser.parse(url)\n",
    "    news = []\n",
    "    fuente = feed.feed.title\n",
    "    # Usa una cadena sin procesar (raw string) con el prefijo r\n",
    "    section_match = re.search(r\"[-:]\\s*(\\w+)\", fuente)\n",
    "    # Si hay un match, extraemos la sección, de lo contrario dejamos la sección como None\n",
    "    section = section_match.group(1) if section_match else None\n",
    "    for entry in feed.entries:\n",
    "        # Extraemos la fecha en formato dd/mm/yyyy\n",
    "        formatted_date = format_date(entry.published)\n",
    "        news.append({\n",
    "            'Source': feed.feed.title,\n",
    "            'Title': entry.title,\n",
    "            'Content': entry.description,\n",
    "            'Section': section,\n",
    "            'URL': entry.link,\n",
    "            'Date': formatted_date\n",
    "        })\n",
    "    return news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to raw_data_corpus2.csv\n"
     ]
    }
   ],
   "source": [
    "csv_file = \"raw_data_corpus2.csv\"\n",
    "for i, url_list in enumerate([la_jornada_urls, expansion_urls]):\n",
    "    for url in url_list:\n",
    "        news = get_news(url)\n",
    "        df = pd.DataFrame(news)\n",
    "\n",
    "        # Write the header only for the first URL processed\n",
    "        if i == 0 and url == la_jornada_urls[0]:\n",
    "            df.to_csv(csv_file, mode='a', header=columns, index=False)\n",
    "        else:\n",
    "            df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "\n",
    "print(f\"Data saved to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Data Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "stop_Pos = {'DET', 'ADP', 'SCONJ', 'CCONJ', 'PRON'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    doc = nlp(text)\n",
    "    normalized_tokens =[]\n",
    "    for token in doc:\n",
    "        if not token.is_stop and token.pos_ not in stop_Pos:\n",
    "            normalized_tokens.append(token.lemma_)\n",
    "    return \" \".join(normalized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = pd.read_csv(\"raw_data_corpus.csv\")\n",
    "df_normalized['Normalized Content'] = df_normalized['Content'].apply(normalize_text)\n",
    "df_normalized['Normalized Title'] = df_normalized['Title'].apply(normalize_text)\n",
    "df_normalized.to_csv(\"normalized_data_corpus.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
